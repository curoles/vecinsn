/* DO NOT EDIT THIS FILE MANUALLY! */
#pragma once

#include <stdint.h>

typedef unsigned char      __u8x16  __attribute__ ((__vector_size__ (16)));
typedef   signed char      __s8x16  __attribute__ ((__vector_size__ (16)));
typedef unsigned short     __u16x8  __attribute__ ((__vector_size__ (16)));
typedef          short     __s16x8  __attribute__ ((__vector_size__ (16)));
typedef unsigned int       __u32x4  __attribute__ ((__vector_size__ (16)));
typedef          int       __s32x4  __attribute__ ((__vector_size__ (16)));
typedef unsigned long      __u64x2  __attribute__ ((__vector_size__ (16)));
typedef          long      __s64x2  __attribute__ ((__vector_size__ (16)));
typedef          float     __f32x4  __attribute__ ((__vector_size__ (16)));
typedef          double    __f64x2  __attribute__ ((__vector_size__ (16)));

typedef unsigned char      __u8x32  __attribute__ ((__vector_size__ (32)));
typedef   signed char      __s8x32  __attribute__ ((__vector_size__ (32)));
typedef unsigned short     __u16x16 __attribute__ ((__vector_size__ (32)));
typedef          short     __s16x16 __attribute__ ((__vector_size__ (32)));
typedef unsigned int       __u32x8  __attribute__ ((__vector_size__ (32)));
typedef          int       __s32x8  __attribute__ ((__vector_size__ (32)));
typedef unsigned long      __u64x4  __attribute__ ((__vector_size__ (32)));
typedef          long      __s64x4  __attribute__ ((__vector_size__ (32)));
typedef          float     __f32x8  __attribute__ ((__vector_size__ (32)));
typedef          double    __f64x4  __attribute__ ((__vector_size__ (32)));

typedef unsigned char      __u8x64  __attribute__ ((__vector_size__ (64)));
typedef   signed char      __s8x64  __attribute__ ((__vector_size__ (64)));
typedef unsigned short     __u16x32 __attribute__ ((__vector_size__ (64)));
typedef          short     __s16x32 __attribute__ ((__vector_size__ (64)));
typedef unsigned int       __u32x16 __attribute__ ((__vector_size__ (64)));
typedef          int       __s32x16 __attribute__ ((__vector_size__ (64)));
typedef unsigned long      __u64x8  __attribute__ ((__vector_size__ (64)));
typedef          long      __s64x8  __attribute__ ((__vector_size__ (64)));
typedef          float     __f32x16 __attribute__ ((__vector_size__ (64)));
typedef          double    __f64x8  __attribute__ ((__vector_size__ (64)));

typedef unsigned int       __m128b   __attribute__ ((__vector_size__ (16)));
typedef unsigned int       __m256b   __attribute__ ((__vector_size__ (32)));
typedef unsigned int       __m512b   __attribute__ ((__vector_size__ (64)));
typedef unsigned int       __m1024b  __attribute__ ((__vector_size__ (128)));

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hadd_b_512(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_hadd_d_512(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_hadd_f_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hadd_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hadd_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hadd_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hadd_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hand_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hand_b_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hand_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hand_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hand_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hand_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hand_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hand_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hmax_b_512(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_hmax_d_512(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_hmax_f_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hmax_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hmax_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmax_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hmax_w_512(a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmaxs_b(__s8x64 a)
{
    return (__s8x64) __builtin_tachy_hmaxs_b_512(a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmaxs_h(__s16x32 a)
{
    return (__s16x32) __builtin_tachy_hmaxs_h_512(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmaxs_l(__s64x8 a)
{
    return (__s64x8) __builtin_tachy_hmaxs_l_512(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmaxs_w(__s32x16 a)
{
    return (__s32x16) __builtin_tachy_hmaxs_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hmin_b_512(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_hmin_d_512(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_hmin_f_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hmin_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hmin_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmin_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hmin_w_512(a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmins_b(__s8x64 a)
{
    return (__s8x64) __builtin_tachy_hmins_b_512(a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmins_h(__s16x32 a)
{
    return (__s16x32) __builtin_tachy_hmins_h_512(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmins_l(__s64x8 a)
{
    return (__s64x8) __builtin_tachy_hmins_l_512(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmins_w(__s32x16 a)
{
    return (__s32x16) __builtin_tachy_hmins_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hmul_b_512(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_hmul_d_512(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_hmul_f_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hmul_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hmul_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hmul_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hmul_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hor_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hor_b_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hor_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hor_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hor_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hor_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hor_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hor_w_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hxor_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_hxor_b_512(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hxor_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_hxor_h_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hxor_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_hxor_l_512(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hxor_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_hxor_w_512(a);
}

extern __inline __m1024b
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mmadd_t(__m256b a, __m512b b, __m1024b c)
{
    return (__m1024b) __builtin_tachy_mmadd_tbqbq_512(a, b, c);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_b(__s8x16 a)
{
    return (__s8x16) __builtin_tachy_vabs1_128(a);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_b(__s8x32 a)
{
    return (__s8x32) __builtin_tachy_vabs1_256(a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_b(__s8x64 a)
{
    return (__s8x64) __builtin_tachy_vabs1_512(a);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_h(__s16x8 a)
{
    return (__s16x8) __builtin_tachy_vabs2_128(a);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_h(__s16x16 a)
{
    return (__s16x16) __builtin_tachy_vabs2_256(a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_h(__s16x32 a)
{
    return (__s16x32) __builtin_tachy_vabs2_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_w(__s32x4 a)
{
    return (__s32x4) __builtin_tachy_vabs4_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_w(__s32x8 a)
{
    return (__s32x8) __builtin_tachy_vabs4_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_w(__s32x16 a)
{
    return (__s32x16) __builtin_tachy_vabs4_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_l(__s64x2 a)
{
    return (__s64x2) __builtin_tachy_vabs8_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_l(__s64x4 a)
{
    return (__s64x4) __builtin_tachy_vabs8_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_l(__s64x8 a)
{
    return (__s64x8) __builtin_tachy_vabs8_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vabs_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vabs_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vabs_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_abs_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vabs_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_abs_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vabs_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_abs_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vabs_f_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vadd1_128(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_b_s(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vadd1_128((__u8x16) a, (__u8x16) b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vadd1_256(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_b_s(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vadd1_256((__u8x32) a, (__u8x32) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vadd1_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vadd1_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vadd2_128(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_h_s(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vadd2_128((__u16x8) a, (__u16x8) b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vadd2_256(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_h_s(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vadd2_256((__u16x16) a, (__u16x16) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vadd2_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vadd2_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vadd4_128(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_w_s(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vadd4_128((__u32x4) a, (__u32x4) b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vadd4_256(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_w_s(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vadd4_256((__u32x8) a, (__u32x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vadd4_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vadd4_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vadd8_128(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_l_s(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vadd8_128((__u64x2) a, (__u64x2) b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vadd8_256(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_l_s(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vadd8_256((__u64x4) a, (__u64x4) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vadd8_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vadd8_512((__u64x8) a, (__u64x8) b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vadd_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vadd_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vadd_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_add_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vadd_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_add_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vadd_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_add_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vadd_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addc_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vaddc1_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addc_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vaddc1_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addc_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vaddc1_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addc_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vaddc2_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addc_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vaddc2_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addc_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vaddc2_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addc_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vaddc4_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addc_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vaddc4_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addc_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vaddc4_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addc_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vaddc8_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addc_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vaddc8_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addc_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vaddc8_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vaddco_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vaddco_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vaddco_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vaddco_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vaddco_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vaddco_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vaddco_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vaddco_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vaddco_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vaddco_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vaddco_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vaddco_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco4_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vaddco_x_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco4_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vaddco_x_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco4_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vaddco_x_b_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco4_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vaddco_x_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco4_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vaddco_x_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco4_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vaddco_x_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco4_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vaddco_x_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco4_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vaddco_x_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco4_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vaddco_x_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addco4_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vaddco_x_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addco4_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vaddco_x_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addco4_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vaddco_x_w_512(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vaddp_b_512(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vaddp_d_512(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vaddp_f_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vaddp_h_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vaddp_k_b_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vaddp_k_b_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vaddp_k_b_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vaddp_k_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vaddp_k_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vaddp_k_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vaddp_k_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vaddp_k_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vaddp_k_f_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vaddp_k_h_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vaddp_k_h_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vaddp_k_h_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vaddp_k_l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vaddp_k_l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vaddp_k_l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addp_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vaddp_k_w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addp_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vaddp_k_w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vaddp_k_w_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vaddp_l_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addp2_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vaddp_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adds_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vadds1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adds_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vadds1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adds_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vadds1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adds_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vadds2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adds_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vadds2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adds_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vadds2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adds_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vadds4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adds_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vadds4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adds_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vadds4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adds_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vadds8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adds_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vadds8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adds_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vadds8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addu_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vaddu1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addu_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vaddu1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addu_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vaddu1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addu_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vaddu2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addu_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vaddu2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addu_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vaddu2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addu_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vaddu4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addu_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vaddu4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addu_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vaddu4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_addu_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vaddu8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_addu_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vaddu8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_addu_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vaddu8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adif_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vadif_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adif_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vadif_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adif_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vadif_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_adif_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vadif_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_adif_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vadif_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_adif_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vadif_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vage_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vage_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vage_b_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vage_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vage_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vage_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vage_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vage_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vage_f_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vage_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vage_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vage_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vage_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vage_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vage_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_age_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vage_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_age_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vage_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_age_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vage_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vagt_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vagt_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vagt_b_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vagt_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vagt_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vagt_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vagt_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vagt_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vagt_f_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vagt_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vagt_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vagt_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vagt_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vagt_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vagt_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_agt_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vagt_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_agt_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vagt_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_agt_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vagt_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vale_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vale_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vale_b_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vale_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vale_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vale_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vale_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vale_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vale_f_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vale_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vale_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vale_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vale_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vale_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vale_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ale_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vale_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ale_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vale_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ale_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vale_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_valt_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_valt_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_valt_b_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_valt_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_valt_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_valt_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_valt_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_valt_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_valt_f_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_valt_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_valt_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_valt_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_valt_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_valt_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_valt_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_alt_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_valt_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_alt_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_valt_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_alt_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_valt_w_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_amax_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vamax_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_amax_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vamax_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_amax_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vamax_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_amax_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vamax_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_amax_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vamax_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_amax_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vamax_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_amin_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vamin_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_amin_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vamin_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_amin_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vamin_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_amin_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vamin_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_amin_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vamin_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_amin_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vamin_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_and_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vand1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_and_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vand1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_and_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vand1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_and_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vand2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_and_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vand2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_and_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vand2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_and_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vand4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_and_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vand4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_and_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vand4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_and_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vand8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_and_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vand8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_and_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vand8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ann_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vann1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ann_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vann1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ann_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vann1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ann_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vann2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ann_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vann2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ann_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vann2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ann_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vann4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ann_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vann4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ann_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vann4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ann_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vann8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ann_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vann8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ann_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vann8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_avg_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vavg1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_avg_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vavg1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_avg_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vavg1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_avg_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vavg2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_avg_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vavg2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_avg_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vavg2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_avg_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vavg4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_avg_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vavg4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_avg_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vavg4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_avg_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vavg8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_avg_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vavg8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_avg_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vavg8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_clz_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vclz1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_clz_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vclz1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_clz_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vclz1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_clz_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vclz2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_clz_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vclz2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_clz_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vclz2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_clz_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vclz4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_clz_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vclz4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_clz_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vclz4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_clz_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vclz8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_clz_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vclz8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_clz_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vclz8_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ctz_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vctz1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ctz_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vctz1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ctz_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vctz1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ctz_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vctz2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ctz_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vctz2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ctz_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vctz2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ctz_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vctz4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ctz_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vctz4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ctz_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vctz4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ctz_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vctz8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ctz_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vctz8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ctz_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vctz8_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvt_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvt_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvt_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvt_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvt_d2w_256(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_f2d(__f32x4 a)
{
    return (__f64x4) __builtin_tachy_vcvt_f2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_f2d(__f32x8 a)
{
    return (__f64x8) __builtin_tachy_vcvt_f2d_512(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvt_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvt_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvt_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvt_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvt_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvt_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvt_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvt_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvt_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvt_l2f_256(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_w2d(__s32x4 a)
{
    return (__f64x4) __builtin_tachy_vcvt_w2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_w2d(__s32x8 a)
{
    return (__f64x8) __builtin_tachy_vcvt_w2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvt_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvt_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvt_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvt_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvt_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvt_w2f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvta_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvta_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvta_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvta_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvta_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvta_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvta_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvta_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvta_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvta_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvta_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvta_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvta_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvta_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvta_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvta_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvta_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvta_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvta_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvta_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvta_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvta_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvta_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvta_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvta_w2f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvtn_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtn_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtn_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtn_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtn_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtn_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtn_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtn_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtn_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtn_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtn_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtn_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvtn_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtn_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtn_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtn_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtn_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtn_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtn_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtn_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtn_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtn_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtn_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtn_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtn_w2f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvtp_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtp_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtp_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtp_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtp_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtp_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtp_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtp_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtp_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtp_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtp_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtp_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvtp_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtp_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtp_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtp_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtp_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtp_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtp_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtp_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtp_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtp_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtp_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtp_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtp_w2f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvtr_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtr_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtr_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtr_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtr_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtr_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtr_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtr_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtr_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtr_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtr_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtr_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvtr_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtr_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtr_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtr_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtr_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtr_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtr_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtr_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtr_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtr_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtr_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtr_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtr_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvts_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvts_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvts_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvts_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvts_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvts_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvts_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvts_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvts_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvts_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvts_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvts_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvts_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvts_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvts_l2f_256(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_w2d(__s32x4 a)
{
    return (__f64x4) __builtin_tachy_vcvts_w2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_w2d(__s32x8 a)
{
    return (__f64x8) __builtin_tachy_vcvts_w2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvts_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvts_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvts_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvts_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvts_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvts_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtsa_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsa_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsa_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsa_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsa_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsa_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsa_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsa_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsa_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsa_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsa_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsa_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtsa_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtsa_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtsa_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsa_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtsa_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsa_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsa_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsa_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsa_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsa_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsa_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsa_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtsa_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtsn_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsn_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsn_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsn_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsn_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsn_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsn_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsn_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsn_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsn_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsn_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsn_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtsn_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtsn_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtsn_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsn_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtsn_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsn_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsn_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsn_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsn_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsn_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsn_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsn_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtsn_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtsp_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsp_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsp_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsp_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsp_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsp_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsp_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsp_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsp_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsp_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsp_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsp_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtsp_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtsp_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtsp_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsp_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtsp_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsp_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsp_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsp_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsp_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsp_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsp_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsp_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtsp_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtsr_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsr_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsr_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsr_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsr_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsr_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsr_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsr_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsr_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsr_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsr_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsr_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtsr_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtsr_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtsr_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsr_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtsr_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsr_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsr_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsr_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsr_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsr_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsr_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsr_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtsr_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtst_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtst_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtst_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtst_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtst_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtst_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtst_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtst_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtst_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtst_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtst_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtst_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtst_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtst_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtst_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtst_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtst_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtst_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtst_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtst_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtst_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtst_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtst_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtst_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtst_w2f_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_d2l(__f64x2 a)
{
    return (__s64x2) __builtin_tachy_vcvtsx_d2l_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_d2l(__f64x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsx_d2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsx_d2l(__f64x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsx_d2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_d2w(__f64x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsx_d2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_d2w(__f64x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsx_d2w_256(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_f2l(__f32x4 a)
{
    return (__s64x4) __builtin_tachy_vcvtsx_f2l_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsx_f2l(__f32x8 a)
{
    return (__s64x8) __builtin_tachy_vcvtsx_f2l_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_f2w(__f32x4 a)
{
    return (__s32x4) __builtin_tachy_vcvtsx_f2w_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_f2w(__f32x8 a)
{
    return (__s32x8) __builtin_tachy_vcvtsx_f2w_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsx_f2w(__f32x16 a)
{
    return (__s32x16) __builtin_tachy_vcvtsx_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtsx_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtsx_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsx_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtsx_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsx_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsx_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtsx_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtsx_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtsx_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtsx_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtsx_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtsx_w2f_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_d2f(__f64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtt_d2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_d2f(__f64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtt_d2f_256(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvtt_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtt_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtt_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtt_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtt_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtt_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtt_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtt_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtt_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtt_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtt_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtt_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvtt_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtt_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtt_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtt_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtt_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtt_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtt_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtt_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtt_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtt_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtt_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtt_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtt_w2f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_d2l(__f64x2 a)
{
    return (__u64x2) __builtin_tachy_vcvtx_d2l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_d2l(__f64x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtx_d2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtx_d2l(__f64x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtx_d2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_d2w(__f64x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtx_d2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_d2w(__f64x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtx_d2w_256(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_f2l(__f32x4 a)
{
    return (__u64x4) __builtin_tachy_vcvtx_f2l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtx_f2l(__f32x8 a)
{
    return (__u64x8) __builtin_tachy_vcvtx_f2l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_f2w(__f32x4 a)
{
    return (__u32x4) __builtin_tachy_vcvtx_f2w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_f2w(__f32x8 a)
{
    return (__u32x8) __builtin_tachy_vcvtx_f2w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtx_f2w(__f32x16 a)
{
    return (__u32x16) __builtin_tachy_vcvtx_f2w_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_l2d(__s64x2 a)
{
    return (__f64x2) __builtin_tachy_vcvtx_l2d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_l2d(__s64x4 a)
{
    return (__f64x4) __builtin_tachy_vcvtx_l2d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtx_l2d(__s64x8 a)
{
    return (__f64x8) __builtin_tachy_vcvtx_l2d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_l2f(__s64x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtx_l2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_l2f(__s64x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtx_l2f_256(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_cvtx_w2f(__s32x4 a)
{
    return (__f32x4) __builtin_tachy_vcvtx_w2f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_cvtx_w2f(__s32x8 a)
{
    return (__f32x8) __builtin_tachy_vcvtx_w2f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cvtx_w2f(__s32x16 a)
{
    return (__f32x16) __builtin_tachy_vcvtx_w2f_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vdiv1_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vdiv2_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vdiv4_512(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vdiv8_512(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vdiv_d_512(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_div_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vdiv_f_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divs_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vdivs1_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divs_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vdivs2_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divs_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vdivs4_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divs_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vdivs8_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divsz_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vdivsz1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divsz_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vdivsz1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divsz_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vdivsz1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divsz_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vdivsz2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divsz_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vdivsz2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divsz_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vdivsz2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divsz_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vdivsz4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divsz_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vdivsz4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divsz_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vdivsz4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divsz_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vdivsz8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divsz_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vdivsz8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divsz_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vdivsz8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divz_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vdivz1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divz_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vdivz1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divz_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vdivz1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divz_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vdivz2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divz_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vdivz2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divz_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vdivz2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divz_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vdivz4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divz_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vdivz4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divz_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vdivz4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_divz_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vdivz8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_divz_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vdivz8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_divz_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vdivz8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_veq1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_veq1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_veq1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_veq2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_veq2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_veq2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_veq4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_veq4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_veq4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_veq8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_veq8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_veq8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_veq_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_veq_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_veq_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_veq_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_veq_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_veq_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_veq_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_veq_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_veq_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_eq_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_veq_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_eq_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_veq_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_eq_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_veq_u_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_erf_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_verf_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_erf_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_verf_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_erf_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_verf_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_erf_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_verf_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_erf_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_verf_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_erf_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_verf_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_exp2_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vexp2_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_exp2_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vexp2_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_exp2_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vexp2_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_exp2_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vexp2_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_exp2_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vexp2_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_exp2_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vexp2_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_expe_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vexpe_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_expe_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vexpe_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_expe_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vexpe_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_expe_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vexpe_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_expe_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vexpe_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_expe_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vexpe_f_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fillv_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vfill1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fillv_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vfill1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fillv_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vfill1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fillv_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vfill2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fillv_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vfill2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fillv_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vfill2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fillv_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vfill4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fillv_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vfill4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fillv_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vfill4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fillv_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vfill8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fillv_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vfill8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fillv_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vfill8_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fill_b(uint8_t a)
{
    return (__u8x16) __builtin_tachy_vfillr1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fill_b(uint8_t a)
{
    return (__u8x32) __builtin_tachy_vfillr1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fill_b(uint8_t a)
{
    return (__u8x64) __builtin_tachy_vfillr1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fill_h(uint16_t a)
{
    return (__u16x8) __builtin_tachy_vfillr2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fill_h(uint16_t a)
{
    return (__u16x16) __builtin_tachy_vfillr2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fill_h(uint16_t a)
{
    return (__u16x32) __builtin_tachy_vfillr2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fill_w(uint32_t a)
{
    return (__u32x4) __builtin_tachy_vfillr4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fill_w(uint32_t a)
{
    return (__u32x8) __builtin_tachy_vfillr4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fill_w(uint32_t a)
{
    return (__u32x16) __builtin_tachy_vfillr4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_fill_l(uint64_t a)
{
    return (__u64x2) __builtin_tachy_vfillr8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_fill_l(uint64_t a)
{
    return (__u64x4) __builtin_tachy_vfillr8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_fill_l(uint64_t a)
{
    return (__u64x8) __builtin_tachy_vfillr8_512(a);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vge1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vge1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vge1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vge2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vge2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vge2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vge4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vge4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vge4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vge8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vge8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vge8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vge_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vge_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vge_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vge_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vge_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vge_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_vge_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_vge_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_vge_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ge_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_vge_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ge_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_vge_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ge_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_vge_u_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_gt_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vgt_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_gt_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vgt_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_gt_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vgt_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_gt_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vgt_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_gt_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vgt_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_gt_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vgt_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_hs_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vhs1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_hs_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vhs1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hs_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vhs1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_hs_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vhs2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_hs_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vhs2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hs_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vhs2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_hs_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vhs4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_hs_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vhs4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hs_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vhs4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_hs_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vhs8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_hs_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vhs8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_hs_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vhs8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_insv_b(__u8x16 a, __u8x16 b, unsigned int c)
{
    return (__u8x16) __builtin_tachy_vins_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insv_b(__u8x32 a, __u8x32 b, unsigned int c)
{
    return (__u8x32) __builtin_tachy_vins_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insv_b(__u8x64 a, __u8x64 b, unsigned int c)
{
    return (__u8x64) __builtin_tachy_vins_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_insv_h(__u16x8 a, __u16x8 b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vins_h_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insv_h(__u16x16 a, __u16x16 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vins_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insv_h(__u16x32 a, __u16x32 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vins_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_insv_l(__u64x2 a, __u64x2 b, unsigned int c)
{
    return (__u64x2) __builtin_tachy_vins_l_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insv_l(__u64x4 a, __u64x4 b, unsigned int c)
{
    return (__u64x4) __builtin_tachy_vins_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insv_l(__u64x8 a, __u64x8 b, unsigned int c)
{
    return (__u64x8) __builtin_tachy_vins_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ins_b(__u8x16 a, unsigned int b, unsigned int c)
{
    return (__u8x16) __builtin_tachy_vins_r_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ins_b(__u8x32 a, unsigned int b, unsigned int c)
{
    return (__u8x32) __builtin_tachy_vins_r_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ins_b(__u8x64 a, unsigned int b, unsigned int c)
{
    return (__u8x64) __builtin_tachy_vins_r_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ins_h(__u16x8 a, unsigned int b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vins_r_h_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ins_h(__u16x16 a, unsigned int b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vins_r_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ins_h(__u16x32 a, unsigned int b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vins_r_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ins_l(__u64x2 a, unsigned int b, unsigned int c)
{
    return (__u64x2) __builtin_tachy_vins_r_l_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ins_l(__u64x4 a, unsigned int b, unsigned int c)
{
    return (__u64x4) __builtin_tachy_vins_r_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ins_l(__u64x8 a, unsigned int b, unsigned int c)
{
    return (__u64x8) __builtin_tachy_vins_r_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ins_w(__u32x4 a, unsigned int b, unsigned int c)
{
    return (__u32x4) __builtin_tachy_vins_r_w_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ins_w(__u32x8 a, unsigned int b, unsigned int c)
{
    return (__u32x8) __builtin_tachy_vins_r_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ins_w(__u32x16 a, unsigned int b, unsigned int c)
{
    return (__u32x16) __builtin_tachy_vins_r_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_insv_w(__u32x4 a, __u32x4 b, unsigned int c)
{
    return (__u32x4) __builtin_tachy_vins_w_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insv_w(__u32x8 a, __u32x8 b, unsigned int c)
{
    return (__u32x8) __builtin_tachy_vins_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insv_w(__u32x16 a, __u32x16 b, unsigned int c)
{
    return (__u32x16) __builtin_tachy_vins_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insx_b(__u8x32 a, __u8x16 b, unsigned int c)
{
    return (__u8x32) __builtin_tachy_vinsx_x_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insx_b(__u8x64 a, __u8x16 b, unsigned int c)
{
    return (__u8x64) __builtin_tachy_vinsx_x_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insx_h(__u16x16 a, __u16x8 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vinsx_x_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insx_h(__u16x32 a, __u16x8 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vinsx_x_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insx_l(__u64x4 a, __u64x2 b, unsigned int c)
{
    return (__u64x4) __builtin_tachy_vinsx_x_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insx_l(__u64x8 a, __u64x2 b, unsigned int c)
{
    return (__u64x8) __builtin_tachy_vinsx_x_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_insx_w(__u32x8 a, __u32x4 b, unsigned int c)
{
    return (__u32x8) __builtin_tachy_vinsx_x_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insx_w(__u32x16 a, __u32x4 b, unsigned int c)
{
    return (__u32x16) __builtin_tachy_vinsx_x_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insxx_b(__u8x64 a, __u8x32 b, unsigned int c)
{
    return (__u8x64) __builtin_tachy_vinsxx_x_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insxx_h(__u16x32 a, __u16x16 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vinsxx_x_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insxx_l(__u64x8 a, __u64x4 b, unsigned int c)
{
    return (__u64x8) __builtin_tachy_vinsxx_x_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_insxx_w(__u32x16 a, __u32x8 b, unsigned int c)
{
    return (__u32x16) __builtin_tachy_vinsxx_x_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_le_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vle_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_le_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vle_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_le_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vle_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_le_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vle_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_le_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vle_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_le_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vle_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lo_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vlo1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lo_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vlo1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lo_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vlo1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lo_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vlo2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lo_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vlo2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lo_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vlo2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lo_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vlo4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lo_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vlo4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lo_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vlo4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lo_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vlo8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lo_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vlo8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lo_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vlo8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_log2_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vlog2_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_log2_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vlog2_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_log2_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vlog2_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_log2_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vlog2_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_log2_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vlog2_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_log2_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vlog2_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_loge_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vloge_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_loge_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vloge_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_loge_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vloge_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_loge_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vloge_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_loge_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vloge_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_loge_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vloge_f_512(a);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vlt1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vlt1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vlt1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vlt2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vlt2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vlt2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vlt4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vlt4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vlt4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vlt8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vlt8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vlt8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vlt_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vlt_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vlt_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vlt_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vlt_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vlt_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_vlt_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_vlt_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_vlt_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_lt_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_vlt_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_lt_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_vlt_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_lt_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_vlt_u_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmadd1_128(a, b, c);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_b_s(__s8x16 a, __s8x16 b, __s8x16 c)
{
    return (__s8x16) __builtin_tachy_vmadd1_128((__u8x16) a, (__u8x16) b, (__u8x16) c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmadd1_256(a, b, c);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_b_s(__s8x32 a, __s8x32 b, __s8x32 c)
{
    return (__s8x32) __builtin_tachy_vmadd1_256((__u8x32) a, (__u8x32) b, (__u8x32) c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmadd1_512(a, b, c);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_b_s(__s8x64 a, __s8x64 b, __s8x64 c)
{
    return (__s8x64) __builtin_tachy_vmadd1_512((__u8x64) a, (__u8x64) b, (__u8x64) c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmadd2_128(a, b, c);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_h_s(__s16x8 a, __s16x8 b, __s16x8 c)
{
    return (__s16x8) __builtin_tachy_vmadd2_128((__u16x8) a, (__u16x8) b, (__u16x8) c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmadd2_256(a, b, c);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_h_s(__s16x16 a, __s16x16 b, __s16x16 c)
{
    return (__s16x16) __builtin_tachy_vmadd2_256((__u16x16) a, (__u16x16) b, (__u16x16) c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmadd2_512(a, b, c);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_h_s(__s16x32 a, __s16x32 b, __s16x32 c)
{
    return (__s16x32) __builtin_tachy_vmadd2_512((__u16x32) a, (__u16x32) b, (__u16x32) c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmadd4_128(a, b, c);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_w_s(__s32x4 a, __s32x4 b, __s32x4 c)
{
    return (__s32x4) __builtin_tachy_vmadd4_128((__u32x4) a, (__u32x4) b, (__u32x4) c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmadd4_256(a, b, c);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_w_s(__s32x8 a, __s32x8 b, __s32x8 c)
{
    return (__s32x8) __builtin_tachy_vmadd4_256((__u32x8) a, (__u32x8) b, (__u32x8) c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmadd4_512(a, b, c);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_w_s(__s32x16 a, __s32x16 b, __s32x16 c)
{
    return (__s32x16) __builtin_tachy_vmadd4_512((__u32x16) a, (__u32x16) b, (__u32x16) c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmadd8_128(a, b, c);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_l_s(__s64x2 a, __s64x2 b, __s64x2 c)
{
    return (__s64x2) __builtin_tachy_vmadd8_128((__u64x2) a, (__u64x2) b, (__u64x2) c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmadd8_256(a, b, c);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_l_s(__s64x4 a, __s64x4 b, __s64x4 c)
{
    return (__s64x4) __builtin_tachy_vmadd8_256((__u64x4) a, (__u64x4) b, (__u64x4) c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmadd8_512(a, b, c);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_l_s(__s64x8 a, __s64x8 b, __s64x8 c)
{
    return (__s64x8) __builtin_tachy_vmadd8_512((__u64x8) a, (__u64x8) b, (__u64x8) c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmadd_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmadd_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmadd_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madd_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmadd_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madd_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmadd_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madd_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmadd_f_512(a, b, c);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madds_b(__s8x16 a, __s8x16 b, __s8x16 c)
{
    return (__s8x16) __builtin_tachy_vmadds1_128(a, b, c);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madds_b(__s8x32 a, __s8x32 b, __s8x32 c)
{
    return (__s8x32) __builtin_tachy_vmadds1_256(a, b, c);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madds_b(__s8x64 a, __s8x64 b, __s8x64 c)
{
    return (__s8x64) __builtin_tachy_vmadds1_512(a, b, c);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madds_h(__s16x8 a, __s16x8 b, __s16x8 c)
{
    return (__s16x8) __builtin_tachy_vmadds2_128(a, b, c);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madds_h(__s16x16 a, __s16x16 b, __s16x16 c)
{
    return (__s16x16) __builtin_tachy_vmadds2_256(a, b, c);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madds_h(__s16x32 a, __s16x32 b, __s16x32 c)
{
    return (__s16x32) __builtin_tachy_vmadds2_512(a, b, c);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madds_w(__s32x4 a, __s32x4 b, __s32x4 c)
{
    return (__s32x4) __builtin_tachy_vmadds4_128(a, b, c);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madds_w(__s32x8 a, __s32x8 b, __s32x8 c)
{
    return (__s32x8) __builtin_tachy_vmadds4_256(a, b, c);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madds_w(__s32x16 a, __s32x16 b, __s32x16 c)
{
    return (__s32x16) __builtin_tachy_vmadds4_512(a, b, c);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_madds_l(__s64x2 a, __s64x2 b, __s64x2 c)
{
    return (__s64x2) __builtin_tachy_vmadds8_128(a, b, c);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_madds_l(__s64x4 a, __s64x4 b, __s64x4 c)
{
    return (__s64x4) __builtin_tachy_vmadds8_256(a, b, c);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_madds_l(__s64x8 a, __s64x8 b, __s64x8 c)
{
    return (__s64x8) __builtin_tachy_vmadds8_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maddu_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmaddu1_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maddu_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmaddu1_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maddu_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmaddu1_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maddu_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmaddu2_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maddu_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmaddu2_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maddu_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmaddu2_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maddu_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmaddu4_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maddu_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmaddu4_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maddu_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmaddu4_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maddu_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmaddu8_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maddu_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmaddu8_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maddu_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmaddu8_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mas_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmas_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mas_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmas_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mas_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmas_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mas_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmas_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mas_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmas_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mas_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmas_f_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmax1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmax1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmax1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmax2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmax2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmax2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmax4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmax4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmax4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmax8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmax8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmax8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vmax_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vmax_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vmax_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_max_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vmax_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_max_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vmax_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_max_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vmax_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxn_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vmaxn_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxn_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vmaxn_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxn_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vmaxn_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxn_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vmaxn_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxn_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vmaxn_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxn_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vmaxn_f_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmaxp_b_512(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vmaxp_d_512(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vmaxp_f_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmaxp_h_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vmaxp_k_b_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vmaxp_k_b_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vmaxp_k_b_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vmaxp_k_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vmaxp_k_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vmaxp_k_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vmaxp_k_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vmaxp_k_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vmaxp_k_f_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vmaxp_k_h_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vmaxp_k_h_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vmaxp_k_h_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vmaxp_k_l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vmaxp_k_l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vmaxp_k_l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxp_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vmaxp_k_w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxp_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vmaxp_k_w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vmaxp_k_w_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmaxp_l_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxp2_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmaxp_w_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxs_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vmaxs1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxs_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vmaxs1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxs_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vmaxs1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxs_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vmaxs2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxs_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vmaxs2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxs_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vmaxs2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxs_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vmaxs4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxs_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vmaxs4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxs_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vmaxs4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxs_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vmaxs8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxs_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vmaxs8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxs_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vmaxs8_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp2_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmaxsp_b_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp2_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmaxsp_h_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxsp_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vmaxsp_k_b_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxsp_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vmaxsp_k_b_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vmaxsp_k_b_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxsp_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vmaxsp_k_h_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxsp_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vmaxsp_k_h_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vmaxsp_k_h_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxsp_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vmaxsp_k_l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxsp_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vmaxsp_k_l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vmaxsp_k_l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_maxsp_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vmaxsp_k_w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_maxsp_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vmaxsp_k_w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vmaxsp_k_w_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp2_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmaxsp_l_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_maxsp2_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmaxsp_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmeq_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmeq_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmeq_b_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmeq_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmeq_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmeq_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmeq_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmeq_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmeq_f_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmeq_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmeq_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmeq_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmeq_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmeq_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmeq_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_meq_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmeq_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_meq_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmeq_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_meq_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmeq_w_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mge_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmge_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mge_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmge_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mge_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmge_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mge_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmge_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mge_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmge_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mge_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmge_f_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mgt_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmgt_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mgt_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmgt_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mgt_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmgt_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mgt_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmgt_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mgt_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmgt_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mgt_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmgt_f_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmin1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmin1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmin1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmin2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmin2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmin2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmin4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmin4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmin4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmin8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmin8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmin8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vmin_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vmin_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vmin_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_min_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vmin_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_min_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vmin_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_min_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vmin_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minn_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vminn_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minn_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vminn_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minn_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vminn_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minn_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vminn_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minn_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vminn_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minn_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vminn_f_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vminp_b_512(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vminp_d_512(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vminp_f_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vminp_h_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vminp_k_b_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vminp_k_b_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vminp_k_b_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vminp_k_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vminp_k_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vminp_k_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vminp_k_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vminp_k_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vminp_k_f_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vminp_k_h_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vminp_k_h_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vminp_k_h_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vminp_k_l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vminp_k_l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vminp_k_l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minp_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vminp_k_w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minp_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vminp_k_w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vminp_k_w_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vminp_l_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minp2_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vminp_w_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mins_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vmins1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mins_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vmins1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mins_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vmins1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mins_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vmins2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mins_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vmins2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mins_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vmins2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mins_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vmins4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mins_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vmins4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mins_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vmins4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mins_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vmins8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mins_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vmins8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mins_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vmins8_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp2_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vminsp_b_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp2_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vminsp_h_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minsp_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vminsp_k_b_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minsp_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vminsp_k_b_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vminsp_k_b_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minsp_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vminsp_k_h_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minsp_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vminsp_k_h_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vminsp_k_h_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minsp_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vminsp_k_l_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minsp_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vminsp_k_l_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vminsp_k_l_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_minsp_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vminsp_k_w_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_minsp_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vminsp_k_w_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vminsp_k_w_512(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp2_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vminsp_l_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_minsp2_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vminsp_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmle_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmle_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmle_b_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmle_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmle_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmle_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmle_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmle_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmle_f_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmle_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmle_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmle_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmle_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmle_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmle_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mle_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmle_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mle_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmle_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mle_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmle_w_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmlt_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmlt_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmlt_b_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmlt_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmlt_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmlt_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmlt_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmlt_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmlt_f_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmlt_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmlt_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmlt_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmlt_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmlt_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmlt_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mlt_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmlt_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mlt_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmlt_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mlt_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmlt_w_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mne_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmne_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mne_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmne_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mne_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmne_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mne_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmne_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mne_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmne_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mne_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmne_f_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mov_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vmov1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mov_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vmov1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mov_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vmov1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mov_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vmov2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mov_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vmov2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mov_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vmov2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mov_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vmov4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mov_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vmov4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mov_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vmov4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mov_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vmov8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mov_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vmov8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mov_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vmov8_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movh_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmovh_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movh_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmovh_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movh_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmovh_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movh_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmovh_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movh_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmovh_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movh_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmovh_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movh_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmovh_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movh_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmovh_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movh_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmovh_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movh_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmovh_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movh_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmovh_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movh_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmovh_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movhl_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmovhl_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movhl_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmovhl_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movhl_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmovhl_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movhl_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmovhl_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movhl_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmovhl_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movhl_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmovhl_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movhl_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmovhl_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movhl_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmovhl_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movhl_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmovhl_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movhl_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmovhl_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movhl_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmovhl_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movhl_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmovhl_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movl_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmovl_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movl_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmovl_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movl_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmovl_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movl_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmovl_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movl_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmovl_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movl_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmovl_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movl_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmovl_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movl_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmovl_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movl_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmovl_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movl_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmovl_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movl_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmovl_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movl_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmovl_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movlh_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmovlh_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movlh_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmovlh_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movlh_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmovlh_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movlh_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmovlh_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movlh_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmovlh_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movlh_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmovlh_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movlh_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmovlh_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movlh_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmovlh_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movlh_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmovlh_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_movlh_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmovlh_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_movlh_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmovlh_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_movlh_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmovlh_w_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msa_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmsa_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msa_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmsa_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msa_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmsa_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msa_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmsa_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msa_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmsa_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msa_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmsa_f_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vmsub_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vmsub_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vmsub_b_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vmsub_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vmsub_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vmsub_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vmsub_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vmsub_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vmsub_f_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vmsub_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vmsub_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vmsub_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vmsub_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vmsub_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vmsub_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_msub_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vmsub_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_msub_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vmsub_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_msub_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vmsub_w_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmul1_128(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_b_s(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vmul1_128((__u8x16) a, (__u8x16) b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmul1_256(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_b_s(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vmul1_256((__u8x32) a, (__u8x32) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmul1_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vmul1_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmul2_128(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_h_s(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vmul2_128((__u16x8) a, (__u16x8) b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmul2_256(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_h_s(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vmul2_256((__u16x16) a, (__u16x16) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmul2_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vmul2_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmul4_128(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_w_s(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vmul4_128((__u32x4) a, (__u32x4) b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmul4_256(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_w_s(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vmul4_256((__u32x8) a, (__u32x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmul4_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vmul4_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmul8_128(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_l_s(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vmul8_128((__u64x2) a, (__u64x2) b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmul8_256(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_l_s(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vmul8_256((__u64x4) a, (__u64x4) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmul8_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vmul8_512((__u64x8) a, (__u64x8) b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vmul_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vmul_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vmul_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mul_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vmul_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mul_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vmul_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mul_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vmul_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_umulhs_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmulh1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_umulhs_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmulh1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_umulhs_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmulh1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_umulhs_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmulh2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_umulhs_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmulh2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_umulhs_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmulh2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_umulhs_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmulh4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_umulhs_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmulh4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_umulhs_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmulh4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_umulhs_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmulh8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_umulhs_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmulh8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_umulhs_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmulh8_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_smulhs_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vmulhs1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_smulhs_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vmulhs1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_smulhs_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vmulhs1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_smulhs_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vmulhs2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_smulhs_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vmulhs2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_smulhs_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vmulhs2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_smulhs_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vmulhs4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_smulhs_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vmulhs4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_smulhs_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vmulhs4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_smulhs_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vmulhs8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_smulhs_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vmulhs8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_smulhs_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vmulhs8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_muls_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmuls_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_muls_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmuls_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_muls_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmuls_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_muls_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmuls_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_muls_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmuls_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_muls_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmuls_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_muls_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmuls_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_muls_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmuls_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_muls_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmuls_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_muls_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmuls_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_muls_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmuls_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_muls_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmuls_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mulu_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vmulu_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mulu_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vmulu_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mulu_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vmulu_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mulu_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vmulu_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mulu_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vmulu_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mulu_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vmulu_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mulu_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vmulu_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mulu_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vmulu_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mulu_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vmulu_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_mulu_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vmulu_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_mulu_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vmulu_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_mulu_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vmulu_w_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulx_h(__u8x16 a, __u8x16 b)
{
    return (__u16x8) __builtin_tachy_vmulx2_1_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulx_h(__u8x32 a, __u8x32 b)
{
    return (__u16x16) __builtin_tachy_vmulx2_1_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulx_h(__u8x64 a, __u8x64 b)
{
    return (__u16x32) __builtin_tachy_vmulx2_1_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulx_w(__u16x8 a, __u16x8 b)
{
    return (__u32x4) __builtin_tachy_vmulx4_2_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulx_w(__u16x16 a, __u16x16 b)
{
    return (__u32x8) __builtin_tachy_vmulx4_2_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulx_w(__u16x32 a, __u16x32 b)
{
    return (__u32x16) __builtin_tachy_vmulx4_2_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulx_l(__u32x4 a, __u32x4 b)
{
    return (__u64x2) __builtin_tachy_vmulx8_4_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulx_l(__u32x8 a, __u32x8 b)
{
    return (__u64x4) __builtin_tachy_vmulx8_4_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulx_l(__u32x16 a, __u32x16 b)
{
    return (__u64x8) __builtin_tachy_vmulx8_4_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulxs_h(__s8x16 a, __s8x16 b)
{
    return (__s16x8) __builtin_tachy_vmulxs2_1_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulxs_h(__s8x32 a, __s8x32 b)
{
    return (__s16x16) __builtin_tachy_vmulxs2_1_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulxs_h(__s8x64 a, __s8x64 b)
{
    return (__s16x32) __builtin_tachy_vmulxs2_1_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulxs_w(__s16x8 a, __s16x8 b)
{
    return (__s32x4) __builtin_tachy_vmulxs4_2_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulxs_w(__s16x16 a, __s16x16 b)
{
    return (__s32x8) __builtin_tachy_vmulxs4_2_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulxs_w(__s16x32 a, __s16x32 b)
{
    return (__s32x16) __builtin_tachy_vmulxs4_2_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vmulxs_l(__s32x4 a, __s32x4 b)
{
    return (__s64x2) __builtin_tachy_vmulxs8_4_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vmulxs_l(__s32x8 a, __s32x8 b)
{
    return (__s64x4) __builtin_tachy_vmulxs8_4_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vmulxs_l(__s32x16 a, __s32x16 b)
{
    return (__s64x8) __builtin_tachy_vmulxs8_4_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_b(__s8x16 a)
{
    return (__s8x16) __builtin_tachy_vnabs1_128(a);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_b(__s8x32 a)
{
    return (__s8x32) __builtin_tachy_vnabs1_256(a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_b(__s8x64 a)
{
    return (__s8x64) __builtin_tachy_vnabs1_512(a);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_h(__s16x8 a)
{
    return (__s16x8) __builtin_tachy_vnabs2_128(a);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_h(__s16x16 a)
{
    return (__s16x16) __builtin_tachy_vnabs2_256(a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_h(__s16x32 a)
{
    return (__s16x32) __builtin_tachy_vnabs2_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_w(__s32x4 a)
{
    return (__s32x4) __builtin_tachy_vnabs4_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_w(__s32x8 a)
{
    return (__s32x8) __builtin_tachy_vnabs4_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_w(__s32x16 a)
{
    return (__s32x16) __builtin_tachy_vnabs4_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_l(__s64x2 a)
{
    return (__s64x2) __builtin_tachy_vnabs8_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_l(__s64x4 a)
{
    return (__s64x4) __builtin_tachy_vnabs8_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_l(__s64x8 a)
{
    return (__s64x8) __builtin_tachy_vnabs8_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vnabs_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vnabs_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vnabs_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nabs_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vnabs_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nabs_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vnabs_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nabs_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vnabs_f_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vne1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vne1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vne1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vne2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vne2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vne2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vne4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vne4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vne4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vne8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vne8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vne8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vne_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vne_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vne_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vne_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vne_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vne_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_vne_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_vne_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_vne_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ne_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_vne_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ne_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_vne_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ne_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_vne_u_f_512(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_b(__s8x16 a)
{
    return (__s8x16) __builtin_tachy_vneg1_128(a);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_b(__s8x32 a)
{
    return (__s8x32) __builtin_tachy_vneg1_256(a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_b(__s8x64 a)
{
    return (__s8x64) __builtin_tachy_vneg1_512(a);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_h(__s16x8 a)
{
    return (__s16x8) __builtin_tachy_vneg2_128(a);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_h(__s16x16 a)
{
    return (__s16x16) __builtin_tachy_vneg2_256(a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_h(__s16x32 a)
{
    return (__s16x32) __builtin_tachy_vneg2_512(a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_w(__s32x4 a)
{
    return (__s32x4) __builtin_tachy_vneg4_128(a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_w(__s32x8 a)
{
    return (__s32x8) __builtin_tachy_vneg4_256(a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_w(__s32x16 a)
{
    return (__s32x16) __builtin_tachy_vneg4_512(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_l(__s64x2 a)
{
    return (__s64x2) __builtin_tachy_vneg8_128(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_l(__s64x4 a)
{
    return (__s64x4) __builtin_tachy_vneg8_256(a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_l(__s64x8 a)
{
    return (__s64x8) __builtin_tachy_vneg8_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vneg_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vneg_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vneg_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_neg_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vneg_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_neg_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vneg_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_neg_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vneg_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmadd_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vnmadd_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmadd_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vnmadd_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmadd_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vnmadd_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmadd_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vnmadd_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmadd_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vnmadd_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmadd_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vnmadd_f_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmsub_d(__f64x2 a, __f64x2 b, __f64x2 c)
{
    return (__f64x2) __builtin_tachy_vnmsub_d_128(a, b, c);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmsub_d(__f64x4 a, __f64x4 b, __f64x4 c)
{
    return (__f64x4) __builtin_tachy_vnmsub_d_256(a, b, c);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmsub_d(__f64x8 a, __f64x8 b, __f64x8 c)
{
    return (__f64x8) __builtin_tachy_vnmsub_d_512(a, b, c);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmsub_f(__f32x4 a, __f32x4 b, __f32x4 c)
{
    return (__f32x4) __builtin_tachy_vnmsub_f_128(a, b, c);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmsub_f(__f32x8 a, __f32x8 b, __f32x8 c)
{
    return (__f32x8) __builtin_tachy_vnmsub_f_256(a, b, c);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmsub_f(__f32x16 a, __f32x16 b, __f32x16 c)
{
    return (__f32x16) __builtin_tachy_vnmsub_f_512(a, b, c);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmul_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vnmul_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmul_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vnmul_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmul_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vnmul_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_nmul_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vnmul_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_nmul_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vnmul_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_nmul_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vnmul_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_not_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vnot1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_not_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vnot1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_not_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vnot1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_not_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vnot2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_not_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vnot2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_not_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vnot2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_not_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vnot4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_not_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vnot4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_not_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vnot4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_not_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vnot8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_not_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vnot8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_not_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vnot8_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_or_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vor1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_or_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vor1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_or_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vor1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_or_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vor2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_or_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vor2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_or_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vor2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_or_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vor4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_or_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vor4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_or_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vor4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_or_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vor8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_or_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vor8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_or_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vor8_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ord_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vord_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ord_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vord_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ord_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vord_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ord_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vord_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ord_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vord_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ord_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vord_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ord_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_vord_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ord_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_vord_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ord_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_vord_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ord_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_vord_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ord_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_vord_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ord_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_vord_u_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_par_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vpar1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_par_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vpar1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_par_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vpar1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_par_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vpar2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_par_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vpar2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_par_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vpar2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_par_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vpar4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_par_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vpar4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_par_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vpar4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_par_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vpar8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_par_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vpar8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_par_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vpar8_512(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_b(__u16x32 a, __u16x32 b)
{
    return (__u8x64) __builtin_tachy_vpck_b_h_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_b_s(__s16x32 a, __s16x32 b)
{
    return (__s8x64) __builtin_tachy_vpck_b_h_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_h(__u32x16 a, __u32x16 b)
{
    return (__u16x32) __builtin_tachy_vpck_h_w_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_h_s(__s32x16 a, __s32x16 b)
{
    return (__s16x32) __builtin_tachy_vpck_h_w_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_w(__u64x8 a, __u64x8 b)
{
    return (__u32x16) __builtin_tachy_vpck_w_l_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pck_w_s(__s64x8 a, __s64x8 b)
{
    return (__s32x16) __builtin_tachy_vpck_w_l_512((__u64x8) a, (__u64x8) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcks_b(__u16x32 a, __u16x32 b)
{
    return (__u8x64) __builtin_tachy_vpcks_b_h_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcks_h(__u32x16 a, __u32x16 b)
{
    return (__u16x32) __builtin_tachy_vpcks_h_w_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcks_w(__u64x8 a, __u64x8 b)
{
    return (__u32x16) __builtin_tachy_vpcks_w_l_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcku_b(__u16x32 a, __u16x32 b)
{
    return (__u8x64) __builtin_tachy_vpcku_b_h_512(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcku_h(__u32x16 a, __u32x16 b)
{
    return (__u16x32) __builtin_tachy_vpcku_h_w_512(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_pcku_w(__u64x8 a, __u64x8 b)
{
    return (__u32x16) __builtin_tachy_vpcku_w_l_512(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vperm_b_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vperm_b_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vperm_h_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vperm_h_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vperm_l_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vperm_l_512((__u64x8) a, (__u64x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vperm_w_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perm_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vperm_w_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vperm_x_b_128(a, b, c);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_b_s(__s8x16 a, __s8x16 b, __s8x16 c)
{
    return (__s8x16) __builtin_tachy_vperm_x_b_128((__u8x16) a, (__u8x16) b, (__u8x16) c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vperm_x_b_256(a, b, c);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_b_s(__s8x32 a, __s8x32 b, __s8x32 c)
{
    return (__s8x32) __builtin_tachy_vperm_x_b_256((__u8x32) a, (__u8x32) b, (__u8x32) c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vperm_x_b_512(a, b, c);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_b_s(__s8x64 a, __s8x64 b, __s8x64 c)
{
    return (__s8x64) __builtin_tachy_vperm_x_b_512((__u8x64) a, (__u8x64) b, (__u8x64) c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vperm_x_h_128(a, b, c);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_h_s(__s16x8 a, __s16x8 b, __s16x8 c)
{
    return (__s16x8) __builtin_tachy_vperm_x_h_128((__u16x8) a, (__u16x8) b, (__u16x8) c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vperm_x_h_256(a, b, c);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_h_s(__s16x16 a, __s16x16 b, __s16x16 c)
{
    return (__s16x16) __builtin_tachy_vperm_x_h_256((__u16x16) a, (__u16x16) b, (__u16x16) c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vperm_x_h_512(a, b, c);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_h_s(__s16x32 a, __s16x32 b, __s16x32 c)
{
    return (__s16x32) __builtin_tachy_vperm_x_h_512((__u16x32) a, (__u16x32) b, (__u16x32) c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vperm_x_l_128(a, b, c);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_l_s(__s64x2 a, __s64x2 b, __s64x2 c)
{
    return (__s64x2) __builtin_tachy_vperm_x_l_128((__u64x2) a, (__u64x2) b, (__u64x2) c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vperm_x_l_256(a, b, c);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_l_s(__s64x4 a, __s64x4 b, __s64x4 c)
{
    return (__s64x4) __builtin_tachy_vperm_x_l_256((__u64x4) a, (__u64x4) b, (__u64x4) c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vperm_x_l_512(a, b, c);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_l_s(__s64x8 a, __s64x8 b, __s64x8 c)
{
    return (__s64x8) __builtin_tachy_vperm_x_l_512((__u64x8) a, (__u64x8) b, (__u64x8) c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vperm_x_w_128(a, b, c);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permx_w_s(__s32x4 a, __s32x4 b, __s32x4 c)
{
    return (__s32x4) __builtin_tachy_vperm_x_w_128((__u32x4) a, (__u32x4) b, (__u32x4) c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vperm_x_w_256(a, b, c);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permx_w_s(__s32x8 a, __s32x8 b, __s32x8 c)
{
    return (__s32x8) __builtin_tachy_vperm_x_w_256((__u32x8) a, (__u32x8) b, (__u32x8) c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vperm_x_w_512(a, b, c);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permx_w_s(__s32x16 a, __s32x16 b, __s32x16 c)
{
    return (__s32x16) __builtin_tachy_vperm_x_w_512((__u32x16) a, (__u32x16) b, (__u32x16) c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vperml_b_128(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_b_s(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vperml_b_128((__u8x16) a, (__u8x16) b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vperml_b_256(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_b_s(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vperml_b_256((__u8x32) a, (__u8x32) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vperml_b_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vperml_b_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vperml_h_128(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_h_s(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vperml_h_128((__u16x8) a, (__u16x8) b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vperml_h_256(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_h_s(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vperml_h_256((__u16x16) a, (__u16x16) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vperml_h_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vperml_h_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vperml_l_128(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_l_s(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vperml_l_128((__u64x2) a, (__u64x2) b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vperml_l_256(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_l_s(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vperml_l_256((__u64x4) a, (__u64x4) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vperml_l_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vperml_l_512((__u64x8) a, (__u64x8) b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vperml_w_128(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_perml_w_s(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vperml_w_128((__u32x4) a, (__u32x4) b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vperml_w_256(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_perml_w_s(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vperml_w_256((__u32x8) a, (__u32x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vperml_w_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_perml_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vperml_w_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vpermxx_b_128(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_b_s(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vpermxx_b_128((__u8x16) a, (__u8x16) b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vpermxx_b_256(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_b_s(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vpermxx_b_256((__u8x32) a, (__u8x32) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vpermxx_b_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vpermxx_b_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vpermxx_h_128(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_h_s(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vpermxx_h_128((__u16x8) a, (__u16x8) b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vpermxx_h_256(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_h_s(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vpermxx_h_256((__u16x16) a, (__u16x16) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vpermxx_h_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vpermxx_h_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vpermxx_l_128(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_l_s(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vpermxx_l_128((__u64x2) a, (__u64x2) b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vpermxx_l_256(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_l_s(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vpermxx_l_256((__u64x4) a, (__u64x4) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vpermxx_l_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vpermxx_l_512((__u64x8) a, (__u64x8) b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vpermxx_w_128(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_permxx_w_s(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vpermxx_w_128((__u32x4) a, (__u32x4) b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vpermxx_w_256(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_permxx_w_s(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vpermxx_w_256((__u32x8) a, (__u32x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vpermxx_w_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_permxx_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vpermxx_w_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_popc_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vpopc1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_popc_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vpopc1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_popc_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vpopc1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_popc_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vpopc2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_popc_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vpopc2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_popc_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vpopc2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_popc_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vpopc4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_popc_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vpopc4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_popc_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vpopc4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_popc_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vpopc8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_popc_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vpopc8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_popc_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vpopc8_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprod_h(__u8x16 a, __u8x16 b)
{
    return (__u16x8) __builtin_tachy_vprod2_1_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprod_h(__u8x32 a, __u8x32 b)
{
    return (__u16x16) __builtin_tachy_vprod2_1_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprod_h(__u8x64 a, __u8x64 b)
{
    return (__u16x32) __builtin_tachy_vprod2_1_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprod_w(__u16x8 a, __u16x8 b)
{
    return (__u32x4) __builtin_tachy_vprod4_2_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprod_w(__u16x16 a, __u16x16 b)
{
    return (__u32x8) __builtin_tachy_vprod4_2_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprod_w(__u16x32 a, __u16x32 b)
{
    return (__u32x16) __builtin_tachy_vprod4_2_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprod_l(__u32x4 a, __u32x4 b)
{
    return (__u64x2) __builtin_tachy_vprod8_4_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprod_l(__u32x8 a, __u32x8 b)
{
    return (__u64x4) __builtin_tachy_vprod8_4_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprod_l(__u32x16 a, __u32x16 b)
{
    return (__u64x8) __builtin_tachy_vprod8_4_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprods_h(__s8x16 a, __s8x16 b)
{
    return (__s16x8) __builtin_tachy_vprods2_1_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprods_h(__s8x32 a, __s8x32 b)
{
    return (__s16x16) __builtin_tachy_vprods2_1_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprods_h(__s8x64 a, __s8x64 b)
{
    return (__s16x32) __builtin_tachy_vprods2_1_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprods_w(__s16x8 a, __s16x8 b)
{
    return (__s32x4) __builtin_tachy_vprods4_2_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprods_w(__s16x16 a, __s16x16 b)
{
    return (__s32x8) __builtin_tachy_vprods4_2_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprods_w(__s16x32 a, __s16x32 b)
{
    return (__s32x16) __builtin_tachy_vprods4_2_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprods_l(__s32x4 a, __s32x4 b)
{
    return (__s64x2) __builtin_tachy_vprods8_4_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprods_l(__s32x8 a, __s32x8 b)
{
    return (__s64x4) __builtin_tachy_vprods8_4_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprods_l(__s32x16 a, __s32x16 b)
{
    return (__s64x8) __builtin_tachy_vprods8_4_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprodus_h(__u8x16 a, __s8x16 b)
{
    return (__s16x8) __builtin_tachy_vprodus2_1_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprodus_h(__u8x32 a, __s8x32 b)
{
    return (__s16x16) __builtin_tachy_vprodus2_1_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprodus_h(__u8x64 a, __s8x64 b)
{
    return (__s16x32) __builtin_tachy_vprodus2_1_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprodus_w(__u16x8 a, __s16x8 b)
{
    return (__s32x4) __builtin_tachy_vprodus4_2_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprodus_w(__u16x16 a, __s16x16 b)
{
    return (__s32x8) __builtin_tachy_vprodus4_2_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprodus_w(__u16x32 a, __s16x32 b)
{
    return (__s32x16) __builtin_tachy_vprodus4_2_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vprodus_l(__u32x4 a, __s32x4 b)
{
    return (__s64x2) __builtin_tachy_vprodus8_4_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vprodus_l(__u32x8 a, __s32x8 b)
{
    return (__s64x4) __builtin_tachy_vprodus8_4_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vprodus_l(__u32x16 a, __s32x16 b)
{
    return (__s64x8) __builtin_tachy_vprodus8_4_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rbit_b(__u8x16 a)
{
    return (__u8x16) __builtin_tachy_vrbit1_128(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rbit_b(__u8x32 a)
{
    return (__u8x32) __builtin_tachy_vrbit1_256(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rbit_b(__u8x64 a)
{
    return (__u8x64) __builtin_tachy_vrbit1_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rbit_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vrbit2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rbit_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vrbit2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rbit_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vrbit2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rbit_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vrbit4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rbit_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vrbit4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rbit_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vrbit4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rbit_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vrbit8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rbit_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vrbit8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rbit_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vrbit8_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rev_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vrev2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rev_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vrev2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rev_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vrev2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rev_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vrev4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rev_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vrev4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rev_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vrev4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rev_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vrev8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rev_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vrev8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rev_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vrev8_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rolv_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vrol1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rolv_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vrol1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rolv_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vrol1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rolv_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vrol2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rolv_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vrol2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rolv_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vrol2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rolv_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vrol4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rolv_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vrol4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rolv_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vrol4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rolv_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vrol8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rolv_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vrol8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rolv_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vrol8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rol_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vrol_r_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rol_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vrol_r_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rol_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vrol_r_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rol_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vrol_r_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rol_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vrol_r_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rol_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vrol_r_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rol_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vrol_r_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rol_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vrol_r_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rol_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vrol_r_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rol_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vrol_r_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rol_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vrol_r_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rol_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vrol_r_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_role_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vrole_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_role_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vrole_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_role_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vrole_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_role_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vrole_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_role_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vrole_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_role_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vrole_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_role_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vrole_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_role_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vrole_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_role_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vrole_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_role_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vrole_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_role_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vrole_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_role_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vrole_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rorv_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vror1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rorv_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vror1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rorv_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vror1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rorv_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vror2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rorv_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vror2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rorv_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vror2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rorv_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vror4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rorv_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vror4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rorv_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vror4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rorv_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vror8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rorv_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vror8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rorv_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vror8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ror_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vror_r_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ror_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vror_r_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ror_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vror_r_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ror_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vror_r_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ror_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vror_r_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ror_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vror_r_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ror_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vror_r_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ror_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vror_r_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ror_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vror_r_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_ror_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vror_r_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_ror_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vror_r_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_ror_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vror_r_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rore_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vrore_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rore_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vrore_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rore_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vrore_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rore_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vrore_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rore_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vrore_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rore_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vrore_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rore_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vrore_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rore_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vrore_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rore_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vrore_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rore_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vrore_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rore_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vrore_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rore_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vrore_w_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_round_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vround_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_round_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vround_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_round_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vround_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_round_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vround_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_round_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vround_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_round_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vround_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rounda_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vrounda_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rounda_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vrounda_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rounda_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vrounda_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rounda_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vrounda_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rounda_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vrounda_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rounda_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vrounda_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundi_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundi_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundi_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundi_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundi_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundi_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundi_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundi_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundi_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundi_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundi_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundi_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundia_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundia_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundia_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundia_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundia_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundia_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundia_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundia_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundia_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundia_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundia_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundia_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundin_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundin_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundin_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundin_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundin_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundin_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundin_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundin_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundin_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundin_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundin_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundin_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundip_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundip_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundip_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundip_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundip_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundip_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundip_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundip_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundip_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundip_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundip_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundip_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundir_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundir_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundir_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundir_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundir_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundir_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundir_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundir_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundir_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundir_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundir_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundir_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundit_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundit_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundit_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundit_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundit_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundit_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundit_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundit_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundit_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundit_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundit_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundit_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundix_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundix_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundix_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundix_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundix_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundix_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundix_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundix_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundix_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundix_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundix_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundix_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundn_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundn_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundn_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundn_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundn_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundn_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundn_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundn_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundn_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundn_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundn_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundn_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundp_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundp_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundp_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundp_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundp_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundp_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundp_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundp_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundp_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundp_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundp_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundp_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundr_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundr_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundr_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundr_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundr_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundr_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundr_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundr_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundr_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundr_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundr_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundr_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundt_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundt_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundt_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundt_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundt_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundt_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundt_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundt_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundt_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundt_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundt_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundt_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundx_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vroundx_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundx_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vroundx_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundx_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vroundx_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_roundx_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vroundx_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_roundx_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vroundx_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_roundx_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vroundx_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rsqrt_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vrsqrt_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rsqrt_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vrsqrt_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rsqrt_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vrsqrt_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rsqrt_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vrsqrt_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rsqrt_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vrsqrt_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rsqrt_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vrsqrt_f_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rsqrtx_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vrsqrtx_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rsqrtx_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vrsqrtx_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rsqrtx_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vrsqrtx_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_rsqrtx_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vrsqrtx_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_rsqrtx_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vrsqrtx_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_rsqrtx_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vrsqrtx_f_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sad_l(__u8x16 a, __u8x16 b)
{
    return (__u64x2) __builtin_tachy_vsad_l_b_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sad_l(__u8x32 a, __u8x32 b)
{
    return (__u64x4) __builtin_tachy_vsad_l_b_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sad_l(__u8x64 a, __u8x64 b)
{
    return (__u64x8) __builtin_tachy_vsad_l_b_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sada_l(__u8x16 a, __u8x16 b)
{
    return (__u64x2) __builtin_tachy_vsada_l_b_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sada_l(__u8x32 a, __u8x32 b)
{
    return (__u64x4) __builtin_tachy_vsada_l_b_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sada_l(__u8x64 a, __u8x64 b)
{
    return (__u64x8) __builtin_tachy_vsada_l_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sadm_h(__u8x16 a, __u8x16 b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vsadm_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sadm_h(__u8x32 a, __u8x32 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vsadm_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sadm_h(__u8x64 a, __u8x64 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vsadm_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sadma_h(__u8x16 a, __u8x16 b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vsadma_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sadma_h(__u8x32 a, __u8x32 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vsadma_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sadma_h(__u8x64 a, __u8x64 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vsadma_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sadt_h(__u8x16 a, __u8x16 b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vsadt_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sadt_h(__u8x32 a, __u8x32 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vsadt_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sadt_h(__u8x64 a, __u8x64 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vsadt_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sadta_h(__u8x16 a, __u8x16 b, unsigned int c)
{
    return (__u16x8) __builtin_tachy_vsadta_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sadta_h(__u8x32 a, __u8x32 b, unsigned int c)
{
    return (__u16x16) __builtin_tachy_vsadta_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sadta_h(__u8x64 a, __u8x64 b, unsigned int c)
{
    return (__u16x32) __builtin_tachy_vsadta_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sarv_b(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vsar1_128(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sarv_b(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vsar1_256(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sarv_b(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vsar1_512(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sarv_h(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vsar2_128(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sarv_h(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vsar2_256(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sarv_h(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vsar2_512(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sarv_w(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vsar4_128(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sarv_w(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vsar4_256(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sarv_w(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vsar4_512(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sarv_l(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vsar8_128(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sarv_l(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vsar8_256(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sarv_l(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vsar8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sar_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vsar_r_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sar_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vsar_r_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sar_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vsar_r_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sar_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vsar_r_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sar_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vsar_r_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sar_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vsar_r_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sar_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vsar_r_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sar_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vsar_r_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sar_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vsar_r_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sar_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vsar_r_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sar_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vsar_r_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sar_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vsar_r_w_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shf_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshf_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shf_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshf_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shf_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshf_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shf_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vshf_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shf_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vshf_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shf_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vshf_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shfh_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshfh_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shfh_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshfh_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shfh_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshfh_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shfl_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshfl_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shfl_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshfl_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shfl_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshfl_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shlv_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vshl1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shlv_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vshl1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shlv_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vshl1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shlv_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vshl2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shlv_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vshl2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shlv_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vshl2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shlv_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vshl4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shlv_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vshl4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shlv_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vshl4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shlv_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vshl8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shlv_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vshl8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shlv_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vshl8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shl_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vshl_r_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shl_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vshl_r_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shl_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vshl_r_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shl_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshl_r_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shl_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshl_r_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shl_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshl_r_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shl_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vshl_r_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shl_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vshl_r_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shl_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vshl_r_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shl_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vshl_r_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shl_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vshl_r_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shl_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vshl_r_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shle_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vshle_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shle_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vshle_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shle_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vshle_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shle_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshle_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shle_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshle_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shle_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshle_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shle_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vshle_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shle_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vshle_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shle_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vshle_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shle_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vshle_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shle_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vshle_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shle_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vshle_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shrv_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vshr1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shrv_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vshr1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shrv_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vshr1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shrv_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vshr2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shrv_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vshr2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shrv_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vshr2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shrv_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vshr4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shrv_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vshr4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shrv_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vshr4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shrv_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vshr8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shrv_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vshr8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shrv_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vshr8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shr_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vshr_r_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shr_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vshr_r_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shr_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vshr_r_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shr_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshr_r_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shr_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshr_r_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shr_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshr_r_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shr_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vshr_r_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shr_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vshr_r_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shr_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vshr_r_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shr_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vshr_r_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shr_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vshr_r_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shr_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vshr_r_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shre_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vshre_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shre_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vshre_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shre_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vshre_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shre_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vshre_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shre_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vshre_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shre_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vshre_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shre_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vshre_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shre_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vshre_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shre_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vshre_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_shre_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vshre_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_shre_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vshre_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_shre_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vshre_w_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprod_h(__u8x16 a, __u8x16 b)
{
    return (__u16x8) __builtin_tachy_vsprod2_1_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprod_h(__u8x32 a, __u8x32 b)
{
    return (__u16x16) __builtin_tachy_vsprod2_1_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprod_h(__u8x64 a, __u8x64 b)
{
    return (__u16x32) __builtin_tachy_vsprod2_1_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprod_w(__u16x8 a, __u16x8 b)
{
    return (__u32x4) __builtin_tachy_vsprod4_2_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprod_w(__u16x16 a, __u16x16 b)
{
    return (__u32x8) __builtin_tachy_vsprod4_2_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprod_w(__u16x32 a, __u16x32 b)
{
    return (__u32x16) __builtin_tachy_vsprod4_2_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprod_l(__u32x4 a, __u32x4 b)
{
    return (__u64x2) __builtin_tachy_vsprod8_4_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprod_l(__u32x8 a, __u32x8 b)
{
    return (__u64x4) __builtin_tachy_vsprod8_4_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprod_l(__u32x16 a, __u32x16 b)
{
    return (__u64x8) __builtin_tachy_vsprod8_4_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprods_h(__u8x16 a, __u8x16 b)
{
    return (__u16x8) __builtin_tachy_vsprods2_1_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprods_h(__u8x32 a, __u8x32 b)
{
    return (__u16x16) __builtin_tachy_vsprods2_1_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprods_h(__u8x64 a, __u8x64 b)
{
    return (__u16x32) __builtin_tachy_vsprods2_1_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprods_w(__u16x8 a, __u16x8 b)
{
    return (__u32x4) __builtin_tachy_vsprods4_2_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprods_w(__u16x16 a, __u16x16 b)
{
    return (__u32x8) __builtin_tachy_vsprods4_2_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprods_w(__u16x32 a, __u16x32 b)
{
    return (__u32x16) __builtin_tachy_vsprods4_2_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprods_l(__u32x4 a, __u32x4 b)
{
    return (__u64x2) __builtin_tachy_vsprods8_4_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprods_l(__u32x8 a, __u32x8 b)
{
    return (__u64x4) __builtin_tachy_vsprods8_4_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprods_l(__u32x16 a, __u32x16 b)
{
    return (__u64x8) __builtin_tachy_vsprods8_4_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprodus_h(__u8x16 a, __u8x16 b)
{
    return (__u16x8) __builtin_tachy_vsprodus2_1_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprodus_h(__u8x32 a, __u8x32 b)
{
    return (__u16x16) __builtin_tachy_vsprodus2_1_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprodus_h(__u8x64 a, __u8x64 b)
{
    return (__u16x32) __builtin_tachy_vsprodus2_1_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprodus_w(__u16x8 a, __u16x8 b)
{
    return (__u32x4) __builtin_tachy_vsprodus4_2_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprodus_w(__u16x16 a, __u16x16 b)
{
    return (__u32x8) __builtin_tachy_vsprodus4_2_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprodus_w(__u16x32 a, __u16x32 b)
{
    return (__u32x16) __builtin_tachy_vsprodus4_2_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_vsprodus_l(__u32x4 a, __u32x4 b)
{
    return (__u64x2) __builtin_tachy_vsprodus8_4_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_vsprodus_l(__u32x8 a, __u32x8 b)
{
    return (__u64x4) __builtin_tachy_vsprodus8_4_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_vsprodus_l(__u32x16 a, __u32x16 b)
{
    return (__u64x8) __builtin_tachy_vsprodus8_4_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sqrt_d(__f64x2 a)
{
    return (__f64x2) __builtin_tachy_vsqrt_d_128(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sqrt_d(__f64x4 a)
{
    return (__f64x4) __builtin_tachy_vsqrt_d_256(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sqrt_d(__f64x8 a)
{
    return (__f64x8) __builtin_tachy_vsqrt_d_512(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sqrt_f(__f32x4 a)
{
    return (__f32x4) __builtin_tachy_vsqrt_f_128(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sqrt_f(__f32x8 a)
{
    return (__f32x8) __builtin_tachy_vsqrt_f_256(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sqrt_f(__f32x16 a)
{
    return (__f32x16) __builtin_tachy_vsqrt_f_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vsub1_128(a, b);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_b_s(__s8x16 a, __s8x16 b)
{
    return (__s8x16) __builtin_tachy_vsub1_128((__u8x16) a, (__u8x16) b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vsub1_256(a, b);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_b_s(__s8x32 a, __s8x32 b)
{
    return (__s8x32) __builtin_tachy_vsub1_256((__u8x32) a, (__u8x32) b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vsub1_512(a, b);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_b_s(__s8x64 a, __s8x64 b)
{
    return (__s8x64) __builtin_tachy_vsub1_512((__u8x64) a, (__u8x64) b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vsub2_128(a, b);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_h_s(__s16x8 a, __s16x8 b)
{
    return (__s16x8) __builtin_tachy_vsub2_128((__u16x8) a, (__u16x8) b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vsub2_256(a, b);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_h_s(__s16x16 a, __s16x16 b)
{
    return (__s16x16) __builtin_tachy_vsub2_256((__u16x16) a, (__u16x16) b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vsub2_512(a, b);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_h_s(__s16x32 a, __s16x32 b)
{
    return (__s16x32) __builtin_tachy_vsub2_512((__u16x32) a, (__u16x32) b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vsub4_128(a, b);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_w_s(__s32x4 a, __s32x4 b)
{
    return (__s32x4) __builtin_tachy_vsub4_128((__u32x4) a, (__u32x4) b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vsub4_256(a, b);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_w_s(__s32x8 a, __s32x8 b)
{
    return (__s32x8) __builtin_tachy_vsub4_256((__u32x8) a, (__u32x8) b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vsub4_512(a, b);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_w_s(__s32x16 a, __s32x16 b)
{
    return (__s32x16) __builtin_tachy_vsub4_512((__u32x16) a, (__u32x16) b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vsub8_128(a, b);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_l_s(__s64x2 a, __s64x2 b)
{
    return (__s64x2) __builtin_tachy_vsub8_128((__u64x2) a, (__u64x2) b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vsub8_256(a, b);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_l_s(__s64x4 a, __s64x4 b)
{
    return (__s64x4) __builtin_tachy_vsub8_256((__u64x4) a, (__u64x4) b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vsub8_512(a, b);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_l_s(__s64x8 a, __s64x8 b)
{
    return (__s64x8) __builtin_tachy_vsub8_512((__u64x8) a, (__u64x8) b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vsub_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vsub_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vsub_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sub_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vsub_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sub_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vsub_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sub_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vsub_f_512(a, b);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_suba_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vsuba_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_suba_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vsuba_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_suba_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vsuba_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_suba_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vsuba_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_suba_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vsuba_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_suba_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vsuba_f_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subc_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vsubc1_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subc_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vsubc1_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subc_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vsubc1_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subc_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vsubc2_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subc_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vsubc2_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subc_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vsubc2_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subc_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vsubc4_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subc_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vsubc4_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subc_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vsubc4_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subc_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vsubc8_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subc_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vsubc8_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subc_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vsubc8_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vsubco_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vsubco_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vsubco_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vsubco_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vsubco_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vsubco_h_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vsubco_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vsubco_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vsubco_l_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vsubco_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vsubco_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vsubco_w_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco4_b(__u8x16 a, __u8x16 b, __u8x16 c)
{
    return (__u8x16) __builtin_tachy_vsubco_x_b_128(a, b, c);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco4_b(__u8x32 a, __u8x32 b, __u8x32 c)
{
    return (__u8x32) __builtin_tachy_vsubco_x_b_256(a, b, c);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco4_b(__u8x64 a, __u8x64 b, __u8x64 c)
{
    return (__u8x64) __builtin_tachy_vsubco_x_b_512(a, b, c);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco4_h(__u16x8 a, __u16x8 b, __u16x8 c)
{
    return (__u16x8) __builtin_tachy_vsubco_x_h_128(a, b, c);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco4_h(__u16x16 a, __u16x16 b, __u16x16 c)
{
    return (__u16x16) __builtin_tachy_vsubco_x_h_256(a, b, c);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco4_h(__u16x32 a, __u16x32 b, __u16x32 c)
{
    return (__u16x32) __builtin_tachy_vsubco_x_h_512(a, b, c);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco4_l(__u64x2 a, __u64x2 b, __u64x2 c)
{
    return (__u64x2) __builtin_tachy_vsubco_x_l_128(a, b, c);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco4_l(__u64x4 a, __u64x4 b, __u64x4 c)
{
    return (__u64x4) __builtin_tachy_vsubco_x_l_256(a, b, c);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco4_l(__u64x8 a, __u64x8 b, __u64x8 c)
{
    return (__u64x8) __builtin_tachy_vsubco_x_l_512(a, b, c);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subco4_w(__u32x4 a, __u32x4 b, __u32x4 c)
{
    return (__u32x4) __builtin_tachy_vsubco_x_w_128(a, b, c);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subco4_w(__u32x8 a, __u32x8 b, __u32x8 c)
{
    return (__u32x8) __builtin_tachy_vsubco_x_w_256(a, b, c);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subco4_w(__u32x16 a, __u32x16 b, __u32x16 c)
{
    return (__u32x16) __builtin_tachy_vsubco_x_w_512(a, b, c);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subs_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vsubs1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subs_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vsubs1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subs_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vsubs1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subs_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vsubs2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subs_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vsubs2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subs_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vsubs2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subs_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vsubs4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subs_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vsubs4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subs_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vsubs4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subs_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vsubs8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subs_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vsubs8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subs_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vsubs8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subu_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vsubu1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subu_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vsubu1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subu_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vsubu1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subu_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vsubu2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subu_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vsubu2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subu_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vsubu2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subu_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vsubu4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subu_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vsubu4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subu_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vsubu4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_subu_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vsubu8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_subu_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vsubu8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_subu_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vsubu8_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxb_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vsxb2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxb_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vsxb2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxb_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vsxb2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxb_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vsxb4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxb_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vsxb4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxb_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vsxb4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxb_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vsxb8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxb_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vsxb8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxb_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vsxb8_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxh_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vsxh4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxh_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vsxh4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxh_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vsxh4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxh_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vsxh8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxh_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vsxh8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxh_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vsxh8_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_sxw_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vsxw8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_sxw_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vsxw8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_sxw_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vsxw8_512(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_uno_d(__f64x2 a, __f64x2 b)
{
    return (__f64x2) __builtin_tachy_vuno_d_128(a, b);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_uno_d(__f64x4 a, __f64x4 b)
{
    return (__f64x4) __builtin_tachy_vuno_d_256(a, b);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_uno_d(__f64x8 a, __f64x8 b)
{
    return (__f64x8) __builtin_tachy_vuno_d_512(a, b);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_uno_f(__f32x4 a, __f32x4 b)
{
    return (__f32x4) __builtin_tachy_vuno_f_128(a, b);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_uno_f(__f32x8 a, __f32x8 b)
{
    return (__f32x8) __builtin_tachy_vuno_f_256(a, b);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_uno_f(__f32x16 a, __f32x16 b)
{
    return (__f32x16) __builtin_tachy_vuno_f_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_uno_ld(__f64x2 a, __f64x2 b)
{
    return (__u64x2) __builtin_tachy_vuno_u_d_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_uno_ld(__f64x4 a, __f64x4 b)
{
    return (__u64x4) __builtin_tachy_vuno_u_d_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_uno_ld(__f64x8 a, __f64x8 b)
{
    return (__u64x8) __builtin_tachy_vuno_u_d_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_uno_wf(__f32x4 a, __f32x4 b)
{
    return (__u32x4) __builtin_tachy_vuno_u_f_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_uno_wf(__f32x8 a, __f32x8 b)
{
    return (__u32x8) __builtin_tachy_vuno_u_f_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_uno_wf(__f32x16 a, __f32x16 b)
{
    return (__u32x16) __builtin_tachy_vuno_u_f_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpk_h(__u8x16 a)
{
    return (__u16x8) __builtin_tachy_vunpk_h_b_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpk_h(__u8x32 a)
{
    return (__u16x16) __builtin_tachy_vunpk_h_b_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpk_h(__u8x64 a)
{
    return (__u16x32) __builtin_tachy_vunpk_h_b_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpk_l(__u32x4 a)
{
    return (__u64x2) __builtin_tachy_vunpk_l_w_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpk_l(__u32x8 a)
{
    return (__u64x4) __builtin_tachy_vunpk_l_w_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpk_l(__u32x16 a)
{
    return (__u64x8) __builtin_tachy_vunpk_l_w_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpk_w(__u16x8 a)
{
    return (__u32x4) __builtin_tachy_vunpk_w_h_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpk_w(__u16x16 a)
{
    return (__u32x8) __builtin_tachy_vunpk_w_h_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpk_w(__u16x32 a)
{
    return (__u32x16) __builtin_tachy_vunpk_w_h_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkh_h(__u8x16 a)
{
    return (__u16x8) __builtin_tachy_vunpkh_h_b_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkh_h(__u8x32 a)
{
    return (__u16x16) __builtin_tachy_vunpkh_h_b_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkh_h(__u8x64 a)
{
    return (__u16x32) __builtin_tachy_vunpkh_h_b_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkh_l(__u32x4 a)
{
    return (__u64x2) __builtin_tachy_vunpkh_l_w_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkh_l(__u32x8 a)
{
    return (__u64x4) __builtin_tachy_vunpkh_l_w_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkh_l(__u32x16 a)
{
    return (__u64x8) __builtin_tachy_vunpkh_l_w_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkh_w(__u16x8 a)
{
    return (__u32x4) __builtin_tachy_vunpkh_w_h_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkh_w(__u16x16 a)
{
    return (__u32x8) __builtin_tachy_vunpkh_w_h_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkh_w(__u16x32 a)
{
    return (__u32x16) __builtin_tachy_vunpkh_w_h_512(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkhs_h(__u8x16 a)
{
    return (__u16x8) __builtin_tachy_vunpkhs_h_b_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkhs_h(__u8x32 a)
{
    return (__u16x16) __builtin_tachy_vunpkhs_h_b_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkhs_h(__u8x64 a)
{
    return (__u16x32) __builtin_tachy_vunpkhs_h_b_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkhs_l(__u32x4 a)
{
    return (__u64x2) __builtin_tachy_vunpkhs_l_w_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkhs_l(__u32x8 a)
{
    return (__u64x4) __builtin_tachy_vunpkhs_l_w_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkhs_l(__u32x16 a)
{
    return (__u64x8) __builtin_tachy_vunpkhs_l_w_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkhs_w(__u16x8 a)
{
    return (__u32x4) __builtin_tachy_vunpkhs_w_h_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkhs_w(__u16x16 a)
{
    return (__u32x8) __builtin_tachy_vunpkhs_w_h_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkhs_w(__u16x32 a)
{
    return (__u32x16) __builtin_tachy_vunpkhs_w_h_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpki_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vunpki_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpki_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vunpki_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpki_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vunpki_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpki_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vunpki_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpki_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vunpki_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpki_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vunpki_h_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpki_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vunpki_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpki_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vunpki_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpki_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vunpki_w_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpki_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vunpki_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpki_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vunpki_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpki_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vunpki_l_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkih_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vunpkih_b_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkih_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vunpkih_b_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkih_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vunpkih_b_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkih_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vunpkih_h_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkih_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vunpkih_h_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkih_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vunpkih_h_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkih_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vunpkih_w_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkih_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vunpkih_w_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkih_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vunpkih_w_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpkih_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vunpkih_l_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpkih_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vunpkih_l_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpkih_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vunpkih_l_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpks_h(__u8x16 a)
{
    return (__u16x8) __builtin_tachy_vunpks_h_b_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpks_h(__u8x32 a)
{
    return (__u16x16) __builtin_tachy_vunpks_h_b_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpks_h(__u8x64 a)
{
    return (__u16x32) __builtin_tachy_vunpks_h_b_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpks_l(__u32x4 a)
{
    return (__u64x2) __builtin_tachy_vunpks_l_w_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpks_l(__u32x8 a)
{
    return (__u64x4) __builtin_tachy_vunpks_l_w_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpks_l(__u32x16 a)
{
    return (__u64x8) __builtin_tachy_vunpks_l_w_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_unpks_w(__u16x8 a)
{
    return (__u32x4) __builtin_tachy_vunpks_w_h_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_unpks_w(__u16x16 a)
{
    return (__u32x8) __builtin_tachy_vunpks_w_h_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_unpks_w(__u16x32 a)
{
    return (__u32x16) __builtin_tachy_vunpks_w_h_512(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xor_b(__u8x16 a, __u8x16 b)
{
    return (__u8x16) __builtin_tachy_vxor1_128(a, b);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xor_b(__u8x32 a, __u8x32 b)
{
    return (__u8x32) __builtin_tachy_vxor1_256(a, b);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xor_b(__u8x64 a, __u8x64 b)
{
    return (__u8x64) __builtin_tachy_vxor1_512(a, b);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xor_h(__u16x8 a, __u16x8 b)
{
    return (__u16x8) __builtin_tachy_vxor2_128(a, b);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xor_h(__u16x16 a, __u16x16 b)
{
    return (__u16x16) __builtin_tachy_vxor2_256(a, b);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xor_h(__u16x32 a, __u16x32 b)
{
    return (__u16x32) __builtin_tachy_vxor2_512(a, b);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xor_w(__u32x4 a, __u32x4 b)
{
    return (__u32x4) __builtin_tachy_vxor4_128(a, b);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xor_w(__u32x8 a, __u32x8 b)
{
    return (__u32x8) __builtin_tachy_vxor4_256(a, b);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xor_w(__u32x16 a, __u32x16 b)
{
    return (__u32x16) __builtin_tachy_vxor4_512(a, b);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xor_l(__u64x2 a, __u64x2 b)
{
    return (__u64x2) __builtin_tachy_vxor8_128(a, b);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xor_l(__u64x4 a, __u64x4 b)
{
    return (__u64x4) __builtin_tachy_vxor8_256(a, b);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xor_l(__u64x8 a, __u64x8 b)
{
    return (__u64x8) __builtin_tachy_vxor8_512(a, b);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrv_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vxtr_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrv_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vxtr_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrv_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vxtr_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrv_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vxtr_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrv_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vxtr_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrv_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vxtr_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrv_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vxtr_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrv_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vxtr_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrv_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vxtr_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtr_b(__u8x16 a, unsigned int b)
{
    return (uint8_t) __builtin_tachy_vxtr_r_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtr_b(__u8x32 a, unsigned int b)
{
    return (uint8_t) __builtin_tachy_vxtr_r_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtr_b(__u8x64 a, unsigned int b)
{
    return (uint8_t) __builtin_tachy_vxtr_r_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtr_h(__u16x8 a, unsigned int b)
{
    return (uint16_t) __builtin_tachy_vxtr_r_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtr_h(__u16x16 a, unsigned int b)
{
    return (uint16_t) __builtin_tachy_vxtr_r_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtr_h(__u16x32 a, unsigned int b)
{
    return (uint16_t) __builtin_tachy_vxtr_r_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtr_l(__u64x2 a, unsigned int b)
{
    return (uint64_t) __builtin_tachy_vxtr_r_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtr_l(__u64x4 a, unsigned int b)
{
    return (uint64_t) __builtin_tachy_vxtr_r_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtr_l(__u64x8 a, unsigned int b)
{
    return (uint64_t) __builtin_tachy_vxtr_r_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtr_w(__u32x4 a, unsigned int b)
{
    return (uint32_t) __builtin_tachy_vxtr_r_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtr_w(__u32x8 a, unsigned int b)
{
    return (uint32_t) __builtin_tachy_vxtr_r_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtr_w(__u32x16 a, unsigned int b)
{
    return (uint32_t) __builtin_tachy_vxtr_r_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrv_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vxtr_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrv_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vxtr_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrv_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vxtr_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrx_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vxtrx_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrx_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vxtrx_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrx_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vxtrx_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrx_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vxtrx_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrx_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vxtrx_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrx_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vxtrx_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrx_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vxtrx_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrx_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vxtrx_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrx_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vxtrx_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrx_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vxtrx_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrx_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vxtrx_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrx_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vxtrx_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrxx_b(__u8x16 a, unsigned int b)
{
    return (__u8x16) __builtin_tachy_vxtrxx_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrxx_b(__u8x32 a, unsigned int b)
{
    return (__u8x32) __builtin_tachy_vxtrxx_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrxx_b(__u8x64 a, unsigned int b)
{
    return (__u8x64) __builtin_tachy_vxtrxx_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrxx_h(__u16x8 a, unsigned int b)
{
    return (__u16x8) __builtin_tachy_vxtrxx_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrxx_h(__u16x16 a, unsigned int b)
{
    return (__u16x16) __builtin_tachy_vxtrxx_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrxx_h(__u16x32 a, unsigned int b)
{
    return (__u16x32) __builtin_tachy_vxtrxx_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrxx_l(__u64x2 a, unsigned int b)
{
    return (__u64x2) __builtin_tachy_vxtrxx_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrxx_l(__u64x4 a, unsigned int b)
{
    return (__u64x4) __builtin_tachy_vxtrxx_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrxx_l(__u64x8 a, unsigned int b)
{
    return (__u64x8) __builtin_tachy_vxtrxx_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_xtrxx_w(__u32x4 a, unsigned int b)
{
    return (__u32x4) __builtin_tachy_vxtrxx_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_xtrxx_w(__u32x8 a, unsigned int b)
{
    return (__u32x8) __builtin_tachy_vxtrxx_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_xtrxx_w(__u32x16 a, unsigned int b)
{
    return (__u32x16) __builtin_tachy_vxtrxx_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxb_h(__u16x8 a)
{
    return (__u16x8) __builtin_tachy_vzxb2_128(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxb_h(__u16x16 a)
{
    return (__u16x16) __builtin_tachy_vzxb2_256(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxb_h(__u16x32 a)
{
    return (__u16x32) __builtin_tachy_vzxb2_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxb_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vzxb4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxb_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vzxb4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxb_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vzxb4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxb_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vzxb8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxb_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vzxb8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxb_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vzxb8_512(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxh_w(__u32x4 a)
{
    return (__u32x4) __builtin_tachy_vzxh4_128(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxh_w(__u32x8 a)
{
    return (__u32x8) __builtin_tachy_vzxh4_256(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxh_w(__u32x16 a)
{
    return (__u32x16) __builtin_tachy_vzxh4_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxh_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vzxh8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxh_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vzxh8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxh_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vzxh8_512(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx128_zxw_l(__u64x2 a)
{
    return (__u64x2) __builtin_tachy_vzxw8_128(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx256_zxw_l(__u64x4 a)
{
    return (__u64x4) __builtin_tachy_vzxw8_256(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_zxw_l(__u64x8 a)
{
    return (__u64x8) __builtin_tachy_vzxw8_512(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8df_to_v4df(__f64x8 a)
{
    return __builtin_tachy_cast_v8df_to_v4df(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16sf_to_v8sf(__f32x16 a)
{
    return __builtin_tachy_cast_v16sf_to_v8sf(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8di_to_v4di(__u64x8 a)
{
    return __builtin_tachy_cast_v8di_to_v4di(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16si_to_v8si(__u32x16 a)
{
    return __builtin_tachy_cast_v16si_to_v8si(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v32hi_to_v16hi(__u16x32 a)
{
    return __builtin_tachy_cast_v32hi_to_v16hi(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v64qi_to_v32qi(__u8x64 a)
{
    return __builtin_tachy_cast_v64qi_to_v32qi(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4df_to_v8df(__f64x4 a)
{
    return __builtin_tachy_cast_v4df_to_v8df(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8sf_to_v16sf(__f32x8 a)
{
    return __builtin_tachy_cast_v8sf_to_v16sf(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4di_to_v8di(__u64x4 a)
{
    return __builtin_tachy_cast_v4di_to_v8di(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8si_to_v16si(__u32x8 a)
{
    return __builtin_tachy_cast_v8si_to_v16si(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16hi_to_v32hi(__u16x16 a)
{
    return __builtin_tachy_cast_v16hi_to_v32hi(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v32qi_to_v64qi(__u8x32 a)
{
    return __builtin_tachy_cast_v32qi_to_v64qi(a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8df_to_v2df(__f64x8 a)
{
    return __builtin_tachy_cast_v8df_to_v2df(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16sf_to_v4sf(__f32x16 a)
{
    return __builtin_tachy_cast_v16sf_to_v4sf(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8di_to_v2di(__u64x8 a)
{
    return __builtin_tachy_cast_v8di_to_v2di(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16si_to_v4si(__u32x16 a)
{
    return __builtin_tachy_cast_v16si_to_v4si(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v32hi_to_v8hi(__u16x32 a)
{
    return __builtin_tachy_cast_v32hi_to_v8hi(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v64qi_to_v16qi(__u8x64 a)
{
    return __builtin_tachy_cast_v64qi_to_v16qi(a);
}

extern __inline __f64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v2df_to_v8df(__f64x2 a)
{
    return __builtin_tachy_cast_v2df_to_v8df(a);
}

extern __inline __f32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4sf_to_v16sf(__f32x4 a)
{
    return __builtin_tachy_cast_v4sf_to_v16sf(a);
}

extern __inline __u64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v2di_to_v8di(__u64x2 a)
{
    return __builtin_tachy_cast_v2di_to_v8di(a);
}

extern __inline __u32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4si_to_v16si(__u32x4 a)
{
    return __builtin_tachy_cast_v4si_to_v16si(a);
}

extern __inline __u16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8hi_to_v32hi(__u16x8 a)
{
    return __builtin_tachy_cast_v8hi_to_v32hi(a);
}

extern __inline __u8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16qi_to_v64qi(__u8x16 a)
{
    return __builtin_tachy_cast_v16qi_to_v64qi(a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8di_to_v4di(__s64x8 a)
{
    return (__s64x4) __builtin_tachy_cast_v8di_to_v4di((__u64x8) a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16si_to_v8si(__s32x16 a)
{
    return (__s32x8) __builtin_tachy_cast_v16si_to_v8si((__u32x16) a);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v32hi_to_v16hi(__s16x32 a)
{
    return (__s16x16) __builtin_tachy_cast_v32hi_to_v16hi((__u16x32) a);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v64qi_to_v32qi(__s8x64 a)
{
    return (__s8x32) __builtin_tachy_cast_v64qi_to_v32qi((__u8x64) a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v4di_to_v8di(__s64x4 a)
{
    return (__s64x8) __builtin_tachy_cast_v4di_to_v8di((__u64x4) a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8si_to_v16si(__s32x8 a)
{
    return (__s32x16) __builtin_tachy_cast_v8si_to_v16si((__u32x8) a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16hi_to_v32hi(__s16x16 a)
{
    return (__s16x32) __builtin_tachy_cast_v16hi_to_v32hi((__u16x16) a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v32qi_to_v64qi(__s8x32 a)
{
    return (__s8x64) __builtin_tachy_cast_v32qi_to_v64qi((__u8x32) a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8di_to_v2di(__s64x8 a)
{
    return (__s64x2) __builtin_tachy_cast_v8di_to_v2di((__u64x8) a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16si_to_v4si(__s32x16 a)
{
    return (__s32x4) __builtin_tachy_cast_v16si_to_v4si((__u32x16) a);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v32hi_to_v8hi(__s16x32 a)
{
    return (__s16x8) __builtin_tachy_cast_v32hi_to_v8hi((__u16x32) a);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v64qi_to_v16qi(__s8x64 a)
{
    return (__s8x16) __builtin_tachy_cast_v64qi_to_v16qi((__u8x64) a);
}

extern __inline __s64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v2di_to_v8di(__s64x2 a)
{
    return (__s64x8) __builtin_tachy_cast_v2di_to_v8di((__u64x2) a);
}

extern __inline __s32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v4si_to_v16si(__s32x4 a)
{
    return (__s32x16) __builtin_tachy_cast_v4si_to_v16si((__u32x4) a);
}

extern __inline __s16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8hi_to_v32hi(__s16x8 a)
{
    return (__s16x32) __builtin_tachy_cast_v8hi_to_v32hi((__u16x8) a);
}

extern __inline __s8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16qi_to_v64qi(__s8x16 a)
{
    return (__s8x64) __builtin_tachy_cast_v16qi_to_v64qi((__u8x16) a);
}

extern __inline __f64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4df_to_v2df(__f64x4 a)
{
    return __builtin_tachy_cast_v4df_to_v2df(a);
}

extern __inline __f32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8sf_to_v4sf(__f32x8 a)
{
    return __builtin_tachy_cast_v8sf_to_v4sf(a);
}

extern __inline __u64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4di_to_v2di(__u64x4 a)
{
    return __builtin_tachy_cast_v4di_to_v2di(a);
}

extern __inline __u32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8si_to_v4si(__u32x8 a)
{
    return __builtin_tachy_cast_v8si_to_v4si(a);
}

extern __inline __u16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16hi_to_v8hi(__u16x16 a)
{
    return __builtin_tachy_cast_v16hi_to_v8hi(a);
}

extern __inline __u8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v32qi_to_v16qi(__u8x32 a)
{
    return __builtin_tachy_cast_v32qi_to_v16qi(a);
}

extern __inline __f64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v2df_to_v4df(__f64x2 a)
{
    return __builtin_tachy_cast_v2df_to_v4df(a);
}

extern __inline __f32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4sf_to_v8sf(__f32x4 a)
{
    return __builtin_tachy_cast_v4sf_to_v8sf(a);
}

extern __inline __u64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v2di_to_v4di(__u64x2 a)
{
    return __builtin_tachy_cast_v2di_to_v4di(a);
}

extern __inline __u32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v4si_to_v8si(__u32x4 a)
{
    return __builtin_tachy_cast_v4si_to_v8si(a);
}

extern __inline __u16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v8hi_to_v16hi(__u16x8 a)
{
    return __builtin_tachy_cast_v8hi_to_v16hi(a);
}

extern __inline __u8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_cast_v16qi_to_v32qi(__u8x16 a)
{
    return __builtin_tachy_cast_v16qi_to_v32qi(a);
}

extern __inline __s64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v4di_to_v2di(__s64x4 a)
{
    return (__s64x2) __builtin_tachy_cast_v4di_to_v2di((__u64x4) a);
}

extern __inline __s32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8si_to_v4si(__s32x8 a)
{
    return (__s32x4) __builtin_tachy_cast_v8si_to_v4si((__u32x8) a);
}

extern __inline __s16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16hi_to_v8hi(__s16x16 a)
{
    return (__s16x8) __builtin_tachy_cast_v16hi_to_v8hi((__u16x16) a);
}

extern __inline __s8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v32qi_to_v16qi(__s8x32 a)
{
    return (__s8x16) __builtin_tachy_cast_v32qi_to_v16qi((__u8x32) a);
}

extern __inline __s64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v2di_to_v4di(__s64x2 a)
{
    return (__s64x4) __builtin_tachy_cast_v2di_to_v4di((__u64x2) a);
}

extern __inline __s32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v4si_to_v8si(__s32x4 a)
{
    return (__s32x8) __builtin_tachy_cast_v4si_to_v8si((__u32x4) a);
}

extern __inline __s16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v8hi_to_v16hi(__s16x8 a)
{
    return (__s16x16) __builtin_tachy_cast_v8hi_to_v16hi((__u16x8) a);
}

extern __inline __s8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
_vx512_casts_v16qi_to_v32qi(__s8x16 a)
{
    return (__s8x32) __builtin_tachy_cast_v16qi_to_v32qi((__u8x16) a);
}


#ifndef __cplusplus
#define _hadd(a) \
    _Generic(a, \
    __u8x64:  _vx512_hadd_b, \
    __f64x8:  _vx512_hadd_d, \
    __f32x16: _vx512_hadd_f, \
    __u16x32: _vx512_hadd_h, \
    __u64x8:  _vx512_hadd_l, \
    __u32x16: _vx512_hadd_w)(a)


#define _hand(a) \
    _Generic(a, \
    __u8x64:  _vx512_hand_b, \
    __u16x32: _vx512_hand_h, \
    __u64x8:  _vx512_hand_l, \
    __u32x16: _vx512_hand_w)(a)


#define _hmax(a) \
    _Generic(a, \
    __u8x64:  _vx512_hmax_b, \
    __f64x8:  _vx512_hmax_d, \
    __f32x16: _vx512_hmax_f, \
    __u16x32: _vx512_hmax_h, \
    __u64x8:  _vx512_hmax_l, \
    __u32x16: _vx512_hmax_w)(a)


#define _hmaxs(a) \
    _Generic(a, \
    __s8x64:  _vx512_hmaxs_b, \
    __s16x32: _vx512_hmaxs_h, \
    __s64x8:  _vx512_hmaxs_l, \
    __s32x16: _vx512_hmaxs_w)(a)


#define _hmin(a) \
    _Generic(a, \
    __u8x64:  _vx512_hmin_b, \
    __f64x8:  _vx512_hmin_d, \
    __f32x16: _vx512_hmin_f, \
    __u16x32: _vx512_hmin_h, \
    __u64x8:  _vx512_hmin_l, \
    __u32x16: _vx512_hmin_w)(a)


#define _hmins(a) \
    _Generic(a, \
    __s8x64:  _vx512_hmins_b, \
    __s16x32: _vx512_hmins_h, \
    __s64x8:  _vx512_hmins_l, \
    __s32x16: _vx512_hmins_w)(a)


#define _hmul(a) \
    _Generic(a, \
    __u8x64:  _vx512_hmul_b, \
    __f64x8:  _vx512_hmul_d, \
    __f32x16: _vx512_hmul_f, \
    __u16x32: _vx512_hmul_h, \
    __u64x8:  _vx512_hmul_l, \
    __u32x16: _vx512_hmul_w)(a)


#define _hor(a) \
    _Generic(a, \
    __u8x64:  _vx512_hor_b, \
    __u16x32: _vx512_hor_h, \
    __u64x8:  _vx512_hor_l, \
    __u32x16: _vx512_hor_w)(a)


#define _hxor(a) \
    _Generic(a, \
    __u8x64:  _vx512_hxor_b, \
    __u16x32: _vx512_hxor_h, \
    __u64x8:  _vx512_hxor_l, \
    __u32x16: _vx512_hxor_w)(a)


#define _vabs(a) \
    _Generic(a, \
    __s8x16:  _vx128_abs_b, \
    __s8x32:  _vx256_abs_b, \
    __s8x64:  _vx512_abs_b, \
    __s16x8:  _vx128_abs_h, \
    __s16x16: _vx256_abs_h, \
    __s16x32: _vx512_abs_h, \
    __s32x4:  _vx128_abs_w, \
    __s32x8:  _vx256_abs_w, \
    __s32x16: _vx512_abs_w, \
    __s64x2:  _vx128_abs_l, \
    __s64x4:  _vx256_abs_l, \
    __s64x8:  _vx512_abs_l, \
    __f64x2:  _vx128_abs_d, \
    __f64x4:  _vx256_abs_d, \
    __f64x8:  _vx512_abs_d, \
    __f32x4:  _vx128_abs_f, \
    __f32x8:  _vx256_abs_f, \
    __f32x16: _vx512_abs_f)(a)


#define _vadd(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_add_b, \
    __s8x16:  _vx128_add_b_s, \
    __u8x32:  _vx256_add_b, \
    __s8x32:  _vx256_add_b_s, \
    __u8x64:  _vx512_add_b, \
    __s8x64:  _vx512_add_b_s, \
    __u16x8:  _vx128_add_h, \
    __s16x8:  _vx128_add_h_s, \
    __u16x16: _vx256_add_h, \
    __s16x16: _vx256_add_h_s, \
    __u16x32: _vx512_add_h, \
    __s16x32: _vx512_add_h_s, \
    __u32x4:  _vx128_add_w, \
    __s32x4:  _vx128_add_w_s, \
    __u32x8:  _vx256_add_w, \
    __s32x8:  _vx256_add_w_s, \
    __u32x16: _vx512_add_w, \
    __s32x16: _vx512_add_w_s, \
    __u64x2:  _vx128_add_l, \
    __s64x2:  _vx128_add_l_s, \
    __u64x4:  _vx256_add_l, \
    __s64x4:  _vx256_add_l_s, \
    __u64x8:  _vx512_add_l, \
    __s64x8:  _vx512_add_l_s, \
    __f64x2:  _vx128_add_d, \
    __f64x4:  _vx256_add_d, \
    __f64x8:  _vx512_add_d, \
    __f32x4:  _vx128_add_f, \
    __f32x8:  _vx256_add_f, \
    __f32x16: _vx512_add_f)(a, b)


#define _vaddc(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_addc_b, \
    __u8x32:  _vx256_addc_b, \
    __u8x64:  _vx512_addc_b, \
    __u16x8:  _vx128_addc_h, \
    __u16x16: _vx256_addc_h, \
    __u16x32: _vx512_addc_h, \
    __u32x4:  _vx128_addc_w, \
    __u32x8:  _vx256_addc_w, \
    __u32x16: _vx512_addc_w, \
    __u64x2:  _vx128_addc_l, \
    __u64x4:  _vx256_addc_l, \
    __u64x8:  _vx512_addc_l)(a, b, c)


#define _vaddco(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_addco4_b, \
    __u8x32:  _vx256_addco4_b, \
    __u8x64:  _vx512_addco4_b, \
    __u16x8:  _vx128_addco4_h, \
    __u16x16: _vx256_addco4_h, \
    __u16x32: _vx512_addco4_h, \
    __u64x2:  _vx128_addco4_l, \
    __u64x4:  _vx256_addco4_l, \
    __u64x8:  _vx512_addco4_l, \
    __u32x4:  _vx128_addco4_w, \
    __u32x8:  _vx256_addco4_w, \
    __u32x16: _vx512_addco4_w)(a, b, c)


#define _vaddp(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_addp_b, \
    __f64x8:  _vx512_addp_d, \
    __f32x16: _vx512_addp_f, \
    __u16x32: _vx512_addp_h, \
    __u8x16:  _vx128_addp_b, \
    __u8x32:  _vx256_addp_b, \
    __f64x2:  _vx128_addp_d, \
    __f64x4:  _vx256_addp_d, \
    __f32x4:  _vx128_addp_f, \
    __f32x8:  _vx256_addp_f, \
    __u16x8:  _vx128_addp_h, \
    __u16x16: _vx256_addp_h, \
    __u64x2:  _vx128_addp_l, \
    __u64x4:  _vx256_addp_l, \
    __u64x8:  _vx512_addp2_l, \
    __u32x4:  _vx128_addp_w, \
    __u32x8:  _vx256_addp_w, \
    __u32x16: _vx512_addp2_w)(a, b)


#define _vadds(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_adds_b, \
    __u8x32:  _vx256_adds_b, \
    __u8x64:  _vx512_adds_b, \
    __u16x8:  _vx128_adds_h, \
    __u16x16: _vx256_adds_h, \
    __u16x32: _vx512_adds_h, \
    __u32x4:  _vx128_adds_w, \
    __u32x8:  _vx256_adds_w, \
    __u32x16: _vx512_adds_w, \
    __u64x2:  _vx128_adds_l, \
    __u64x4:  _vx256_adds_l, \
    __u64x8:  _vx512_adds_l)(a, b)


#define _vaddu(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_addu_b, \
    __u8x32:  _vx256_addu_b, \
    __u8x64:  _vx512_addu_b, \
    __u16x8:  _vx128_addu_h, \
    __u16x16: _vx256_addu_h, \
    __u16x32: _vx512_addu_h, \
    __u32x4:  _vx128_addu_w, \
    __u32x8:  _vx256_addu_w, \
    __u32x16: _vx512_addu_w, \
    __u64x2:  _vx128_addu_l, \
    __u64x4:  _vx256_addu_l, \
    __u64x8:  _vx512_addu_l)(a, b)


#define _vadif(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_adif_d, \
    __f64x4:  _vx256_adif_d, \
    __f64x8:  _vx512_adif_d, \
    __f32x4:  _vx128_adif_f, \
    __f32x8:  _vx256_adif_f, \
    __f32x16: _vx512_adif_f)(a, b)


#define _vage(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_age_b, \
    __u8x32:  _vx256_age_b, \
    __u8x64:  _vx512_age_b, \
    __f64x2:  _vx128_age_d, \
    __f64x4:  _vx256_age_d, \
    __f64x8:  _vx512_age_d, \
    __f32x4:  _vx128_age_f, \
    __f32x8:  _vx256_age_f, \
    __f32x16: _vx512_age_f, \
    __u16x8:  _vx128_age_h, \
    __u16x16: _vx256_age_h, \
    __u16x32: _vx512_age_h, \
    __u64x2:  _vx128_age_l, \
    __u64x4:  _vx256_age_l, \
    __u64x8:  _vx512_age_l, \
    __u32x4:  _vx128_age_w, \
    __u32x8:  _vx256_age_w, \
    __u32x16: _vx512_age_w)(a, b)


#define _vagt(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_agt_b, \
    __u8x32:  _vx256_agt_b, \
    __u8x64:  _vx512_agt_b, \
    __f64x2:  _vx128_agt_d, \
    __f64x4:  _vx256_agt_d, \
    __f64x8:  _vx512_agt_d, \
    __f32x4:  _vx128_agt_f, \
    __f32x8:  _vx256_agt_f, \
    __f32x16: _vx512_agt_f, \
    __u16x8:  _vx128_agt_h, \
    __u16x16: _vx256_agt_h, \
    __u16x32: _vx512_agt_h, \
    __u64x2:  _vx128_agt_l, \
    __u64x4:  _vx256_agt_l, \
    __u64x8:  _vx512_agt_l, \
    __u32x4:  _vx128_agt_w, \
    __u32x8:  _vx256_agt_w, \
    __u32x16: _vx512_agt_w)(a, b)


#define _vale(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_ale_b, \
    __u8x32:  _vx256_ale_b, \
    __u8x64:  _vx512_ale_b, \
    __f64x2:  _vx128_ale_d, \
    __f64x4:  _vx256_ale_d, \
    __f64x8:  _vx512_ale_d, \
    __f32x4:  _vx128_ale_f, \
    __f32x8:  _vx256_ale_f, \
    __f32x16: _vx512_ale_f, \
    __u16x8:  _vx128_ale_h, \
    __u16x16: _vx256_ale_h, \
    __u16x32: _vx512_ale_h, \
    __u64x2:  _vx128_ale_l, \
    __u64x4:  _vx256_ale_l, \
    __u64x8:  _vx512_ale_l, \
    __u32x4:  _vx128_ale_w, \
    __u32x8:  _vx256_ale_w, \
    __u32x16: _vx512_ale_w)(a, b)


#define _valt(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_alt_b, \
    __u8x32:  _vx256_alt_b, \
    __u8x64:  _vx512_alt_b, \
    __f64x2:  _vx128_alt_d, \
    __f64x4:  _vx256_alt_d, \
    __f64x8:  _vx512_alt_d, \
    __f32x4:  _vx128_alt_f, \
    __f32x8:  _vx256_alt_f, \
    __f32x16: _vx512_alt_f, \
    __u16x8:  _vx128_alt_h, \
    __u16x16: _vx256_alt_h, \
    __u16x32: _vx512_alt_h, \
    __u64x2:  _vx128_alt_l, \
    __u64x4:  _vx256_alt_l, \
    __u64x8:  _vx512_alt_l, \
    __u32x4:  _vx128_alt_w, \
    __u32x8:  _vx256_alt_w, \
    __u32x16: _vx512_alt_w)(a, b)


#define _vamax(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_amax_d, \
    __f64x4:  _vx256_amax_d, \
    __f64x8:  _vx512_amax_d, \
    __f32x4:  _vx128_amax_f, \
    __f32x8:  _vx256_amax_f, \
    __f32x16: _vx512_amax_f)(a, b)


#define _vamin(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_amin_d, \
    __f64x4:  _vx256_amin_d, \
    __f64x8:  _vx512_amin_d, \
    __f32x4:  _vx128_amin_f, \
    __f32x8:  _vx256_amin_f, \
    __f32x16: _vx512_amin_f)(a, b)


#define _vand(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_and_b, \
    __u8x32:  _vx256_and_b, \
    __u8x64:  _vx512_and_b, \
    __u16x8:  _vx128_and_h, \
    __u16x16: _vx256_and_h, \
    __u16x32: _vx512_and_h, \
    __u32x4:  _vx128_and_w, \
    __u32x8:  _vx256_and_w, \
    __u32x16: _vx512_and_w, \
    __u64x2:  _vx128_and_l, \
    __u64x4:  _vx256_and_l, \
    __u64x8:  _vx512_and_l)(a, b)


#define _vann(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_ann_b, \
    __u8x32:  _vx256_ann_b, \
    __u8x64:  _vx512_ann_b, \
    __u16x8:  _vx128_ann_h, \
    __u16x16: _vx256_ann_h, \
    __u16x32: _vx512_ann_h, \
    __u32x4:  _vx128_ann_w, \
    __u32x8:  _vx256_ann_w, \
    __u32x16: _vx512_ann_w, \
    __u64x2:  _vx128_ann_l, \
    __u64x4:  _vx256_ann_l, \
    __u64x8:  _vx512_ann_l)(a, b)


#define _vavg(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_avg_b, \
    __u8x32:  _vx256_avg_b, \
    __u8x64:  _vx512_avg_b, \
    __u16x8:  _vx128_avg_h, \
    __u16x16: _vx256_avg_h, \
    __u16x32: _vx512_avg_h, \
    __u32x4:  _vx128_avg_w, \
    __u32x8:  _vx256_avg_w, \
    __u32x16: _vx512_avg_w, \
    __u64x2:  _vx128_avg_l, \
    __u64x4:  _vx256_avg_l, \
    __u64x8:  _vx512_avg_l)(a, b)


#define _vclz(a) \
    _Generic(a, \
    __u8x16:  _vx128_clz_b, \
    __u8x32:  _vx256_clz_b, \
    __u8x64:  _vx512_clz_b, \
    __u16x8:  _vx128_clz_h, \
    __u16x16: _vx256_clz_h, \
    __u16x32: _vx512_clz_h, \
    __u32x4:  _vx128_clz_w, \
    __u32x8:  _vx256_clz_w, \
    __u32x16: _vx512_clz_w, \
    __u64x2:  _vx128_clz_l, \
    __u64x4:  _vx256_clz_l, \
    __u64x8:  _vx512_clz_l)(a)


#define _vctz(a) \
    _Generic(a, \
    __u8x16:  _vx128_ctz_b, \
    __u8x32:  _vx256_ctz_b, \
    __u8x64:  _vx512_ctz_b, \
    __u16x8:  _vx128_ctz_h, \
    __u16x16: _vx256_ctz_h, \
    __u16x32: _vx512_ctz_h, \
    __u32x4:  _vx128_ctz_w, \
    __u32x8:  _vx256_ctz_w, \
    __u32x16: _vx512_ctz_w, \
    __u64x2:  _vx128_ctz_l, \
    __u64x4:  _vx256_ctz_l, \
    __u64x8:  _vx512_ctz_l)(a)


#define _vcvt_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvt_d2l, \
    __f64x4:  _vx256_cvt_d2l, \
    __f64x8:  _vx512_cvt_d2l)(a)


#define _vcvt_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvt_d2w, \
    __f64x8:  _vx256_cvt_d2w)(a)


#define _vcvt_f2d(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvt_f2d, \
    __f32x8:  _vx512_cvt_f2d)(a)


#define _vcvt_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvt_f2l, \
    __f32x8:  _vx512_cvt_f2l)(a)


#define _vcvt_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvt_f2w, \
    __f32x8:  _vx256_cvt_f2w, \
    __f32x16: _vx512_cvt_f2w)(a)


#define _vcvt_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvt_l2d, \
    __s64x4:  _vx256_cvt_l2d, \
    __s64x8:  _vx512_cvt_l2d)(a)


#define _vcvt_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvt_l2f, \
    __s64x8:  _vx256_cvt_l2f)(a)


#define _vcvt_w2d(a) \
    _Generic(a, \
    __s32x4:  _vx256_cvt_w2d, \
    __s32x8:  _vx512_cvt_w2d)(a)


#define _vcvt_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvt_w2f, \
    __s32x8:  _vx256_cvt_w2f, \
    __s32x16: _vx512_cvt_w2f)(a)


#define _vcvta_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvta_d2l, \
    __f64x4:  _vx256_cvta_d2l, \
    __f64x8:  _vx512_cvta_d2l)(a)


#define _vcvta_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvta_d2w, \
    __f64x8:  _vx256_cvta_d2w)(a)


#define _vcvta_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvta_f2l, \
    __f32x8:  _vx512_cvta_f2l)(a)


#define _vcvta_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvta_f2w, \
    __f32x8:  _vx256_cvta_f2w, \
    __f32x16: _vx512_cvta_f2w)(a)


#define _vcvta_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvta_l2d, \
    __s64x4:  _vx256_cvta_l2d, \
    __s64x8:  _vx512_cvta_l2d)(a)


#define _vcvta_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvta_l2f, \
    __s64x8:  _vx256_cvta_l2f)(a)


#define _vcvta_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvta_w2f, \
    __s32x8:  _vx256_cvta_w2f, \
    __s32x16: _vx512_cvta_w2f)(a)


#define _vcvtn_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtn_d2l, \
    __f64x4:  _vx256_cvtn_d2l, \
    __f64x8:  _vx512_cvtn_d2l)(a)


#define _vcvtn_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtn_d2w, \
    __f64x8:  _vx256_cvtn_d2w)(a)


#define _vcvtn_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtn_f2l, \
    __f32x8:  _vx512_cvtn_f2l)(a)


#define _vcvtn_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtn_f2w, \
    __f32x8:  _vx256_cvtn_f2w, \
    __f32x16: _vx512_cvtn_f2w)(a)


#define _vcvtn_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtn_l2d, \
    __s64x4:  _vx256_cvtn_l2d, \
    __s64x8:  _vx512_cvtn_l2d)(a)


#define _vcvtn_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtn_l2f, \
    __s64x8:  _vx256_cvtn_l2f)(a)


#define _vcvtn_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtn_w2f, \
    __s32x8:  _vx256_cvtn_w2f, \
    __s32x16: _vx512_cvtn_w2f)(a)


#define _vcvtp_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtp_d2l, \
    __f64x4:  _vx256_cvtp_d2l, \
    __f64x8:  _vx512_cvtp_d2l)(a)


#define _vcvtp_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtp_d2w, \
    __f64x8:  _vx256_cvtp_d2w)(a)


#define _vcvtp_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtp_f2l, \
    __f32x8:  _vx512_cvtp_f2l)(a)


#define _vcvtp_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtp_f2w, \
    __f32x8:  _vx256_cvtp_f2w, \
    __f32x16: _vx512_cvtp_f2w)(a)


#define _vcvtp_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtp_l2d, \
    __s64x4:  _vx256_cvtp_l2d, \
    __s64x8:  _vx512_cvtp_l2d)(a)


#define _vcvtp_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtp_l2f, \
    __s64x8:  _vx256_cvtp_l2f)(a)


#define _vcvtp_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtp_w2f, \
    __s32x8:  _vx256_cvtp_w2f, \
    __s32x16: _vx512_cvtp_w2f)(a)


#define _vcvtr_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtr_d2l, \
    __f64x4:  _vx256_cvtr_d2l, \
    __f64x8:  _vx512_cvtr_d2l)(a)


#define _vcvtr_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtr_d2w, \
    __f64x8:  _vx256_cvtr_d2w)(a)


#define _vcvtr_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtr_f2l, \
    __f32x8:  _vx512_cvtr_f2l)(a)


#define _vcvtr_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtr_f2w, \
    __f32x8:  _vx256_cvtr_f2w, \
    __f32x16: _vx512_cvtr_f2w)(a)


#define _vcvtr_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtr_l2d, \
    __s64x4:  _vx256_cvtr_l2d, \
    __s64x8:  _vx512_cvtr_l2d)(a)


#define _vcvtr_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtr_l2f, \
    __s64x8:  _vx256_cvtr_l2f)(a)


#define _vcvtr_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtr_w2f, \
    __s32x8:  _vx256_cvtr_w2f, \
    __s32x16: _vx512_cvtr_w2f)(a)


#define _vcvts_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvts_d2l, \
    __f64x4:  _vx256_cvts_d2l, \
    __f64x8:  _vx512_cvts_d2l)(a)


#define _vcvts_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvts_d2w, \
    __f64x8:  _vx256_cvts_d2w)(a)


#define _vcvts_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvts_f2l, \
    __f32x8:  _vx512_cvts_f2l)(a)


#define _vcvts_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvts_f2w, \
    __f32x8:  _vx256_cvts_f2w, \
    __f32x16: _vx512_cvts_f2w)(a)


#define _vcvts_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvts_l2d, \
    __s64x4:  _vx256_cvts_l2d, \
    __s64x8:  _vx512_cvts_l2d)(a)


#define _vcvts_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvts_l2f, \
    __s64x8:  _vx256_cvts_l2f)(a)


#define _vcvts_w2d(a) \
    _Generic(a, \
    __s32x4:  _vx256_cvts_w2d, \
    __s32x8:  _vx512_cvts_w2d)(a)


#define _vcvts_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvts_w2f, \
    __s32x8:  _vx256_cvts_w2f, \
    __s32x16: _vx512_cvts_w2f)(a)


#define _vcvtsa_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtsa_d2l, \
    __f64x4:  _vx256_cvtsa_d2l, \
    __f64x8:  _vx512_cvtsa_d2l)(a)


#define _vcvtsa_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtsa_d2w, \
    __f64x8:  _vx256_cvtsa_d2w)(a)


#define _vcvtsa_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtsa_f2l, \
    __f32x8:  _vx512_cvtsa_f2l)(a)


#define _vcvtsa_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtsa_f2w, \
    __f32x8:  _vx256_cvtsa_f2w, \
    __f32x16: _vx512_cvtsa_f2w)(a)


#define _vcvtsa_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtsa_l2d, \
    __s64x4:  _vx256_cvtsa_l2d, \
    __s64x8:  _vx512_cvtsa_l2d)(a)


#define _vcvtsa_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtsa_l2f, \
    __s64x8:  _vx256_cvtsa_l2f)(a)


#define _vcvtsa_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtsa_w2f, \
    __s32x8:  _vx256_cvtsa_w2f, \
    __s32x16: _vx512_cvtsa_w2f)(a)


#define _vcvtsn_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtsn_d2l, \
    __f64x4:  _vx256_cvtsn_d2l, \
    __f64x8:  _vx512_cvtsn_d2l)(a)


#define _vcvtsn_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtsn_d2w, \
    __f64x8:  _vx256_cvtsn_d2w)(a)


#define _vcvtsn_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtsn_f2l, \
    __f32x8:  _vx512_cvtsn_f2l)(a)


#define _vcvtsn_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtsn_f2w, \
    __f32x8:  _vx256_cvtsn_f2w, \
    __f32x16: _vx512_cvtsn_f2w)(a)


#define _vcvtsn_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtsn_l2d, \
    __s64x4:  _vx256_cvtsn_l2d, \
    __s64x8:  _vx512_cvtsn_l2d)(a)


#define _vcvtsn_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtsn_l2f, \
    __s64x8:  _vx256_cvtsn_l2f)(a)


#define _vcvtsn_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtsn_w2f, \
    __s32x8:  _vx256_cvtsn_w2f, \
    __s32x16: _vx512_cvtsn_w2f)(a)


#define _vcvtsp_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtsp_d2l, \
    __f64x4:  _vx256_cvtsp_d2l, \
    __f64x8:  _vx512_cvtsp_d2l)(a)


#define _vcvtsp_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtsp_d2w, \
    __f64x8:  _vx256_cvtsp_d2w)(a)


#define _vcvtsp_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtsp_f2l, \
    __f32x8:  _vx512_cvtsp_f2l)(a)


#define _vcvtsp_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtsp_f2w, \
    __f32x8:  _vx256_cvtsp_f2w, \
    __f32x16: _vx512_cvtsp_f2w)(a)


#define _vcvtsp_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtsp_l2d, \
    __s64x4:  _vx256_cvtsp_l2d, \
    __s64x8:  _vx512_cvtsp_l2d)(a)


#define _vcvtsp_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtsp_l2f, \
    __s64x8:  _vx256_cvtsp_l2f)(a)


#define _vcvtsp_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtsp_w2f, \
    __s32x8:  _vx256_cvtsp_w2f, \
    __s32x16: _vx512_cvtsp_w2f)(a)


#define _vcvtsr_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtsr_d2l, \
    __f64x4:  _vx256_cvtsr_d2l, \
    __f64x8:  _vx512_cvtsr_d2l)(a)


#define _vcvtsr_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtsr_d2w, \
    __f64x8:  _vx256_cvtsr_d2w)(a)


#define _vcvtsr_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtsr_f2l, \
    __f32x8:  _vx512_cvtsr_f2l)(a)


#define _vcvtsr_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtsr_f2w, \
    __f32x8:  _vx256_cvtsr_f2w, \
    __f32x16: _vx512_cvtsr_f2w)(a)


#define _vcvtsr_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtsr_l2d, \
    __s64x4:  _vx256_cvtsr_l2d, \
    __s64x8:  _vx512_cvtsr_l2d)(a)


#define _vcvtsr_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtsr_l2f, \
    __s64x8:  _vx256_cvtsr_l2f)(a)


#define _vcvtsr_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtsr_w2f, \
    __s32x8:  _vx256_cvtsr_w2f, \
    __s32x16: _vx512_cvtsr_w2f)(a)


#define _vcvtst_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtst_d2l, \
    __f64x4:  _vx256_cvtst_d2l, \
    __f64x8:  _vx512_cvtst_d2l)(a)


#define _vcvtst_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtst_d2w, \
    __f64x8:  _vx256_cvtst_d2w)(a)


#define _vcvtst_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtst_f2l, \
    __f32x8:  _vx512_cvtst_f2l)(a)


#define _vcvtst_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtst_f2w, \
    __f32x8:  _vx256_cvtst_f2w, \
    __f32x16: _vx512_cvtst_f2w)(a)


#define _vcvtst_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtst_l2d, \
    __s64x4:  _vx256_cvtst_l2d, \
    __s64x8:  _vx512_cvtst_l2d)(a)


#define _vcvtst_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtst_l2f, \
    __s64x8:  _vx256_cvtst_l2f)(a)


#define _vcvtst_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtst_w2f, \
    __s32x8:  _vx256_cvtst_w2f, \
    __s32x16: _vx512_cvtst_w2f)(a)


#define _vcvtsx_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtsx_d2l, \
    __f64x4:  _vx256_cvtsx_d2l, \
    __f64x8:  _vx512_cvtsx_d2l)(a)


#define _vcvtsx_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtsx_d2w, \
    __f64x8:  _vx256_cvtsx_d2w)(a)


#define _vcvtsx_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtsx_f2l, \
    __f32x8:  _vx512_cvtsx_f2l)(a)


#define _vcvtsx_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtsx_f2w, \
    __f32x8:  _vx256_cvtsx_f2w, \
    __f32x16: _vx512_cvtsx_f2w)(a)


#define _vcvtsx_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtsx_l2d, \
    __s64x4:  _vx256_cvtsx_l2d, \
    __s64x8:  _vx512_cvtsx_l2d)(a)


#define _vcvtsx_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtsx_l2f, \
    __s64x8:  _vx256_cvtsx_l2f)(a)


#define _vcvtsx_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtsx_w2f, \
    __s32x8:  _vx256_cvtsx_w2f, \
    __s32x16: _vx512_cvtsx_w2f)(a)


#define _vcvtt_d2f(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtt_d2f, \
    __f64x8:  _vx256_cvtt_d2f)(a)


#define _vcvtt_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtt_d2l, \
    __f64x4:  _vx256_cvtt_d2l, \
    __f64x8:  _vx512_cvtt_d2l)(a)


#define _vcvtt_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtt_d2w, \
    __f64x8:  _vx256_cvtt_d2w)(a)


#define _vcvtt_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtt_f2l, \
    __f32x8:  _vx512_cvtt_f2l)(a)


#define _vcvtt_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtt_f2w, \
    __f32x8:  _vx256_cvtt_f2w, \
    __f32x16: _vx512_cvtt_f2w)(a)


#define _vcvtt_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtt_l2d, \
    __s64x4:  _vx256_cvtt_l2d, \
    __s64x8:  _vx512_cvtt_l2d)(a)


#define _vcvtt_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtt_l2f, \
    __s64x8:  _vx256_cvtt_l2f)(a)


#define _vcvtt_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtt_w2f, \
    __s32x8:  _vx256_cvtt_w2f, \
    __s32x16: _vx512_cvtt_w2f)(a)


#define _vcvtx_d2l(a) \
    _Generic(a, \
    __f64x2:  _vx128_cvtx_d2l, \
    __f64x4:  _vx256_cvtx_d2l, \
    __f64x8:  _vx512_cvtx_d2l)(a)


#define _vcvtx_d2w(a) \
    _Generic(a, \
    __f64x4:  _vx128_cvtx_d2w, \
    __f64x8:  _vx256_cvtx_d2w)(a)


#define _vcvtx_f2l(a) \
    _Generic(a, \
    __f32x4:  _vx256_cvtx_f2l, \
    __f32x8:  _vx512_cvtx_f2l)(a)


#define _vcvtx_f2w(a) \
    _Generic(a, \
    __f32x4:  _vx128_cvtx_f2w, \
    __f32x8:  _vx256_cvtx_f2w, \
    __f32x16: _vx512_cvtx_f2w)(a)


#define _vcvtx_l2d(a) \
    _Generic(a, \
    __s64x2:  _vx128_cvtx_l2d, \
    __s64x4:  _vx256_cvtx_l2d, \
    __s64x8:  _vx512_cvtx_l2d)(a)


#define _vcvtx_l2f(a) \
    _Generic(a, \
    __s64x4:  _vx128_cvtx_l2f, \
    __s64x8:  _vx256_cvtx_l2f)(a)


#define _vcvtx_w2f(a) \
    _Generic(a, \
    __s32x4:  _vx128_cvtx_w2f, \
    __s32x8:  _vx256_cvtx_w2f, \
    __s32x16: _vx512_cvtx_w2f)(a)


#define _vdiv(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_div_b, \
    __u16x32: _vx512_div_h, \
    __u32x16: _vx512_div_w, \
    __u64x8:  _vx512_div_l, \
    __f64x8:  _vx512_div_d, \
    __f32x16: _vx512_div_f)(a, b)


#define _vdivs(a, b) \
    _Generic(a, \
    __s8x64:  _vx512_divs_b, \
    __s16x32: _vx512_divs_h, \
    __s32x16: _vx512_divs_w, \
    __s64x8:  _vx512_divs_l)(a, b)


#define _vdivsz(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_divsz_b, \
    __s8x32:  _vx256_divsz_b, \
    __s8x64:  _vx512_divsz_b, \
    __s16x8:  _vx128_divsz_h, \
    __s16x16: _vx256_divsz_h, \
    __s16x32: _vx512_divsz_h, \
    __s32x4:  _vx128_divsz_w, \
    __s32x8:  _vx256_divsz_w, \
    __s32x16: _vx512_divsz_w, \
    __s64x2:  _vx128_divsz_l, \
    __s64x4:  _vx256_divsz_l, \
    __s64x8:  _vx512_divsz_l)(a, b)


#define _vdivz(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_divz_b, \
    __u8x32:  _vx256_divz_b, \
    __u8x64:  _vx512_divz_b, \
    __u16x8:  _vx128_divz_h, \
    __u16x16: _vx256_divz_h, \
    __u16x32: _vx512_divz_h, \
    __u32x4:  _vx128_divz_w, \
    __u32x8:  _vx256_divz_w, \
    __u32x16: _vx512_divz_w, \
    __u64x2:  _vx128_divz_l, \
    __u64x4:  _vx256_divz_l, \
    __u64x8:  _vx512_divz_l)(a, b)


#define _veq(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_eq_b, \
    __u8x32:  _vx256_eq_b, \
    __u8x64:  _vx512_eq_b, \
    __u16x8:  _vx128_eq_h, \
    __u16x16: _vx256_eq_h, \
    __u16x32: _vx512_eq_h, \
    __u32x4:  _vx128_eq_w, \
    __u32x8:  _vx256_eq_w, \
    __u32x16: _vx512_eq_w, \
    __u64x2:  _vx128_eq_l, \
    __u64x4:  _vx256_eq_l, \
    __u64x8:  _vx512_eq_l, \
    __f64x2:  _vx128_eq_d, \
    __f64x4:  _vx256_eq_d, \
    __f64x8:  _vx512_eq_d, \
    __f32x4:  _vx128_eq_f, \
    __f32x8:  _vx256_eq_f, \
    __f32x16: _vx512_eq_f)(a, b)


#define _veq_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_eq_ld, \
    __f64x4:  _vx256_eq_ld, \
    __f64x8:  _vx512_eq_ld, \
    __f32x4:  _vx128_eq_wf, \
    __f32x8:  _vx256_eq_wf, \
    __f32x16: _vx512_eq_wf)(a, b)


#define _verf(a) \
    _Generic(a, \
    __f64x2:  _vx128_erf_d, \
    __f64x4:  _vx256_erf_d, \
    __f64x8:  _vx512_erf_d, \
    __f32x4:  _vx128_erf_f, \
    __f32x8:  _vx256_erf_f, \
    __f32x16: _vx512_erf_f)(a)


#define _vexp2(a) \
    _Generic(a, \
    __f64x2:  _vx128_exp2_d, \
    __f64x4:  _vx256_exp2_d, \
    __f64x8:  _vx512_exp2_d, \
    __f32x4:  _vx128_exp2_f, \
    __f32x8:  _vx256_exp2_f, \
    __f32x16: _vx512_exp2_f)(a)


#define _vexpe(a) \
    _Generic(a, \
    __f64x2:  _vx128_expe_d, \
    __f64x4:  _vx256_expe_d, \
    __f64x8:  _vx512_expe_d, \
    __f32x4:  _vx128_expe_f, \
    __f32x8:  _vx256_expe_f, \
    __f32x16: _vx512_expe_f)(a)


#define _vfill(a) \
    _Generic(a, \
    __u8x16:  _vx128_fillv_b, \
    __u8x32:  _vx256_fillv_b, \
    __u8x64:  _vx512_fillv_b, \
    __u16x8:  _vx128_fillv_h, \
    __u16x16: _vx256_fillv_h, \
    __u16x32: _vx512_fillv_h, \
    __u32x4:  _vx128_fillv_w, \
    __u32x8:  _vx256_fillv_w, \
    __u32x16: _vx512_fillv_w, \
    __u64x2:  _vx128_fillv_l, \
    __u64x4:  _vx256_fillv_l, \
    __u64x8:  _vx512_fillv_l, \
    uint8_t:  _vx512_fill_b, \
    uint16_t: _vx512_fill_h, \
    uint32_t: _vx512_fill_w, \
    uint64_t: _vx512_fill_l)(a)


#define _vge(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_ge_b, \
    __s8x32:  _vx256_ge_b, \
    __s8x64:  _vx512_ge_b, \
    __s16x8:  _vx128_ge_h, \
    __s16x16: _vx256_ge_h, \
    __s16x32: _vx512_ge_h, \
    __s32x4:  _vx128_ge_w, \
    __s32x8:  _vx256_ge_w, \
    __s32x16: _vx512_ge_w, \
    __s64x2:  _vx128_ge_l, \
    __s64x4:  _vx256_ge_l, \
    __s64x8:  _vx512_ge_l, \
    __f64x2:  _vx128_ge_d, \
    __f64x4:  _vx256_ge_d, \
    __f64x8:  _vx512_ge_d, \
    __f32x4:  _vx128_ge_f, \
    __f32x8:  _vx256_ge_f, \
    __f32x16: _vx512_ge_f)(a, b)


#define _vge_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_ge_ld, \
    __f64x4:  _vx256_ge_ld, \
    __f64x8:  _vx512_ge_ld, \
    __f32x4:  _vx128_ge_wf, \
    __f32x8:  _vx256_ge_wf, \
    __f32x16: _vx512_ge_wf)(a, b)


#define _vgt(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_gt_d, \
    __f64x4:  _vx256_gt_d, \
    __f64x8:  _vx512_gt_d, \
    __f32x4:  _vx128_gt_f, \
    __f32x8:  _vx256_gt_f, \
    __f32x16: _vx512_gt_f)(a, b)


#define _vhs(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_hs_b, \
    __u8x32:  _vx256_hs_b, \
    __u8x64:  _vx512_hs_b, \
    __u16x8:  _vx128_hs_h, \
    __u16x16: _vx256_hs_h, \
    __u16x32: _vx512_hs_h, \
    __u32x4:  _vx128_hs_w, \
    __u32x8:  _vx256_hs_w, \
    __u32x16: _vx512_hs_w, \
    __u64x2:  _vx128_hs_l, \
    __u64x4:  _vx256_hs_l, \
    __u64x8:  _vx512_hs_l)(a, b)


#define _vins(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_ins_b, \
    __u8x32:  _vx256_ins_b, \
    __u8x64:  _vx512_ins_b, \
    __u16x8:  _vx128_ins_h, \
    __u16x16: _vx256_ins_h, \
    __u16x32: _vx512_ins_h, \
    __u64x2:  _vx128_ins_l, \
    __u64x4:  _vx256_ins_l, \
    __u64x8:  _vx512_ins_l, \
    __u32x4:  _vx128_insv_w, \
    __u32x8:  _vx256_insv_w, \
    __u32x16: _vx512_insv_w)(a, b, c)


#define _vinsx(a, b, c) \
    _Generic(a, \
    __u8x32:  _vx256_insx_b, \
    __u8x64:  _vx512_insx_b, \
    __u16x16: _vx256_insx_h, \
    __u16x32: _vx512_insx_h, \
    __u64x4:  _vx256_insx_l, \
    __u64x8:  _vx512_insx_l, \
    __u32x8:  _vx256_insx_w, \
    __u32x16: _vx512_insx_w)(a, b, c)


#define _vinsxx(a, b, c) \
    _Generic(a, \
    __u8x64:  _vx512_insxx_b, \
    __u16x32: _vx512_insxx_h, \
    __u64x8:  _vx512_insxx_l, \
    __u32x16: _vx512_insxx_w)(a, b, c)


#define _vle(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_le_d, \
    __f64x4:  _vx256_le_d, \
    __f64x8:  _vx512_le_d, \
    __f32x4:  _vx128_le_f, \
    __f32x8:  _vx256_le_f, \
    __f32x16: _vx512_le_f)(a, b)


#define _vlo(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_lo_b, \
    __u8x32:  _vx256_lo_b, \
    __u8x64:  _vx512_lo_b, \
    __u16x8:  _vx128_lo_h, \
    __u16x16: _vx256_lo_h, \
    __u16x32: _vx512_lo_h, \
    __u32x4:  _vx128_lo_w, \
    __u32x8:  _vx256_lo_w, \
    __u32x16: _vx512_lo_w, \
    __u64x2:  _vx128_lo_l, \
    __u64x4:  _vx256_lo_l, \
    __u64x8:  _vx512_lo_l)(a, b)


#define _vlog2(a) \
    _Generic(a, \
    __f64x2:  _vx128_log2_d, \
    __f64x4:  _vx256_log2_d, \
    __f64x8:  _vx512_log2_d, \
    __f32x4:  _vx128_log2_f, \
    __f32x8:  _vx256_log2_f, \
    __f32x16: _vx512_log2_f)(a)


#define _vloge(a) \
    _Generic(a, \
    __f64x2:  _vx128_loge_d, \
    __f64x4:  _vx256_loge_d, \
    __f64x8:  _vx512_loge_d, \
    __f32x4:  _vx128_loge_f, \
    __f32x8:  _vx256_loge_f, \
    __f32x16: _vx512_loge_f)(a)


#define _vlt(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_lt_b, \
    __s8x32:  _vx256_lt_b, \
    __s8x64:  _vx512_lt_b, \
    __s16x8:  _vx128_lt_h, \
    __s16x16: _vx256_lt_h, \
    __s16x32: _vx512_lt_h, \
    __s32x4:  _vx128_lt_w, \
    __s32x8:  _vx256_lt_w, \
    __s32x16: _vx512_lt_w, \
    __s64x2:  _vx128_lt_l, \
    __s64x4:  _vx256_lt_l, \
    __s64x8:  _vx512_lt_l, \
    __f64x2:  _vx128_lt_d, \
    __f64x4:  _vx256_lt_d, \
    __f64x8:  _vx512_lt_d, \
    __f32x4:  _vx128_lt_f, \
    __f32x8:  _vx256_lt_f, \
    __f32x16: _vx512_lt_f)(a, b)


#define _vlt_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_lt_ld, \
    __f64x4:  _vx256_lt_ld, \
    __f64x8:  _vx512_lt_ld, \
    __f32x4:  _vx128_lt_wf, \
    __f32x8:  _vx256_lt_wf, \
    __f32x16: _vx512_lt_wf)(a, b)


#define _vmadd(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_madd_b, \
    __s8x16:  _vx128_madd_b_s, \
    __u8x32:  _vx256_madd_b, \
    __s8x32:  _vx256_madd_b_s, \
    __u8x64:  _vx512_madd_b, \
    __s8x64:  _vx512_madd_b_s, \
    __u16x8:  _vx128_madd_h, \
    __s16x8:  _vx128_madd_h_s, \
    __u16x16: _vx256_madd_h, \
    __s16x16: _vx256_madd_h_s, \
    __u16x32: _vx512_madd_h, \
    __s16x32: _vx512_madd_h_s, \
    __u32x4:  _vx128_madd_w, \
    __s32x4:  _vx128_madd_w_s, \
    __u32x8:  _vx256_madd_w, \
    __s32x8:  _vx256_madd_w_s, \
    __u32x16: _vx512_madd_w, \
    __s32x16: _vx512_madd_w_s, \
    __u64x2:  _vx128_madd_l, \
    __s64x2:  _vx128_madd_l_s, \
    __u64x4:  _vx256_madd_l, \
    __s64x4:  _vx256_madd_l_s, \
    __u64x8:  _vx512_madd_l, \
    __s64x8:  _vx512_madd_l_s, \
    __f64x2:  _vx128_madd_d, \
    __f64x4:  _vx256_madd_d, \
    __f64x8:  _vx512_madd_d, \
    __f32x4:  _vx128_madd_f, \
    __f32x8:  _vx256_madd_f, \
    __f32x16: _vx512_madd_f)(a, b, c)


#define _vmadds(a, b, c) \
    _Generic(a, \
    __s8x16:  _vx128_madds_b, \
    __s8x32:  _vx256_madds_b, \
    __s8x64:  _vx512_madds_b, \
    __s16x8:  _vx128_madds_h, \
    __s16x16: _vx256_madds_h, \
    __s16x32: _vx512_madds_h, \
    __s32x4:  _vx128_madds_w, \
    __s32x8:  _vx256_madds_w, \
    __s32x16: _vx512_madds_w, \
    __s64x2:  _vx128_madds_l, \
    __s64x4:  _vx256_madds_l, \
    __s64x8:  _vx512_madds_l)(a, b, c)


#define _vmaddu(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_maddu_b, \
    __u8x32:  _vx256_maddu_b, \
    __u8x64:  _vx512_maddu_b, \
    __u16x8:  _vx128_maddu_h, \
    __u16x16: _vx256_maddu_h, \
    __u16x32: _vx512_maddu_h, \
    __u32x4:  _vx128_maddu_w, \
    __u32x8:  _vx256_maddu_w, \
    __u32x16: _vx512_maddu_w, \
    __u64x2:  _vx128_maddu_l, \
    __u64x4:  _vx256_maddu_l, \
    __u64x8:  _vx512_maddu_l)(a, b, c)


#define _vmas(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_mas_d, \
    __f64x4:  _vx256_mas_d, \
    __f64x8:  _vx512_mas_d, \
    __f32x4:  _vx128_mas_f, \
    __f32x8:  _vx256_mas_f, \
    __f32x16: _vx512_mas_f)(a, b, c)


#define _vmax(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_max_b, \
    __u8x32:  _vx256_max_b, \
    __u8x64:  _vx512_max_b, \
    __u16x8:  _vx128_max_h, \
    __u16x16: _vx256_max_h, \
    __u16x32: _vx512_max_h, \
    __u32x4:  _vx128_max_w, \
    __u32x8:  _vx256_max_w, \
    __u32x16: _vx512_max_w, \
    __u64x2:  _vx128_max_l, \
    __u64x4:  _vx256_max_l, \
    __u64x8:  _vx512_max_l, \
    __f64x2:  _vx128_max_d, \
    __f64x4:  _vx256_max_d, \
    __f64x8:  _vx512_max_d, \
    __f32x4:  _vx128_max_f, \
    __f32x8:  _vx256_max_f, \
    __f32x16: _vx512_max_f)(a, b)


#define _vmaxn(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_maxn_d, \
    __f64x4:  _vx256_maxn_d, \
    __f64x8:  _vx512_maxn_d, \
    __f32x4:  _vx128_maxn_f, \
    __f32x8:  _vx256_maxn_f, \
    __f32x16: _vx512_maxn_f)(a, b)


#define _vmaxp(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_maxp_b, \
    __f64x8:  _vx512_maxp_d, \
    __f32x16: _vx512_maxp_f, \
    __u16x32: _vx512_maxp_h, \
    __u8x16:  _vx128_maxp_b, \
    __u8x32:  _vx256_maxp_b, \
    __f64x2:  _vx128_maxp_d, \
    __f64x4:  _vx256_maxp_d, \
    __f32x4:  _vx128_maxp_f, \
    __f32x8:  _vx256_maxp_f, \
    __u16x8:  _vx128_maxp_h, \
    __u16x16: _vx256_maxp_h, \
    __u64x2:  _vx128_maxp_l, \
    __u64x4:  _vx256_maxp_l, \
    __u64x8:  _vx512_maxp2_l, \
    __u32x4:  _vx128_maxp_w, \
    __u32x8:  _vx256_maxp_w, \
    __u32x16: _vx512_maxp2_w)(a, b)


#define _vmaxs(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_maxs_b, \
    __s8x32:  _vx256_maxs_b, \
    __s8x64:  _vx512_maxs_b, \
    __s16x8:  _vx128_maxs_h, \
    __s16x16: _vx256_maxs_h, \
    __s16x32: _vx512_maxs_h, \
    __s32x4:  _vx128_maxs_w, \
    __s32x8:  _vx256_maxs_w, \
    __s32x16: _vx512_maxs_w, \
    __s64x2:  _vx128_maxs_l, \
    __s64x4:  _vx256_maxs_l, \
    __s64x8:  _vx512_maxs_l)(a, b)


#define _vmaxsp(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_maxsp_b, \
    __u16x32: _vx512_maxsp_h, \
    __u8x16:  _vx128_maxsp_b, \
    __u8x32:  _vx256_maxsp_b, \
    __u16x8:  _vx128_maxsp_h, \
    __u16x16: _vx256_maxsp_h, \
    __u64x2:  _vx128_maxsp_l, \
    __u64x4:  _vx256_maxsp_l, \
    __u64x8:  _vx512_maxsp2_l, \
    __u32x4:  _vx128_maxsp_w, \
    __u32x8:  _vx256_maxsp_w, \
    __u32x16: _vx512_maxsp2_w)(a, b)


#define _vmeq(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_meq_b, \
    __u8x32:  _vx256_meq_b, \
    __u8x64:  _vx512_meq_b, \
    __f64x2:  _vx128_meq_d, \
    __f64x4:  _vx256_meq_d, \
    __f64x8:  _vx512_meq_d, \
    __f32x4:  _vx128_meq_f, \
    __f32x8:  _vx256_meq_f, \
    __f32x16: _vx512_meq_f, \
    __u16x8:  _vx128_meq_h, \
    __u16x16: _vx256_meq_h, \
    __u16x32: _vx512_meq_h, \
    __u64x2:  _vx128_meq_l, \
    __u64x4:  _vx256_meq_l, \
    __u64x8:  _vx512_meq_l, \
    __u32x4:  _vx128_meq_w, \
    __u32x8:  _vx256_meq_w, \
    __u32x16: _vx512_meq_w)(a, b, c)


#define _vmge(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_mge_d, \
    __f64x4:  _vx256_mge_d, \
    __f64x8:  _vx512_mge_d, \
    __f32x4:  _vx128_mge_f, \
    __f32x8:  _vx256_mge_f, \
    __f32x16: _vx512_mge_f)(a, b, c)


#define _vmgt(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_mgt_d, \
    __f64x4:  _vx256_mgt_d, \
    __f64x8:  _vx512_mgt_d, \
    __f32x4:  _vx128_mgt_f, \
    __f32x8:  _vx256_mgt_f, \
    __f32x16: _vx512_mgt_f)(a, b, c)


#define _vmin(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_min_b, \
    __u8x32:  _vx256_min_b, \
    __u8x64:  _vx512_min_b, \
    __u16x8:  _vx128_min_h, \
    __u16x16: _vx256_min_h, \
    __u16x32: _vx512_min_h, \
    __u32x4:  _vx128_min_w, \
    __u32x8:  _vx256_min_w, \
    __u32x16: _vx512_min_w, \
    __u64x2:  _vx128_min_l, \
    __u64x4:  _vx256_min_l, \
    __u64x8:  _vx512_min_l, \
    __f64x2:  _vx128_min_d, \
    __f64x4:  _vx256_min_d, \
    __f64x8:  _vx512_min_d, \
    __f32x4:  _vx128_min_f, \
    __f32x8:  _vx256_min_f, \
    __f32x16: _vx512_min_f)(a, b)


#define _vminn(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_minn_d, \
    __f64x4:  _vx256_minn_d, \
    __f64x8:  _vx512_minn_d, \
    __f32x4:  _vx128_minn_f, \
    __f32x8:  _vx256_minn_f, \
    __f32x16: _vx512_minn_f)(a, b)


#define _vminp(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_minp_b, \
    __f64x8:  _vx512_minp_d, \
    __f32x16: _vx512_minp_f, \
    __u16x32: _vx512_minp_h, \
    __u8x16:  _vx128_minp_b, \
    __u8x32:  _vx256_minp_b, \
    __f64x2:  _vx128_minp_d, \
    __f64x4:  _vx256_minp_d, \
    __f32x4:  _vx128_minp_f, \
    __f32x8:  _vx256_minp_f, \
    __u16x8:  _vx128_minp_h, \
    __u16x16: _vx256_minp_h, \
    __u64x2:  _vx128_minp_l, \
    __u64x4:  _vx256_minp_l, \
    __u64x8:  _vx512_minp2_l, \
    __u32x4:  _vx128_minp_w, \
    __u32x8:  _vx256_minp_w, \
    __u32x16: _vx512_minp2_w)(a, b)


#define _vmins(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_mins_b, \
    __s8x32:  _vx256_mins_b, \
    __s8x64:  _vx512_mins_b, \
    __s16x8:  _vx128_mins_h, \
    __s16x16: _vx256_mins_h, \
    __s16x32: _vx512_mins_h, \
    __s32x4:  _vx128_mins_w, \
    __s32x8:  _vx256_mins_w, \
    __s32x16: _vx512_mins_w, \
    __s64x2:  _vx128_mins_l, \
    __s64x4:  _vx256_mins_l, \
    __s64x8:  _vx512_mins_l)(a, b)


#define _vminsp(a, b) \
    _Generic(a, \
    __u8x64:  _vx512_minsp_b, \
    __u16x32: _vx512_minsp_h, \
    __u8x16:  _vx128_minsp_b, \
    __u8x32:  _vx256_minsp_b, \
    __u16x8:  _vx128_minsp_h, \
    __u16x16: _vx256_minsp_h, \
    __u64x2:  _vx128_minsp_l, \
    __u64x4:  _vx256_minsp_l, \
    __u64x8:  _vx512_minsp2_l, \
    __u32x4:  _vx128_minsp_w, \
    __u32x8:  _vx256_minsp_w, \
    __u32x16: _vx512_minsp2_w)(a, b)


#define _vmle(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_mle_b, \
    __u8x32:  _vx256_mle_b, \
    __u8x64:  _vx512_mle_b, \
    __f64x2:  _vx128_mle_d, \
    __f64x4:  _vx256_mle_d, \
    __f64x8:  _vx512_mle_d, \
    __f32x4:  _vx128_mle_f, \
    __f32x8:  _vx256_mle_f, \
    __f32x16: _vx512_mle_f, \
    __u16x8:  _vx128_mle_h, \
    __u16x16: _vx256_mle_h, \
    __u16x32: _vx512_mle_h, \
    __u64x2:  _vx128_mle_l, \
    __u64x4:  _vx256_mle_l, \
    __u64x8:  _vx512_mle_l, \
    __u32x4:  _vx128_mle_w, \
    __u32x8:  _vx256_mle_w, \
    __u32x16: _vx512_mle_w)(a, b, c)


#define _vmlt(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_mlt_b, \
    __u8x32:  _vx256_mlt_b, \
    __u8x64:  _vx512_mlt_b, \
    __f64x2:  _vx128_mlt_d, \
    __f64x4:  _vx256_mlt_d, \
    __f64x8:  _vx512_mlt_d, \
    __f32x4:  _vx128_mlt_f, \
    __f32x8:  _vx256_mlt_f, \
    __f32x16: _vx512_mlt_f, \
    __u16x8:  _vx128_mlt_h, \
    __u16x16: _vx256_mlt_h, \
    __u16x32: _vx512_mlt_h, \
    __u64x2:  _vx128_mlt_l, \
    __u64x4:  _vx256_mlt_l, \
    __u64x8:  _vx512_mlt_l, \
    __u32x4:  _vx128_mlt_w, \
    __u32x8:  _vx256_mlt_w, \
    __u32x16: _vx512_mlt_w)(a, b, c)


#define _vmne(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_mne_d, \
    __f64x4:  _vx256_mne_d, \
    __f64x8:  _vx512_mne_d, \
    __f32x4:  _vx128_mne_f, \
    __f32x8:  _vx256_mne_f, \
    __f32x16: _vx512_mne_f)(a, b, c)


#define _vmov(a) \
    _Generic(a, \
    __u8x16:  _vx128_mov_b, \
    __u8x32:  _vx256_mov_b, \
    __u8x64:  _vx512_mov_b, \
    __u16x8:  _vx128_mov_h, \
    __u16x16: _vx256_mov_h, \
    __u16x32: _vx512_mov_h, \
    __u32x4:  _vx128_mov_w, \
    __u32x8:  _vx256_mov_w, \
    __u32x16: _vx512_mov_w, \
    __u64x2:  _vx128_mov_l, \
    __u64x4:  _vx256_mov_l, \
    __u64x8:  _vx512_mov_l)(a)


#define _vmovh(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_movh_b, \
    __u8x32:  _vx256_movh_b, \
    __u8x64:  _vx512_movh_b, \
    __u16x8:  _vx128_movh_h, \
    __u16x16: _vx256_movh_h, \
    __u16x32: _vx512_movh_h, \
    __u64x2:  _vx128_movh_l, \
    __u64x4:  _vx256_movh_l, \
    __u64x8:  _vx512_movh_l, \
    __u32x4:  _vx128_movh_w, \
    __u32x8:  _vx256_movh_w, \
    __u32x16: _vx512_movh_w)(a, b)


#define _vmovhl(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_movhl_b, \
    __u8x32:  _vx256_movhl_b, \
    __u8x64:  _vx512_movhl_b, \
    __u16x8:  _vx128_movhl_h, \
    __u16x16: _vx256_movhl_h, \
    __u16x32: _vx512_movhl_h, \
    __u64x2:  _vx128_movhl_l, \
    __u64x4:  _vx256_movhl_l, \
    __u64x8:  _vx512_movhl_l, \
    __u32x4:  _vx128_movhl_w, \
    __u32x8:  _vx256_movhl_w, \
    __u32x16: _vx512_movhl_w)(a, b)


#define _vmovl(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_movl_b, \
    __u8x32:  _vx256_movl_b, \
    __u8x64:  _vx512_movl_b, \
    __u16x8:  _vx128_movl_h, \
    __u16x16: _vx256_movl_h, \
    __u16x32: _vx512_movl_h, \
    __u64x2:  _vx128_movl_l, \
    __u64x4:  _vx256_movl_l, \
    __u64x8:  _vx512_movl_l, \
    __u32x4:  _vx128_movl_w, \
    __u32x8:  _vx256_movl_w, \
    __u32x16: _vx512_movl_w)(a, b)


#define _vmovlh(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_movlh_b, \
    __u8x32:  _vx256_movlh_b, \
    __u8x64:  _vx512_movlh_b, \
    __u16x8:  _vx128_movlh_h, \
    __u16x16: _vx256_movlh_h, \
    __u16x32: _vx512_movlh_h, \
    __u64x2:  _vx128_movlh_l, \
    __u64x4:  _vx256_movlh_l, \
    __u64x8:  _vx512_movlh_l, \
    __u32x4:  _vx128_movlh_w, \
    __u32x8:  _vx256_movlh_w, \
    __u32x16: _vx512_movlh_w)(a, b)


#define _vmsa(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_msa_d, \
    __f64x4:  _vx256_msa_d, \
    __f64x8:  _vx512_msa_d, \
    __f32x4:  _vx128_msa_f, \
    __f32x8:  _vx256_msa_f, \
    __f32x16: _vx512_msa_f)(a, b, c)


#define _vmsub(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_msub_b, \
    __u8x32:  _vx256_msub_b, \
    __u8x64:  _vx512_msub_b, \
    __f64x2:  _vx128_msub_d, \
    __f64x4:  _vx256_msub_d, \
    __f64x8:  _vx512_msub_d, \
    __f32x4:  _vx128_msub_f, \
    __f32x8:  _vx256_msub_f, \
    __f32x16: _vx512_msub_f, \
    __u16x8:  _vx128_msub_h, \
    __u16x16: _vx256_msub_h, \
    __u16x32: _vx512_msub_h, \
    __u64x2:  _vx128_msub_l, \
    __u64x4:  _vx256_msub_l, \
    __u64x8:  _vx512_msub_l, \
    __u32x4:  _vx128_msub_w, \
    __u32x8:  _vx256_msub_w, \
    __u32x16: _vx512_msub_w)(a, b, c)


#define _vmul(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_mul_b, \
    __s8x16:  _vx128_mul_b_s, \
    __u8x32:  _vx256_mul_b, \
    __s8x32:  _vx256_mul_b_s, \
    __u8x64:  _vx512_mul_b, \
    __s8x64:  _vx512_mul_b_s, \
    __u16x8:  _vx128_mul_h, \
    __s16x8:  _vx128_mul_h_s, \
    __u16x16: _vx256_mul_h, \
    __s16x16: _vx256_mul_h_s, \
    __u16x32: _vx512_mul_h, \
    __s16x32: _vx512_mul_h_s, \
    __u32x4:  _vx128_mul_w, \
    __s32x4:  _vx128_mul_w_s, \
    __u32x8:  _vx256_mul_w, \
    __s32x8:  _vx256_mul_w_s, \
    __u32x16: _vx512_mul_w, \
    __s32x16: _vx512_mul_w_s, \
    __u64x2:  _vx128_mul_l, \
    __s64x2:  _vx128_mul_l_s, \
    __u64x4:  _vx256_mul_l, \
    __s64x4:  _vx256_mul_l_s, \
    __u64x8:  _vx512_mul_l, \
    __s64x8:  _vx512_mul_l_s, \
    __f64x2:  _vx128_mul_d, \
    __f64x4:  _vx256_mul_d, \
    __f64x8:  _vx512_mul_d, \
    __f32x4:  _vx128_mul_f, \
    __f32x8:  _vx256_mul_f, \
    __f32x16: _vx512_mul_f)(a, b)


#define _vmulh(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_umulhs_b, \
    __u8x32:  _vx256_umulhs_b, \
    __u8x64:  _vx512_umulhs_b, \
    __u16x8:  _vx128_umulhs_h, \
    __u16x16: _vx256_umulhs_h, \
    __u16x32: _vx512_umulhs_h, \
    __u32x4:  _vx128_umulhs_w, \
    __u32x8:  _vx256_umulhs_w, \
    __u32x16: _vx512_umulhs_w, \
    __u64x2:  _vx128_umulhs_l, \
    __u64x4:  _vx256_umulhs_l, \
    __u64x8:  _vx512_umulhs_l)(a, b)


#define _vmulhs(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_smulhs_b, \
    __s8x32:  _vx256_smulhs_b, \
    __s8x64:  _vx512_smulhs_b, \
    __s16x8:  _vx128_smulhs_h, \
    __s16x16: _vx256_smulhs_h, \
    __s16x32: _vx512_smulhs_h, \
    __s32x4:  _vx128_smulhs_w, \
    __s32x8:  _vx256_smulhs_w, \
    __s32x16: _vx512_smulhs_w, \
    __s64x2:  _vx128_smulhs_l, \
    __s64x4:  _vx256_smulhs_l, \
    __s64x8:  _vx512_smulhs_l)(a, b)


#define _vmuls(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_muls_b, \
    __u8x32:  _vx256_muls_b, \
    __u8x64:  _vx512_muls_b, \
    __u16x8:  _vx128_muls_h, \
    __u16x16: _vx256_muls_h, \
    __u16x32: _vx512_muls_h, \
    __u64x2:  _vx128_muls_l, \
    __u64x4:  _vx256_muls_l, \
    __u64x8:  _vx512_muls_l, \
    __u32x4:  _vx128_muls_w, \
    __u32x8:  _vx256_muls_w, \
    __u32x16: _vx512_muls_w)(a, b)


#define _vmulu(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_mulu_b, \
    __u8x32:  _vx256_mulu_b, \
    __u8x64:  _vx512_mulu_b, \
    __u16x8:  _vx128_mulu_h, \
    __u16x16: _vx256_mulu_h, \
    __u16x32: _vx512_mulu_h, \
    __u64x2:  _vx128_mulu_l, \
    __u64x4:  _vx256_mulu_l, \
    __u64x8:  _vx512_mulu_l, \
    __u32x4:  _vx128_mulu_w, \
    __u32x8:  _vx256_mulu_w, \
    __u32x16: _vx512_mulu_w)(a, b)


#define _vmulx(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vmulx_h, \
    __u8x32:  _vx256_vmulx_h, \
    __u8x64:  _vx512_vmulx_h, \
    __u16x8:  _vx128_vmulx_w, \
    __u16x16: _vx256_vmulx_w, \
    __u16x32: _vx512_vmulx_w, \
    __u32x4:  _vx128_vmulx_l, \
    __u32x8:  _vx256_vmulx_l, \
    __u32x16: _vx512_vmulx_l)(a, b)


#define _vmulxs(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_vmulxs_h, \
    __s8x32:  _vx256_vmulxs_h, \
    __s8x64:  _vx512_vmulxs_h, \
    __s16x8:  _vx128_vmulxs_w, \
    __s16x16: _vx256_vmulxs_w, \
    __s16x32: _vx512_vmulxs_w, \
    __s32x4:  _vx128_vmulxs_l, \
    __s32x8:  _vx256_vmulxs_l, \
    __s32x16: _vx512_vmulxs_l)(a, b)


#define _vnabs(a) \
    _Generic(a, \
    __s8x16:  _vx128_nabs_b, \
    __s8x32:  _vx256_nabs_b, \
    __s8x64:  _vx512_nabs_b, \
    __s16x8:  _vx128_nabs_h, \
    __s16x16: _vx256_nabs_h, \
    __s16x32: _vx512_nabs_h, \
    __s32x4:  _vx128_nabs_w, \
    __s32x8:  _vx256_nabs_w, \
    __s32x16: _vx512_nabs_w, \
    __s64x2:  _vx128_nabs_l, \
    __s64x4:  _vx256_nabs_l, \
    __s64x8:  _vx512_nabs_l, \
    __f64x2:  _vx128_nabs_d, \
    __f64x4:  _vx256_nabs_d, \
    __f64x8:  _vx512_nabs_d, \
    __f32x4:  _vx128_nabs_f, \
    __f32x8:  _vx256_nabs_f, \
    __f32x16: _vx512_nabs_f)(a)


#define _vne(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_ne_b, \
    __u8x32:  _vx256_ne_b, \
    __u8x64:  _vx512_ne_b, \
    __u16x8:  _vx128_ne_h, \
    __u16x16: _vx256_ne_h, \
    __u16x32: _vx512_ne_h, \
    __u32x4:  _vx128_ne_w, \
    __u32x8:  _vx256_ne_w, \
    __u32x16: _vx512_ne_w, \
    __u64x2:  _vx128_ne_l, \
    __u64x4:  _vx256_ne_l, \
    __u64x8:  _vx512_ne_l, \
    __f64x2:  _vx128_ne_d, \
    __f64x4:  _vx256_ne_d, \
    __f64x8:  _vx512_ne_d, \
    __f32x4:  _vx128_ne_f, \
    __f32x8:  _vx256_ne_f, \
    __f32x16: _vx512_ne_f)(a, b)


#define _vne_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_ne_ld, \
    __f64x4:  _vx256_ne_ld, \
    __f64x8:  _vx512_ne_ld, \
    __f32x4:  _vx128_ne_wf, \
    __f32x8:  _vx256_ne_wf, \
    __f32x16: _vx512_ne_wf)(a, b)


#define _vneg(a) \
    _Generic(a, \
    __s8x16:  _vx128_neg_b, \
    __s8x32:  _vx256_neg_b, \
    __s8x64:  _vx512_neg_b, \
    __s16x8:  _vx128_neg_h, \
    __s16x16: _vx256_neg_h, \
    __s16x32: _vx512_neg_h, \
    __s32x4:  _vx128_neg_w, \
    __s32x8:  _vx256_neg_w, \
    __s32x16: _vx512_neg_w, \
    __s64x2:  _vx128_neg_l, \
    __s64x4:  _vx256_neg_l, \
    __s64x8:  _vx512_neg_l, \
    __f64x2:  _vx128_neg_d, \
    __f64x4:  _vx256_neg_d, \
    __f64x8:  _vx512_neg_d, \
    __f32x4:  _vx128_neg_f, \
    __f32x8:  _vx256_neg_f, \
    __f32x16: _vx512_neg_f)(a)


#define _vnmadd(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_nmadd_d, \
    __f64x4:  _vx256_nmadd_d, \
    __f64x8:  _vx512_nmadd_d, \
    __f32x4:  _vx128_nmadd_f, \
    __f32x8:  _vx256_nmadd_f, \
    __f32x16: _vx512_nmadd_f)(a, b, c)


#define _vnmsub(a, b, c) \
    _Generic(a, \
    __f64x2:  _vx128_nmsub_d, \
    __f64x4:  _vx256_nmsub_d, \
    __f64x8:  _vx512_nmsub_d, \
    __f32x4:  _vx128_nmsub_f, \
    __f32x8:  _vx256_nmsub_f, \
    __f32x16: _vx512_nmsub_f)(a, b, c)


#define _vnmul(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_nmul_d, \
    __f64x4:  _vx256_nmul_d, \
    __f64x8:  _vx512_nmul_d, \
    __f32x4:  _vx128_nmul_f, \
    __f32x8:  _vx256_nmul_f, \
    __f32x16: _vx512_nmul_f)(a, b)


#define _vnot(a) \
    _Generic(a, \
    __u8x16:  _vx128_not_b, \
    __u8x32:  _vx256_not_b, \
    __u8x64:  _vx512_not_b, \
    __u16x8:  _vx128_not_h, \
    __u16x16: _vx256_not_h, \
    __u16x32: _vx512_not_h, \
    __u32x4:  _vx128_not_w, \
    __u32x8:  _vx256_not_w, \
    __u32x16: _vx512_not_w, \
    __u64x2:  _vx128_not_l, \
    __u64x4:  _vx256_not_l, \
    __u64x8:  _vx512_not_l)(a)


#define _vor(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_or_b, \
    __u8x32:  _vx256_or_b, \
    __u8x64:  _vx512_or_b, \
    __u16x8:  _vx128_or_h, \
    __u16x16: _vx256_or_h, \
    __u16x32: _vx512_or_h, \
    __u32x4:  _vx128_or_w, \
    __u32x8:  _vx256_or_w, \
    __u32x16: _vx512_or_w, \
    __u64x2:  _vx128_or_l, \
    __u64x4:  _vx256_or_l, \
    __u64x8:  _vx512_or_l)(a, b)


#define _vord(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_ord_d, \
    __f64x4:  _vx256_ord_d, \
    __f64x8:  _vx512_ord_d, \
    __f32x4:  _vx128_ord_f, \
    __f32x8:  _vx256_ord_f, \
    __f32x16: _vx512_ord_f)(a, b)


#define _vord_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_ord_ld, \
    __f64x4:  _vx256_ord_ld, \
    __f64x8:  _vx512_ord_ld, \
    __f32x4:  _vx128_ord_wf, \
    __f32x8:  _vx256_ord_wf, \
    __f32x16: _vx512_ord_wf)(a, b)


#define _vpar(a) \
    _Generic(a, \
    __u8x16:  _vx128_par_b, \
    __u8x32:  _vx256_par_b, \
    __u8x64:  _vx512_par_b, \
    __u16x8:  _vx128_par_h, \
    __u16x16: _vx256_par_h, \
    __u16x32: _vx512_par_h, \
    __u32x4:  _vx128_par_w, \
    __u32x8:  _vx256_par_w, \
    __u32x16: _vx512_par_w, \
    __u64x2:  _vx128_par_l, \
    __u64x4:  _vx256_par_l, \
    __u64x8:  _vx512_par_l)(a)


#define _vpck(a, b) \
    _Generic(a, \
    __u16x32: _vx512_pck_b, \
    __s16x32: _vx512_pck_b_s, \
    __u32x16: _vx512_pck_h, \
    __s32x16: _vx512_pck_h_s, \
    __u64x8:  _vx512_pck_w, \
    __s64x8:  _vx512_pck_w_s)(a, b)


#define _vpcks(a, b) \
    _Generic(a, \
    __u16x32: _vx512_pcks_b, \
    __u32x16: _vx512_pcks_h, \
    __u64x8:  _vx512_pcks_w)(a, b)


#define _vpcku(a, b) \
    _Generic(a, \
    __u16x32: _vx512_pcku_b, \
    __u32x16: _vx512_pcku_h, \
    __u64x8:  _vx512_pcku_w)(a, b)


#define _vperm(a, b, c) \
    _Generic(a, \
    __u8x64:  _vx512_permx_b, \
    __s8x64:  _vx512_permx_b_s, \
    __u16x32: _vx512_permx_h, \
    __s16x32: _vx512_permx_h_s, \
    __u64x8:  _vx512_permx_l, \
    __s64x8:  _vx512_permx_l_s, \
    __u32x16: _vx512_permx_w, \
    __s32x16: _vx512_permx_w_s, \
    __u8x16:  _vx128_permx_b, \
    __s8x16:  _vx128_permx_b_s, \
    __u8x32:  _vx256_permx_b, \
    __s8x32:  _vx256_permx_b_s, \
    __u16x8:  _vx128_permx_h, \
    __s16x8:  _vx128_permx_h_s, \
    __u16x16: _vx256_permx_h, \
    __s16x16: _vx256_permx_h_s, \
    __u64x2:  _vx128_permx_l, \
    __s64x2:  _vx128_permx_l_s, \
    __u64x4:  _vx256_permx_l, \
    __s64x4:  _vx256_permx_l_s, \
    __u32x4:  _vx128_permx_w, \
    __s32x4:  _vx128_permx_w_s, \
    __u32x8:  _vx256_permx_w, \
    __s32x8:  _vx256_permx_w_s)(a, b, c)


#define _vperml(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_perml_b, \
    __s8x16:  _vx128_perml_b_s, \
    __u8x32:  _vx256_perml_b, \
    __s8x32:  _vx256_perml_b_s, \
    __u8x64:  _vx512_perml_b, \
    __s8x64:  _vx512_perml_b_s, \
    __u16x8:  _vx128_perml_h, \
    __s16x8:  _vx128_perml_h_s, \
    __u16x16: _vx256_perml_h, \
    __s16x16: _vx256_perml_h_s, \
    __u16x32: _vx512_perml_h, \
    __s16x32: _vx512_perml_h_s, \
    __u64x2:  _vx128_perml_l, \
    __s64x2:  _vx128_perml_l_s, \
    __u64x4:  _vx256_perml_l, \
    __s64x4:  _vx256_perml_l_s, \
    __u64x8:  _vx512_perml_l, \
    __s64x8:  _vx512_perml_l_s, \
    __u32x4:  _vx128_perml_w, \
    __s32x4:  _vx128_perml_w_s, \
    __u32x8:  _vx256_perml_w, \
    __s32x8:  _vx256_perml_w_s, \
    __u32x16: _vx512_perml_w, \
    __s32x16: _vx512_perml_w_s)(a, b)


#define _vpermxx(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_permxx_b, \
    __s8x16:  _vx128_permxx_b_s, \
    __u8x32:  _vx256_permxx_b, \
    __s8x32:  _vx256_permxx_b_s, \
    __u8x64:  _vx512_permxx_b, \
    __s8x64:  _vx512_permxx_b_s, \
    __u16x8:  _vx128_permxx_h, \
    __s16x8:  _vx128_permxx_h_s, \
    __u16x16: _vx256_permxx_h, \
    __s16x16: _vx256_permxx_h_s, \
    __u16x32: _vx512_permxx_h, \
    __s16x32: _vx512_permxx_h_s, \
    __u64x2:  _vx128_permxx_l, \
    __s64x2:  _vx128_permxx_l_s, \
    __u64x4:  _vx256_permxx_l, \
    __s64x4:  _vx256_permxx_l_s, \
    __u64x8:  _vx512_permxx_l, \
    __s64x8:  _vx512_permxx_l_s, \
    __u32x4:  _vx128_permxx_w, \
    __s32x4:  _vx128_permxx_w_s, \
    __u32x8:  _vx256_permxx_w, \
    __s32x8:  _vx256_permxx_w_s, \
    __u32x16: _vx512_permxx_w, \
    __s32x16: _vx512_permxx_w_s)(a, b)


#define _vpopc(a) \
    _Generic(a, \
    __u8x16:  _vx128_popc_b, \
    __u8x32:  _vx256_popc_b, \
    __u8x64:  _vx512_popc_b, \
    __u16x8:  _vx128_popc_h, \
    __u16x16: _vx256_popc_h, \
    __u16x32: _vx512_popc_h, \
    __u32x4:  _vx128_popc_w, \
    __u32x8:  _vx256_popc_w, \
    __u32x16: _vx512_popc_w, \
    __u64x2:  _vx128_popc_l, \
    __u64x4:  _vx256_popc_l, \
    __u64x8:  _vx512_popc_l)(a)


#define _vprod(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vprod_h, \
    __u8x32:  _vx256_vprod_h, \
    __u8x64:  _vx512_vprod_h, \
    __u16x8:  _vx128_vprod_w, \
    __u16x16: _vx256_vprod_w, \
    __u16x32: _vx512_vprod_w, \
    __u32x4:  _vx128_vprod_l, \
    __u32x8:  _vx256_vprod_l, \
    __u32x16: _vx512_vprod_l)(a, b)


#define _vprods(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_vprods_h, \
    __s8x32:  _vx256_vprods_h, \
    __s8x64:  _vx512_vprods_h, \
    __s16x8:  _vx128_vprods_w, \
    __s16x16: _vx256_vprods_w, \
    __s16x32: _vx512_vprods_w, \
    __s32x4:  _vx128_vprods_l, \
    __s32x8:  _vx256_vprods_l, \
    __s32x16: _vx512_vprods_l)(a, b)


#define _vprodus(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vprodus_h, \
    __u8x32:  _vx256_vprodus_h, \
    __u8x64:  _vx512_vprodus_h, \
    __u16x8:  _vx128_vprodus_w, \
    __u16x16: _vx256_vprodus_w, \
    __u16x32: _vx512_vprodus_w, \
    __u32x4:  _vx128_vprodus_l, \
    __u32x8:  _vx256_vprodus_l, \
    __u32x16: _vx512_vprodus_l)(a, b)


#define _vrbit(a) \
    _Generic(a, \
    __u8x16:  _vx128_rbit_b, \
    __u8x32:  _vx256_rbit_b, \
    __u8x64:  _vx512_rbit_b, \
    __u16x8:  _vx128_rbit_h, \
    __u16x16: _vx256_rbit_h, \
    __u16x32: _vx512_rbit_h, \
    __u32x4:  _vx128_rbit_w, \
    __u32x8:  _vx256_rbit_w, \
    __u32x16: _vx512_rbit_w, \
    __u64x2:  _vx128_rbit_l, \
    __u64x4:  _vx256_rbit_l, \
    __u64x8:  _vx512_rbit_l)(a)


#define _vrev(a) \
    _Generic(a, \
    __u16x8:  _vx128_rev_h, \
    __u16x16: _vx256_rev_h, \
    __u16x32: _vx512_rev_h, \
    __u32x4:  _vx128_rev_w, \
    __u32x8:  _vx256_rev_w, \
    __u32x16: _vx512_rev_w, \
    __u64x2:  _vx128_rev_l, \
    __u64x4:  _vx256_rev_l, \
    __u64x8:  _vx512_rev_l)(a)


#define _vrol(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_rol_b, \
    __u8x32:  _vx256_rol_b, \
    __u8x64:  _vx512_rol_b, \
    __u16x8:  _vx128_rol_h, \
    __u16x16: _vx256_rol_h, \
    __u16x32: _vx512_rol_h, \
    __u32x4:  _vx128_rol_w, \
    __u32x8:  _vx256_rol_w, \
    __u32x16: _vx512_rol_w, \
    __u64x2:  _vx128_rol_l, \
    __u64x4:  _vx256_rol_l, \
    __u64x8:  _vx512_rol_l)(a, b)


#define _vrole(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_role_b, \
    __u8x32:  _vx256_role_b, \
    __u8x64:  _vx512_role_b, \
    __u16x8:  _vx128_role_h, \
    __u16x16: _vx256_role_h, \
    __u16x32: _vx512_role_h, \
    __u64x2:  _vx128_role_l, \
    __u64x4:  _vx256_role_l, \
    __u64x8:  _vx512_role_l, \
    __u32x4:  _vx128_role_w, \
    __u32x8:  _vx256_role_w, \
    __u32x16: _vx512_role_w)(a, b)


#define _vror(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_ror_b, \
    __u8x32:  _vx256_ror_b, \
    __u8x64:  _vx512_ror_b, \
    __u16x8:  _vx128_ror_h, \
    __u16x16: _vx256_ror_h, \
    __u16x32: _vx512_ror_h, \
    __u32x4:  _vx128_ror_w, \
    __u32x8:  _vx256_ror_w, \
    __u32x16: _vx512_ror_w, \
    __u64x2:  _vx128_ror_l, \
    __u64x4:  _vx256_ror_l, \
    __u64x8:  _vx512_ror_l)(a, b)


#define _vrore(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_rore_b, \
    __u8x32:  _vx256_rore_b, \
    __u8x64:  _vx512_rore_b, \
    __u16x8:  _vx128_rore_h, \
    __u16x16: _vx256_rore_h, \
    __u16x32: _vx512_rore_h, \
    __u64x2:  _vx128_rore_l, \
    __u64x4:  _vx256_rore_l, \
    __u64x8:  _vx512_rore_l, \
    __u32x4:  _vx128_rore_w, \
    __u32x8:  _vx256_rore_w, \
    __u32x16: _vx512_rore_w)(a, b)


#define _vround(a) \
    _Generic(a, \
    __f64x2:  _vx128_round_d, \
    __f64x4:  _vx256_round_d, \
    __f64x8:  _vx512_round_d, \
    __f32x4:  _vx128_round_f, \
    __f32x8:  _vx256_round_f, \
    __f32x16: _vx512_round_f)(a)


#define _vrounda(a) \
    _Generic(a, \
    __f64x2:  _vx128_rounda_d, \
    __f64x4:  _vx256_rounda_d, \
    __f64x8:  _vx512_rounda_d, \
    __f32x4:  _vx128_rounda_f, \
    __f32x8:  _vx256_rounda_f, \
    __f32x16: _vx512_rounda_f)(a)


#define _vroundi(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundi_d, \
    __f64x4:  _vx256_roundi_d, \
    __f64x8:  _vx512_roundi_d, \
    __f32x4:  _vx128_roundi_f, \
    __f32x8:  _vx256_roundi_f, \
    __f32x16: _vx512_roundi_f)(a)


#define _vroundia(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundia_d, \
    __f64x4:  _vx256_roundia_d, \
    __f64x8:  _vx512_roundia_d, \
    __f32x4:  _vx128_roundia_f, \
    __f32x8:  _vx256_roundia_f, \
    __f32x16: _vx512_roundia_f)(a)


#define _vroundin(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundin_d, \
    __f64x4:  _vx256_roundin_d, \
    __f64x8:  _vx512_roundin_d, \
    __f32x4:  _vx128_roundin_f, \
    __f32x8:  _vx256_roundin_f, \
    __f32x16: _vx512_roundin_f)(a)


#define _vroundip(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundip_d, \
    __f64x4:  _vx256_roundip_d, \
    __f64x8:  _vx512_roundip_d, \
    __f32x4:  _vx128_roundip_f, \
    __f32x8:  _vx256_roundip_f, \
    __f32x16: _vx512_roundip_f)(a)


#define _vroundir(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundir_d, \
    __f64x4:  _vx256_roundir_d, \
    __f64x8:  _vx512_roundir_d, \
    __f32x4:  _vx128_roundir_f, \
    __f32x8:  _vx256_roundir_f, \
    __f32x16: _vx512_roundir_f)(a)


#define _vroundit(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundit_d, \
    __f64x4:  _vx256_roundit_d, \
    __f64x8:  _vx512_roundit_d, \
    __f32x4:  _vx128_roundit_f, \
    __f32x8:  _vx256_roundit_f, \
    __f32x16: _vx512_roundit_f)(a)


#define _vroundix(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundix_d, \
    __f64x4:  _vx256_roundix_d, \
    __f64x8:  _vx512_roundix_d, \
    __f32x4:  _vx128_roundix_f, \
    __f32x8:  _vx256_roundix_f, \
    __f32x16: _vx512_roundix_f)(a)


#define _vroundn(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundn_d, \
    __f64x4:  _vx256_roundn_d, \
    __f64x8:  _vx512_roundn_d, \
    __f32x4:  _vx128_roundn_f, \
    __f32x8:  _vx256_roundn_f, \
    __f32x16: _vx512_roundn_f)(a)


#define _vroundp(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundp_d, \
    __f64x4:  _vx256_roundp_d, \
    __f64x8:  _vx512_roundp_d, \
    __f32x4:  _vx128_roundp_f, \
    __f32x8:  _vx256_roundp_f, \
    __f32x16: _vx512_roundp_f)(a)


#define _vroundr(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundr_d, \
    __f64x4:  _vx256_roundr_d, \
    __f64x8:  _vx512_roundr_d, \
    __f32x4:  _vx128_roundr_f, \
    __f32x8:  _vx256_roundr_f, \
    __f32x16: _vx512_roundr_f)(a)


#define _vroundt(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundt_d, \
    __f64x4:  _vx256_roundt_d, \
    __f64x8:  _vx512_roundt_d, \
    __f32x4:  _vx128_roundt_f, \
    __f32x8:  _vx256_roundt_f, \
    __f32x16: _vx512_roundt_f)(a)


#define _vroundx(a) \
    _Generic(a, \
    __f64x2:  _vx128_roundx_d, \
    __f64x4:  _vx256_roundx_d, \
    __f64x8:  _vx512_roundx_d, \
    __f32x4:  _vx128_roundx_f, \
    __f32x8:  _vx256_roundx_f, \
    __f32x16: _vx512_roundx_f)(a)


#define _vrsqrt(a) \
    _Generic(a, \
    __f64x2:  _vx128_rsqrt_d, \
    __f64x4:  _vx256_rsqrt_d, \
    __f64x8:  _vx512_rsqrt_d, \
    __f32x4:  _vx128_rsqrt_f, \
    __f32x8:  _vx256_rsqrt_f, \
    __f32x16: _vx512_rsqrt_f)(a)


#define _vrsqrtx(a) \
    _Generic(a, \
    __f64x2:  _vx128_rsqrtx_d, \
    __f64x4:  _vx256_rsqrtx_d, \
    __f64x8:  _vx512_rsqrtx_d, \
    __f32x4:  _vx128_rsqrtx_f, \
    __f32x8:  _vx256_rsqrtx_f, \
    __f32x16: _vx512_rsqrtx_f)(a)


#define _vsad(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_sad_l, \
    __u8x32:  _vx256_sad_l, \
    __u8x64:  _vx512_sad_l)(a, b)


#define _vsada(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_sada_l, \
    __u8x32:  _vx256_sada_l, \
    __u8x64:  _vx512_sada_l)(a, b)


#define _vsadm(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_sadm_h, \
    __u8x32:  _vx256_sadm_h, \
    __u8x64:  _vx512_sadm_h)(a, b, c)


#define _vsadma(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_sadma_h, \
    __u8x32:  _vx256_sadma_h, \
    __u8x64:  _vx512_sadma_h)(a, b, c)


#define _vsadt(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_sadt_h, \
    __u8x32:  _vx256_sadt_h, \
    __u8x64:  _vx512_sadt_h)(a, b, c)


#define _vsadta(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_sadta_h, \
    __u8x32:  _vx256_sadta_h, \
    __u8x64:  _vx512_sadta_h)(a, b, c)


#define _vsar(a, b) \
    _Generic(a, \
    __s8x16:  _vx128_sarv_b, \
    __s8x32:  _vx256_sarv_b, \
    __s8x64:  _vx512_sarv_b, \
    __s16x8:  _vx128_sarv_h, \
    __s16x16: _vx256_sarv_h, \
    __s16x32: _vx512_sarv_h, \
    __s32x4:  _vx128_sarv_w, \
    __s32x8:  _vx256_sarv_w, \
    __s32x16: _vx512_sarv_w, \
    __s64x2:  _vx128_sarv_l, \
    __s64x4:  _vx256_sarv_l, \
    __s64x8:  _vx512_sarv_l, \
    __u8x16:  _vx128_sar_b, \
    __u8x32:  _vx256_sar_b, \
    __u8x64:  _vx512_sar_b, \
    __u16x8:  _vx128_sar_h, \
    __u16x16: _vx256_sar_h, \
    __u16x32: _vx512_sar_h, \
    __u64x2:  _vx128_sar_l, \
    __u64x4:  _vx256_sar_l, \
    __u64x8:  _vx512_sar_l, \
    __u32x4:  _vx128_sar_w, \
    __u32x8:  _vx256_sar_w, \
    __u32x16: _vx512_sar_w)(a, b)


#define _vshf(a, b) \
    _Generic(a, \
    __u16x8:  _vx128_shf_h, \
    __u16x16: _vx256_shf_h, \
    __u16x32: _vx512_shf_h, \
    __u32x4:  _vx128_shf_w, \
    __u32x8:  _vx256_shf_w, \
    __u32x16: _vx512_shf_w)(a, b)


#define _vshfh(a, b) \
    _Generic(a, \
    __u16x8:  _vx128_shfh_h, \
    __u16x16: _vx256_shfh_h, \
    __u16x32: _vx512_shfh_h)(a, b)


#define _vshfl(a, b) \
    _Generic(a, \
    __u16x8:  _vx128_shfl_h, \
    __u16x16: _vx256_shfl_h, \
    __u16x32: _vx512_shfl_h)(a, b)


#define _vshl(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_shl_b, \
    __u8x32:  _vx256_shl_b, \
    __u8x64:  _vx512_shl_b, \
    __u16x8:  _vx128_shl_h, \
    __u16x16: _vx256_shl_h, \
    __u16x32: _vx512_shl_h, \
    __u32x4:  _vx128_shl_w, \
    __u32x8:  _vx256_shl_w, \
    __u32x16: _vx512_shl_w, \
    __u64x2:  _vx128_shl_l, \
    __u64x4:  _vx256_shl_l, \
    __u64x8:  _vx512_shl_l)(a, b)


#define _vshle(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_shle_b, \
    __u8x32:  _vx256_shle_b, \
    __u8x64:  _vx512_shle_b, \
    __u16x8:  _vx128_shle_h, \
    __u16x16: _vx256_shle_h, \
    __u16x32: _vx512_shle_h, \
    __u64x2:  _vx128_shle_l, \
    __u64x4:  _vx256_shle_l, \
    __u64x8:  _vx512_shle_l, \
    __u32x4:  _vx128_shle_w, \
    __u32x8:  _vx256_shle_w, \
    __u32x16: _vx512_shle_w)(a, b)


#define _vshr(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_shr_b, \
    __u8x32:  _vx256_shr_b, \
    __u8x64:  _vx512_shr_b, \
    __u16x8:  _vx128_shr_h, \
    __u16x16: _vx256_shr_h, \
    __u16x32: _vx512_shr_h, \
    __u32x4:  _vx128_shr_w, \
    __u32x8:  _vx256_shr_w, \
    __u32x16: _vx512_shr_w, \
    __u64x2:  _vx128_shr_l, \
    __u64x4:  _vx256_shr_l, \
    __u64x8:  _vx512_shr_l)(a, b)


#define _vshre(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_shre_b, \
    __u8x32:  _vx256_shre_b, \
    __u8x64:  _vx512_shre_b, \
    __u16x8:  _vx128_shre_h, \
    __u16x16: _vx256_shre_h, \
    __u16x32: _vx512_shre_h, \
    __u64x2:  _vx128_shre_l, \
    __u64x4:  _vx256_shre_l, \
    __u64x8:  _vx512_shre_l, \
    __u32x4:  _vx128_shre_w, \
    __u32x8:  _vx256_shre_w, \
    __u32x16: _vx512_shre_w)(a, b)


#define _vsprod(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vsprod_h, \
    __u8x32:  _vx256_vsprod_h, \
    __u8x64:  _vx512_vsprod_h, \
    __u16x8:  _vx128_vsprod_w, \
    __u16x16: _vx256_vsprod_w, \
    __u16x32: _vx512_vsprod_w, \
    __u32x4:  _vx128_vsprod_l, \
    __u32x8:  _vx256_vsprod_l, \
    __u32x16: _vx512_vsprod_l)(a, b)


#define _vsprods(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vsprods_h, \
    __u8x32:  _vx256_vsprods_h, \
    __u8x64:  _vx512_vsprods_h, \
    __u16x8:  _vx128_vsprods_w, \
    __u16x16: _vx256_vsprods_w, \
    __u16x32: _vx512_vsprods_w, \
    __u32x4:  _vx128_vsprods_l, \
    __u32x8:  _vx256_vsprods_l, \
    __u32x16: _vx512_vsprods_l)(a, b)


#define _vsprodus(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_vsprodus_h, \
    __u8x32:  _vx256_vsprodus_h, \
    __u8x64:  _vx512_vsprodus_h, \
    __u16x8:  _vx128_vsprodus_w, \
    __u16x16: _vx256_vsprodus_w, \
    __u16x32: _vx512_vsprodus_w, \
    __u32x4:  _vx128_vsprodus_l, \
    __u32x8:  _vx256_vsprodus_l, \
    __u32x16: _vx512_vsprodus_l)(a, b)


#define _vsqrt(a) \
    _Generic(a, \
    __f64x2:  _vx128_sqrt_d, \
    __f64x4:  _vx256_sqrt_d, \
    __f64x8:  _vx512_sqrt_d, \
    __f32x4:  _vx128_sqrt_f, \
    __f32x8:  _vx256_sqrt_f, \
    __f32x16: _vx512_sqrt_f)(a)


#define _vsub(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_sub_b, \
    __s8x16:  _vx128_sub_b_s, \
    __u8x32:  _vx256_sub_b, \
    __s8x32:  _vx256_sub_b_s, \
    __u8x64:  _vx512_sub_b, \
    __s8x64:  _vx512_sub_b_s, \
    __u16x8:  _vx128_sub_h, \
    __s16x8:  _vx128_sub_h_s, \
    __u16x16: _vx256_sub_h, \
    __s16x16: _vx256_sub_h_s, \
    __u16x32: _vx512_sub_h, \
    __s16x32: _vx512_sub_h_s, \
    __u32x4:  _vx128_sub_w, \
    __s32x4:  _vx128_sub_w_s, \
    __u32x8:  _vx256_sub_w, \
    __s32x8:  _vx256_sub_w_s, \
    __u32x16: _vx512_sub_w, \
    __s32x16: _vx512_sub_w_s, \
    __u64x2:  _vx128_sub_l, \
    __s64x2:  _vx128_sub_l_s, \
    __u64x4:  _vx256_sub_l, \
    __s64x4:  _vx256_sub_l_s, \
    __u64x8:  _vx512_sub_l, \
    __s64x8:  _vx512_sub_l_s, \
    __f64x2:  _vx128_sub_d, \
    __f64x4:  _vx256_sub_d, \
    __f64x8:  _vx512_sub_d, \
    __f32x4:  _vx128_sub_f, \
    __f32x8:  _vx256_sub_f, \
    __f32x16: _vx512_sub_f)(a, b)


#define _vsuba(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_suba_d, \
    __f64x4:  _vx256_suba_d, \
    __f64x8:  _vx512_suba_d, \
    __f32x4:  _vx128_suba_f, \
    __f32x8:  _vx256_suba_f, \
    __f32x16: _vx512_suba_f)(a, b)


#define _vsubc(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_subc_b, \
    __u8x32:  _vx256_subc_b, \
    __u8x64:  _vx512_subc_b, \
    __u16x8:  _vx128_subc_h, \
    __u16x16: _vx256_subc_h, \
    __u16x32: _vx512_subc_h, \
    __u32x4:  _vx128_subc_w, \
    __u32x8:  _vx256_subc_w, \
    __u32x16: _vx512_subc_w, \
    __u64x2:  _vx128_subc_l, \
    __u64x4:  _vx256_subc_l, \
    __u64x8:  _vx512_subc_l)(a, b, c)


#define _vsubco(a, b, c) \
    _Generic(a, \
    __u8x16:  _vx128_subco4_b, \
    __u8x32:  _vx256_subco4_b, \
    __u8x64:  _vx512_subco4_b, \
    __u16x8:  _vx128_subco4_h, \
    __u16x16: _vx256_subco4_h, \
    __u16x32: _vx512_subco4_h, \
    __u64x2:  _vx128_subco4_l, \
    __u64x4:  _vx256_subco4_l, \
    __u64x8:  _vx512_subco4_l, \
    __u32x4:  _vx128_subco4_w, \
    __u32x8:  _vx256_subco4_w, \
    __u32x16: _vx512_subco4_w)(a, b, c)


#define _vsubs(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_subs_b, \
    __u8x32:  _vx256_subs_b, \
    __u8x64:  _vx512_subs_b, \
    __u16x8:  _vx128_subs_h, \
    __u16x16: _vx256_subs_h, \
    __u16x32: _vx512_subs_h, \
    __u32x4:  _vx128_subs_w, \
    __u32x8:  _vx256_subs_w, \
    __u32x16: _vx512_subs_w, \
    __u64x2:  _vx128_subs_l, \
    __u64x4:  _vx256_subs_l, \
    __u64x8:  _vx512_subs_l)(a, b)


#define _vsubu(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_subu_b, \
    __u8x32:  _vx256_subu_b, \
    __u8x64:  _vx512_subu_b, \
    __u16x8:  _vx128_subu_h, \
    __u16x16: _vx256_subu_h, \
    __u16x32: _vx512_subu_h, \
    __u32x4:  _vx128_subu_w, \
    __u32x8:  _vx256_subu_w, \
    __u32x16: _vx512_subu_w, \
    __u64x2:  _vx128_subu_l, \
    __u64x4:  _vx256_subu_l, \
    __u64x8:  _vx512_subu_l)(a, b)


#define _vsxb(a) \
    _Generic(a, \
    __u16x8:  _vx128_sxb_h, \
    __u16x16: _vx256_sxb_h, \
    __u16x32: _vx512_sxb_h, \
    __u32x4:  _vx128_sxb_w, \
    __u32x8:  _vx256_sxb_w, \
    __u32x16: _vx512_sxb_w, \
    __u64x2:  _vx128_sxb_l, \
    __u64x4:  _vx256_sxb_l, \
    __u64x8:  _vx512_sxb_l)(a)


#define _vsxh(a) \
    _Generic(a, \
    __u32x4:  _vx128_sxh_w, \
    __u32x8:  _vx256_sxh_w, \
    __u32x16: _vx512_sxh_w, \
    __u64x2:  _vx128_sxh_l, \
    __u64x4:  _vx256_sxh_l, \
    __u64x8:  _vx512_sxh_l)(a)


#define _vsxw(a) \
    _Generic(a, \
    __u64x2:  _vx128_sxw_l, \
    __u64x4:  _vx256_sxw_l, \
    __u64x8:  _vx512_sxw_l)(a)


#define _vuno(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_uno_d, \
    __f64x4:  _vx256_uno_d, \
    __f64x8:  _vx512_uno_d, \
    __f32x4:  _vx128_uno_f, \
    __f32x8:  _vx256_uno_f, \
    __f32x16: _vx512_uno_f)(a, b)


#define _vuno_u(a, b) \
    _Generic(a, \
    __f64x2:  _vx128_uno_ld, \
    __f64x4:  _vx256_uno_ld, \
    __f64x8:  _vx512_uno_ld, \
    __f32x4:  _vx128_uno_wf, \
    __f32x8:  _vx256_uno_wf, \
    __f32x16: _vx512_uno_wf)(a, b)


#define _vunpk(a) \
    _Generic(a, \
    __u8x16:  _vx128_unpk_h, \
    __u8x32:  _vx256_unpk_h, \
    __u8x64:  _vx512_unpk_h, \
    __u32x4:  _vx128_unpk_l, \
    __u32x8:  _vx256_unpk_l, \
    __u32x16: _vx512_unpk_l, \
    __u16x8:  _vx128_unpk_w, \
    __u16x16: _vx256_unpk_w, \
    __u16x32: _vx512_unpk_w)(a)


#define _vunpkh(a) \
    _Generic(a, \
    __u8x16:  _vx128_unpkh_h, \
    __u8x32:  _vx256_unpkh_h, \
    __u8x64:  _vx512_unpkh_h, \
    __u32x4:  _vx128_unpkh_l, \
    __u32x8:  _vx256_unpkh_l, \
    __u32x16: _vx512_unpkh_l, \
    __u16x8:  _vx128_unpkh_w, \
    __u16x16: _vx256_unpkh_w, \
    __u16x32: _vx512_unpkh_w)(a)


#define _vunpkhs(a) \
    _Generic(a, \
    __u8x16:  _vx128_unpkhs_h, \
    __u8x32:  _vx256_unpkhs_h, \
    __u8x64:  _vx512_unpkhs_h, \
    __u32x4:  _vx128_unpkhs_l, \
    __u32x8:  _vx256_unpkhs_l, \
    __u32x16: _vx512_unpkhs_l, \
    __u16x8:  _vx128_unpkhs_w, \
    __u16x16: _vx256_unpkhs_w, \
    __u16x32: _vx512_unpkhs_w)(a)


#define _vunpki(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_unpki_b, \
    __u8x32:  _vx256_unpki_b, \
    __u8x64:  _vx512_unpki_b, \
    __u16x8:  _vx128_unpki_h, \
    __u16x16: _vx256_unpki_h, \
    __u16x32: _vx512_unpki_h, \
    __u32x4:  _vx128_unpki_w, \
    __u32x8:  _vx256_unpki_w, \
    __u32x16: _vx512_unpki_w, \
    __u64x2:  _vx128_unpki_l, \
    __u64x4:  _vx256_unpki_l, \
    __u64x8:  _vx512_unpki_l)(a, b)


#define _vunpkih(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_unpkih_b, \
    __u8x32:  _vx256_unpkih_b, \
    __u8x64:  _vx512_unpkih_b, \
    __u16x8:  _vx128_unpkih_h, \
    __u16x16: _vx256_unpkih_h, \
    __u16x32: _vx512_unpkih_h, \
    __u32x4:  _vx128_unpkih_w, \
    __u32x8:  _vx256_unpkih_w, \
    __u32x16: _vx512_unpkih_w, \
    __u64x2:  _vx128_unpkih_l, \
    __u64x4:  _vx256_unpkih_l, \
    __u64x8:  _vx512_unpkih_l)(a, b)


#define _vunpks(a) \
    _Generic(a, \
    __u8x16:  _vx128_unpks_h, \
    __u8x32:  _vx256_unpks_h, \
    __u8x64:  _vx512_unpks_h, \
    __u32x4:  _vx128_unpks_l, \
    __u32x8:  _vx256_unpks_l, \
    __u32x16: _vx512_unpks_l, \
    __u16x8:  _vx128_unpks_w, \
    __u16x16: _vx256_unpks_w, \
    __u16x32: _vx512_unpks_w)(a)


#define _vxor(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_xor_b, \
    __u8x32:  _vx256_xor_b, \
    __u8x64:  _vx512_xor_b, \
    __u16x8:  _vx128_xor_h, \
    __u16x16: _vx256_xor_h, \
    __u16x32: _vx512_xor_h, \
    __u32x4:  _vx128_xor_w, \
    __u32x8:  _vx256_xor_w, \
    __u32x16: _vx512_xor_w, \
    __u64x2:  _vx128_xor_l, \
    __u64x4:  _vx256_xor_l, \
    __u64x8:  _vx512_xor_l)(a, b)


#define _vxtr(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_xtrv_b, \
    __u8x32:  _vx256_xtrv_b, \
    __u8x64:  _vx512_xtrv_b, \
    __u16x8:  _vx128_xtrv_h, \
    __u16x16: _vx256_xtrv_h, \
    __u16x32: _vx512_xtrv_h, \
    __u64x2:  _vx128_xtrv_l, \
    __u64x4:  _vx256_xtrv_l, \
    __u64x8:  _vx512_xtrv_l, \
    __u32x4:  _vx128_xtrv_w, \
    __u32x8:  _vx256_xtrv_w, \
    __u32x16: _vx512_xtrv_w)(a, b)


#define _vxtr_el(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_xtr_b, \
    __u8x32:  _vx256_xtr_b, \
    __u8x64:  _vx512_xtr_b, \
    __u16x8:  _vx128_xtr_h, \
    __u16x16: _vx256_xtr_h, \
    __u16x32: _vx512_xtr_h, \
    __u64x2:  _vx128_xtr_l, \
    __u64x4:  _vx256_xtr_l, \
    __u64x8:  _vx512_xtr_l, \
    __u32x4:  _vx128_xtr_w, \
    __u32x8:  _vx256_xtr_w, \
    __u32x16: _vx512_xtr_w)(a, b)


#define _vxtrx(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_xtrx_b, \
    __u8x32:  _vx256_xtrx_b, \
    __u8x64:  _vx512_xtrx_b, \
    __u16x8:  _vx128_xtrx_h, \
    __u16x16: _vx256_xtrx_h, \
    __u16x32: _vx512_xtrx_h, \
    __u64x2:  _vx128_xtrx_l, \
    __u64x4:  _vx256_xtrx_l, \
    __u64x8:  _vx512_xtrx_l, \
    __u32x4:  _vx128_xtrx_w, \
    __u32x8:  _vx256_xtrx_w, \
    __u32x16: _vx512_xtrx_w)(a, b)


#define _vxtrxx(a, b) \
    _Generic(a, \
    __u8x16:  _vx128_xtrxx_b, \
    __u8x32:  _vx256_xtrxx_b, \
    __u8x64:  _vx512_xtrxx_b, \
    __u16x8:  _vx128_xtrxx_h, \
    __u16x16: _vx256_xtrxx_h, \
    __u16x32: _vx512_xtrxx_h, \
    __u64x2:  _vx128_xtrxx_l, \
    __u64x4:  _vx256_xtrxx_l, \
    __u64x8:  _vx512_xtrxx_l, \
    __u32x4:  _vx128_xtrxx_w, \
    __u32x8:  _vx256_xtrxx_w, \
    __u32x16: _vx512_xtrxx_w)(a, b)


#define _vzxb(a) \
    _Generic(a, \
    __u16x8:  _vx128_zxb_h, \
    __u16x16: _vx256_zxb_h, \
    __u16x32: _vx512_zxb_h, \
    __u32x4:  _vx128_zxb_w, \
    __u32x8:  _vx256_zxb_w, \
    __u32x16: _vx512_zxb_w, \
    __u64x2:  _vx128_zxb_l, \
    __u64x4:  _vx256_zxb_l, \
    __u64x8:  _vx512_zxb_l)(a)


#define _vzxh(a) \
    _Generic(a, \
    __u32x4:  _vx128_zxh_w, \
    __u32x8:  _vx256_zxh_w, \
    __u32x16: _vx512_zxh_w, \
    __u64x2:  _vx128_zxh_l, \
    __u64x4:  _vx256_zxh_l, \
    __u64x8:  _vx512_zxh_l)(a)


#define _vzxw(a) \
    _Generic(a, \
    __u64x2:  _vx128_zxw_l, \
    __u64x4:  _vx256_zxw_l, \
    __u64x8:  _vx512_zxw_l)(a)

#define _vcast_to_512(a) \
    _Generic(a, \
    __f64x4:  _vx512_cast_v4df_to_v8df, \
    __f32x8:  _vx512_cast_v8sf_to_v16sf, \
    __u64x4:  _vx512_cast_v4di_to_v8di, \
    __u32x8:  _vx512_cast_v8si_to_v16si, \
    __u16x16: _vx512_cast_v16hi_to_v32hi, \
    __u8x32:  _vx512_cast_v32qi_to_v64qi, \
    __f64x2:  _vx512_cast_v2df_to_v8df, \
    __f32x4:  _vx512_cast_v4sf_to_v16sf, \
    __u64x2:  _vx512_cast_v2di_to_v8di, \
    __u32x4:  _vx512_cast_v4si_to_v16si, \
    __u16x8:  _vx512_cast_v8hi_to_v32hi, \
    __u8x16:  _vx512_cast_v16qi_to_v64qi, \
    __s64x4:  _vx512_casts_v4di_to_v8di, \
    __s32x8:  _vx512_casts_v8si_to_v16si, \
    __s16x16: _vx512_casts_v16hi_to_v32hi, \
    __s8x32:  _vx512_casts_v32qi_to_v64qi, \
    __s64x2:  _vx512_casts_v2di_to_v8di, \
    __s32x4:  _vx512_casts_v4si_to_v16si, \
    __s16x8:  _vx512_casts_v8hi_to_v32hi, \
    __s8x16:  _vx512_casts_v16qi_to_v64qi)(a)

#define _vcast_to_256(a) \
    _Generic(a, \
    __f64x8:  _vx512_cast_v8df_to_v4df, \
    __f32x16: _vx512_cast_v16sf_to_v8sf, \
    __u64x8:  _vx512_cast_v8di_to_v4di, \
    __u32x16: _vx512_cast_v16si_to_v8si, \
    __u16x32: _vx512_cast_v32hi_to_v16hi, \
    __u8x64:  _vx512_cast_v64qi_to_v32qi, \
    __s64x8:  _vx512_casts_v8di_to_v4di, \
    __s32x16: _vx512_casts_v16si_to_v8si, \
    __s16x32: _vx512_casts_v32hi_to_v16hi, \
    __s8x64:  _vx512_casts_v64qi_to_v32qi, \
    __f64x2:  _vx512_cast_v2df_to_v4df, \
    __f32x4:  _vx512_cast_v4sf_to_v8sf, \
    __u64x2:  _vx512_cast_v2di_to_v4di, \
    __u32x4:  _vx512_cast_v4si_to_v8si, \
    __u16x8:  _vx512_cast_v8hi_to_v16hi, \
    __u8x16:  _vx512_cast_v16qi_to_v32qi, \
    __s64x2:  _vx512_casts_v2di_to_v4di, \
    __s32x4:  _vx512_casts_v4si_to_v8si, \
    __s16x8:  _vx512_casts_v8hi_to_v16hi, \
    __s8x16:  _vx512_casts_v16qi_to_v32qi)(a)

#define _vcast_to_128(a) \
    _Generic(a, \
    __f64x8:  _vx512_cast_v8df_to_v2df, \
    __f32x16: _vx512_cast_v16sf_to_v4sf, \
    __u64x8:  _vx512_cast_v8di_to_v2di, \
    __u32x16: _vx512_cast_v16si_to_v4si, \
    __u16x32: _vx512_cast_v32hi_to_v8hi, \
    __u8x64:  _vx512_cast_v64qi_to_v16qi, \
    __s64x8:  _vx512_casts_v8di_to_v2di, \
    __s32x16: _vx512_casts_v16si_to_v4si, \
    __s16x32: _vx512_casts_v32hi_to_v8hi, \
    __s8x64:  _vx512_casts_v64qi_to_v16qi, \
    __f64x4:  _vx512_cast_v4df_to_v2df, \
    __f32x8:  _vx512_cast_v8sf_to_v4sf, \
    __u64x4:  _vx512_cast_v4di_to_v2di, \
    __u32x8:  _vx512_cast_v8si_to_v4si, \
    __u16x16: _vx512_cast_v16hi_to_v8hi, \
    __u8x32:  _vx512_cast_v32qi_to_v16qi, \
    __s64x4:  _vx512_casts_v4di_to_v2di, \
    __s32x8:  _vx512_casts_v8si_to_v4si, \
    __s16x16: _vx512_casts_v16hi_to_v8hi, \
    __s8x32:  _vx512_casts_v32qi_to_v16qi)(a)

#endif
