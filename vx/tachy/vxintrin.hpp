/* DO NOT EDIT THIS FILE MANUALLY! */
#pragma once

#ifdef __cplusplus
extern "C" {
#endif

#include <vxintrin.h>

#ifdef __cplusplus
}
#endif

namespace tvx { // Namespace of Tachyum vector instructions

typedef __u8x16   U8x16;
typedef __s8x16   S8x16;
typedef __u16x8   U16x8;
typedef __s16x8   S16x8;
typedef __u32x4   U32x4;
typedef __s32x4   S32x4;
typedef __u64x2   U64x2;
typedef __s64x2   S64x2;
typedef __f32x4   F32x4;
typedef __f64x2   F64x2;

typedef __u8x32   U8x32;
typedef __s8x32   S8x32;
typedef __u16x16  U16x16;
typedef __s16x16  S16x16;
typedef __u32x8   U32x8;
typedef __s32x8   S32x8;
typedef __u64x4   U64x4;
typedef __s64x4   S64x4;
typedef __f32x8   F32x8;
typedef __f64x4   F64x4;

typedef __u8x64   U8x64;
typedef __s8x64   S8x64;
typedef __u16x32  U16x32;
typedef __s16x32  S16x32;
typedef __u32x16  U32x16;
typedef __s32x16  S32x16;
typedef __u64x8   U64x8;
typedef __s64x8   S64x8;
typedef __f32x16  F32x16;
typedef __f64x8   F64x8;

template <typename T> struct get_base {typedef decltype(((T){})[0]) type;};

template <typename T> constexpr unsigned nrelem() {
    return sizeof(T)/sizeof(typename get_base<T>::type);
}

template <typename T, unsigned N> struct make {typedef void type;};
template <> struct make<float,  4> {typedef tvx::F32x4  type;};
template <> struct make<float,  8> {typedef tvx::F32x8  type;};
template <> struct make<float, 16> {typedef tvx::F32x16 type;};
template <> struct make<double, 2> {typedef tvx::F64x2  type;};
template <> struct make<double, 4> {typedef tvx::F64x4  type;};
template <> struct make<double, 8> {typedef tvx::F64x8  type;};
template <> struct make<int16_t, 8> {typedef tvx::S16x8 type;};
template <> struct make<int32_t, 4> {typedef tvx::S32x4 type;};
template <> struct make<int32_t, 8> {typedef tvx::S32x8 type;};
template <> struct make<int32_t,16> {typedef tvx::S32x16 type;};
template <> struct make<uint32_t, 4> {typedef tvx::U32x4 type;};
template <> struct make<uint32_t, 8> {typedef tvx::U32x8 type;};
template <> struct make<uint32_t,16> {typedef tvx::U32x16 type;};

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(U8x64 a) {
    return __builtin_tachy_hadd_b_512(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(F64x8 a) {
    return __builtin_tachy_hadd_d_512(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(F32x16 a) {
    return __builtin_tachy_hadd_f_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(U16x32 a) {
    return __builtin_tachy_hadd_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(U64x8 a) {
    return __builtin_tachy_hadd_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hadd(U32x16 a) {
    return __builtin_tachy_hadd_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hand(U8x64 a) {
    return __builtin_tachy_hand_b_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hand(U16x32 a) {
    return __builtin_tachy_hand_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hand(U64x8 a) {
    return __builtin_tachy_hand_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hand(U32x16 a) {
    return __builtin_tachy_hand_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(U8x64 a) {
    return __builtin_tachy_hmax_b_512(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(F64x8 a) {
    return __builtin_tachy_hmax_d_512(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(F32x16 a) {
    return __builtin_tachy_hmax_f_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(U16x32 a) {
    return __builtin_tachy_hmax_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(U64x8 a) {
    return __builtin_tachy_hmax_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmax(U32x16 a) {
    return __builtin_tachy_hmax_w_512(a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmaxs(S8x64 a) {
    return __builtin_tachy_hmaxs_b_512(a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmaxs(S16x32 a) {
    return __builtin_tachy_hmaxs_h_512(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmaxs(S64x8 a) {
    return __builtin_tachy_hmaxs_l_512(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmaxs(S32x16 a) {
    return __builtin_tachy_hmaxs_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(U8x64 a) {
    return __builtin_tachy_hmin_b_512(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(F64x8 a) {
    return __builtin_tachy_hmin_d_512(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(F32x16 a) {
    return __builtin_tachy_hmin_f_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(U16x32 a) {
    return __builtin_tachy_hmin_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(U64x8 a) {
    return __builtin_tachy_hmin_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmin(U32x16 a) {
    return __builtin_tachy_hmin_w_512(a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmins(S8x64 a) {
    return __builtin_tachy_hmins_b_512(a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmins(S16x32 a) {
    return __builtin_tachy_hmins_h_512(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmins(S64x8 a) {
    return __builtin_tachy_hmins_l_512(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmins(S32x16 a) {
    return __builtin_tachy_hmins_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(U8x64 a) {
    return __builtin_tachy_hmul_b_512(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(F64x8 a) {
    return __builtin_tachy_hmul_d_512(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(F32x16 a) {
    return __builtin_tachy_hmul_f_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(U16x32 a) {
    return __builtin_tachy_hmul_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(U64x8 a) {
    return __builtin_tachy_hmul_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hmul(U32x16 a) {
    return __builtin_tachy_hmul_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hor(U8x64 a) {
    return __builtin_tachy_hor_b_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hor(U16x32 a) {
    return __builtin_tachy_hor_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hor(U64x8 a) {
    return __builtin_tachy_hor_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hor(U32x16 a) {
    return __builtin_tachy_hor_w_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hxor(U8x64 a) {
    return __builtin_tachy_hxor_b_512(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hxor(U16x32 a) {
    return __builtin_tachy_hxor_h_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hxor(U64x8 a) {
    return __builtin_tachy_hxor_l_512(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
hxor(U32x16 a) {
    return __builtin_tachy_hxor_w_512(a);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S8x16 a) {
    return __builtin_tachy_vabs1_128(a);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S8x32 a) {
    return __builtin_tachy_vabs1_256(a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S8x64 a) {
    return __builtin_tachy_vabs1_512(a);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S16x8 a) {
    return __builtin_tachy_vabs2_128(a);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S16x16 a) {
    return __builtin_tachy_vabs2_256(a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S16x32 a) {
    return __builtin_tachy_vabs2_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S32x4 a) {
    return __builtin_tachy_vabs4_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S32x8 a) {
    return __builtin_tachy_vabs4_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S32x16 a) {
    return __builtin_tachy_vabs4_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S64x2 a) {
    return __builtin_tachy_vabs8_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S64x4 a) {
    return __builtin_tachy_vabs8_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(S64x8 a) {
    return __builtin_tachy_vabs8_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F64x2 a) {
    return __builtin_tachy_vabs_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F64x4 a) {
    return __builtin_tachy_vabs_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F64x8 a) {
    return __builtin_tachy_vabs_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F32x4 a) {
    return __builtin_tachy_vabs_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F32x8 a) {
    return __builtin_tachy_vabs_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vabs(F32x16 a) {
    return __builtin_tachy_vabs_f_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U8x16 a, U8x16 b) {
    return __builtin_tachy_vadd1_128(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S8x16 a, S8x16 b) {
    return (S8x16) __builtin_tachy_vadd1_128((__u8x16) a, (__u8x16) b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U8x32 a, U8x32 b) {
    return __builtin_tachy_vadd1_256(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S8x32 a, S8x32 b) {
    return (S8x32) __builtin_tachy_vadd1_256((__u8x32) a, (__u8x32) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U8x64 a, U8x64 b) {
    return __builtin_tachy_vadd1_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vadd1_512((__u8x64) a, (__u8x64) b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U16x8 a, U16x8 b) {
    return __builtin_tachy_vadd2_128(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S16x8 a, S16x8 b) {
    return (S16x8) __builtin_tachy_vadd2_128((__u16x8) a, (__u16x8) b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U16x16 a, U16x16 b) {
    return __builtin_tachy_vadd2_256(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S16x16 a, S16x16 b) {
    return (S16x16) __builtin_tachy_vadd2_256((__u16x16) a, (__u16x16) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U16x32 a, U16x32 b) {
    return __builtin_tachy_vadd2_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vadd2_512((__u16x32) a, (__u16x32) b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U32x4 a, U32x4 b) {
    return __builtin_tachy_vadd4_128(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S32x4 a, S32x4 b) {
    return (S32x4) __builtin_tachy_vadd4_128((__u32x4) a, (__u32x4) b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U32x8 a, U32x8 b) {
    return __builtin_tachy_vadd4_256(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S32x8 a, S32x8 b) {
    return (S32x8) __builtin_tachy_vadd4_256((__u32x8) a, (__u32x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U32x16 a, U32x16 b) {
    return __builtin_tachy_vadd4_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vadd4_512((__u32x16) a, (__u32x16) b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U64x2 a, U64x2 b) {
    return __builtin_tachy_vadd8_128(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S64x2 a, S64x2 b) {
    return (S64x2) __builtin_tachy_vadd8_128((__u64x2) a, (__u64x2) b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U64x4 a, U64x4 b) {
    return __builtin_tachy_vadd8_256(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S64x4 a, S64x4 b) {
    return (S64x4) __builtin_tachy_vadd8_256((__u64x4) a, (__u64x4) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(U64x8 a, U64x8 b) {
    return __builtin_tachy_vadd8_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vadd8_512((__u64x8) a, (__u64x8) b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F64x2 a, F64x2 b) {
    return __builtin_tachy_vadd_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F64x4 a, F64x4 b) {
    return __builtin_tachy_vadd_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F64x8 a, F64x8 b) {
    return __builtin_tachy_vadd_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F32x4 a, F32x4 b) {
    return __builtin_tachy_vadd_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F32x8 a, F32x8 b) {
    return __builtin_tachy_vadd_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadd(F32x16 a, F32x16 b) {
    return __builtin_tachy_vadd_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vaddc1_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vaddc1_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vaddc1_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vaddc2_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vaddc2_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vaddc2_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vaddc4_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vaddc4_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vaddc4_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vaddc8_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vaddc8_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddc(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vaddc8_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x16 a, U8x16 b) {
    return __builtin_tachy_vaddco_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x32 a, U8x32 b) {
    return __builtin_tachy_vaddco_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x64 a, U8x64 b) {
    return __builtin_tachy_vaddco_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x8 a, U16x8 b) {
    return __builtin_tachy_vaddco_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x16 a, U16x16 b) {
    return __builtin_tachy_vaddco_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x32 a, U16x32 b) {
    return __builtin_tachy_vaddco_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x2 a, U64x2 b) {
    return __builtin_tachy_vaddco_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x4 a, U64x4 b) {
    return __builtin_tachy_vaddco_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x8 a, U64x8 b) {
    return __builtin_tachy_vaddco_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x4 a, U32x4 b) {
    return __builtin_tachy_vaddco_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x8 a, U32x8 b) {
    return __builtin_tachy_vaddco_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x16 a, U32x16 b) {
    return __builtin_tachy_vaddco_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vaddco_x_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vaddco_x_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vaddco_x_b_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vaddco_x_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vaddco_x_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vaddco_x_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vaddco_x_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vaddco_x_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vaddco_x_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vaddco_x_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vaddco_x_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddco(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vaddco_x_w_512(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U8x64 a, U8x64 b) {
    return __builtin_tachy_vaddp_b_512(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F64x8 a, F64x8 b) {
    return __builtin_tachy_vaddp_d_512(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F32x16 a, F32x16 b) {
    return __builtin_tachy_vaddp_f_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U16x32 a, U16x32 b) {
    return __builtin_tachy_vaddp_h_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U8x16 a) {
    return __builtin_tachy_vaddp_k_b_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U8x32 a) {
    return __builtin_tachy_vaddp_k_b_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U8x64 a) {
    return __builtin_tachy_vaddp_k_b_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F64x2 a) {
    return __builtin_tachy_vaddp_k_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F64x4 a) {
    return __builtin_tachy_vaddp_k_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F64x8 a) {
    return __builtin_tachy_vaddp_k_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F32x4 a) {
    return __builtin_tachy_vaddp_k_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F32x8 a) {
    return __builtin_tachy_vaddp_k_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(F32x16 a) {
    return __builtin_tachy_vaddp_k_f_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U16x8 a) {
    return __builtin_tachy_vaddp_k_h_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U16x16 a) {
    return __builtin_tachy_vaddp_k_h_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U16x32 a) {
    return __builtin_tachy_vaddp_k_h_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U64x2 a) {
    return __builtin_tachy_vaddp_k_l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U64x4 a) {
    return __builtin_tachy_vaddp_k_l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U64x8 a) {
    return __builtin_tachy_vaddp_k_l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U32x4 a) {
    return __builtin_tachy_vaddp_k_w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U32x8 a) {
    return __builtin_tachy_vaddp_k_w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U32x16 a) {
    return __builtin_tachy_vaddp_k_w_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U64x8 a, U64x8 b) {
    return __builtin_tachy_vaddp_l_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddp(U32x16 a, U32x16 b) {
    return __builtin_tachy_vaddp_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U8x16 a, U8x16 b) {
    return __builtin_tachy_vadds1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U8x32 a, U8x32 b) {
    return __builtin_tachy_vadds1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U8x64 a, U8x64 b) {
    return __builtin_tachy_vadds1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U16x8 a, U16x8 b) {
    return __builtin_tachy_vadds2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U16x16 a, U16x16 b) {
    return __builtin_tachy_vadds2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U16x32 a, U16x32 b) {
    return __builtin_tachy_vadds2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U32x4 a, U32x4 b) {
    return __builtin_tachy_vadds4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U32x8 a, U32x8 b) {
    return __builtin_tachy_vadds4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U32x16 a, U32x16 b) {
    return __builtin_tachy_vadds4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U64x2 a, U64x2 b) {
    return __builtin_tachy_vadds8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U64x4 a, U64x4 b) {
    return __builtin_tachy_vadds8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadds(U64x8 a, U64x8 b) {
    return __builtin_tachy_vadds8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U8x16 a, U8x16 b) {
    return __builtin_tachy_vaddu1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U8x32 a, U8x32 b) {
    return __builtin_tachy_vaddu1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U8x64 a, U8x64 b) {
    return __builtin_tachy_vaddu1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U16x8 a, U16x8 b) {
    return __builtin_tachy_vaddu2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U16x16 a, U16x16 b) {
    return __builtin_tachy_vaddu2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U16x32 a, U16x32 b) {
    return __builtin_tachy_vaddu2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U32x4 a, U32x4 b) {
    return __builtin_tachy_vaddu4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U32x8 a, U32x8 b) {
    return __builtin_tachy_vaddu4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U32x16 a, U32x16 b) {
    return __builtin_tachy_vaddu4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U64x2 a, U64x2 b) {
    return __builtin_tachy_vaddu8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U64x4 a, U64x4 b) {
    return __builtin_tachy_vaddu8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vaddu(U64x8 a, U64x8 b) {
    return __builtin_tachy_vaddu8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F64x2 a, F64x2 b) {
    return __builtin_tachy_vadif_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F64x4 a, F64x4 b) {
    return __builtin_tachy_vadif_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F64x8 a, F64x8 b) {
    return __builtin_tachy_vadif_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F32x4 a, F32x4 b) {
    return __builtin_tachy_vadif_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F32x8 a, F32x8 b) {
    return __builtin_tachy_vadif_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vadif(F32x16 a, F32x16 b) {
    return __builtin_tachy_vadif_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U8x16 a, U8x16 b) {
    return __builtin_tachy_vage_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U8x32 a, U8x32 b) {
    return __builtin_tachy_vage_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U8x64 a, U8x64 b) {
    return __builtin_tachy_vage_b_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F64x2 a, F64x2 b) {
    return __builtin_tachy_vage_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F64x4 a, F64x4 b) {
    return __builtin_tachy_vage_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F64x8 a, F64x8 b) {
    return __builtin_tachy_vage_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F32x4 a, F32x4 b) {
    return __builtin_tachy_vage_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F32x8 a, F32x8 b) {
    return __builtin_tachy_vage_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(F32x16 a, F32x16 b) {
    return __builtin_tachy_vage_f_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U16x8 a, U16x8 b) {
    return __builtin_tachy_vage_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U16x16 a, U16x16 b) {
    return __builtin_tachy_vage_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U16x32 a, U16x32 b) {
    return __builtin_tachy_vage_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U64x2 a, U64x2 b) {
    return __builtin_tachy_vage_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U64x4 a, U64x4 b) {
    return __builtin_tachy_vage_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U64x8 a, U64x8 b) {
    return __builtin_tachy_vage_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U32x4 a, U32x4 b) {
    return __builtin_tachy_vage_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U32x8 a, U32x8 b) {
    return __builtin_tachy_vage_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vage(U32x16 a, U32x16 b) {
    return __builtin_tachy_vage_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U8x16 a, U8x16 b) {
    return __builtin_tachy_vagt_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U8x32 a, U8x32 b) {
    return __builtin_tachy_vagt_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U8x64 a, U8x64 b) {
    return __builtin_tachy_vagt_b_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F64x2 a, F64x2 b) {
    return __builtin_tachy_vagt_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F64x4 a, F64x4 b) {
    return __builtin_tachy_vagt_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F64x8 a, F64x8 b) {
    return __builtin_tachy_vagt_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F32x4 a, F32x4 b) {
    return __builtin_tachy_vagt_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F32x8 a, F32x8 b) {
    return __builtin_tachy_vagt_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(F32x16 a, F32x16 b) {
    return __builtin_tachy_vagt_f_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U16x8 a, U16x8 b) {
    return __builtin_tachy_vagt_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U16x16 a, U16x16 b) {
    return __builtin_tachy_vagt_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U16x32 a, U16x32 b) {
    return __builtin_tachy_vagt_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U64x2 a, U64x2 b) {
    return __builtin_tachy_vagt_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U64x4 a, U64x4 b) {
    return __builtin_tachy_vagt_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U64x8 a, U64x8 b) {
    return __builtin_tachy_vagt_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U32x4 a, U32x4 b) {
    return __builtin_tachy_vagt_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U32x8 a, U32x8 b) {
    return __builtin_tachy_vagt_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vagt(U32x16 a, U32x16 b) {
    return __builtin_tachy_vagt_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U8x16 a, U8x16 b) {
    return __builtin_tachy_vale_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U8x32 a, U8x32 b) {
    return __builtin_tachy_vale_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U8x64 a, U8x64 b) {
    return __builtin_tachy_vale_b_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F64x2 a, F64x2 b) {
    return __builtin_tachy_vale_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F64x4 a, F64x4 b) {
    return __builtin_tachy_vale_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F64x8 a, F64x8 b) {
    return __builtin_tachy_vale_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F32x4 a, F32x4 b) {
    return __builtin_tachy_vale_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F32x8 a, F32x8 b) {
    return __builtin_tachy_vale_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(F32x16 a, F32x16 b) {
    return __builtin_tachy_vale_f_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U16x8 a, U16x8 b) {
    return __builtin_tachy_vale_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U16x16 a, U16x16 b) {
    return __builtin_tachy_vale_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U16x32 a, U16x32 b) {
    return __builtin_tachy_vale_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U64x2 a, U64x2 b) {
    return __builtin_tachy_vale_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U64x4 a, U64x4 b) {
    return __builtin_tachy_vale_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U64x8 a, U64x8 b) {
    return __builtin_tachy_vale_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U32x4 a, U32x4 b) {
    return __builtin_tachy_vale_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U32x8 a, U32x8 b) {
    return __builtin_tachy_vale_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vale(U32x16 a, U32x16 b) {
    return __builtin_tachy_vale_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U8x16 a, U8x16 b) {
    return __builtin_tachy_valt_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U8x32 a, U8x32 b) {
    return __builtin_tachy_valt_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U8x64 a, U8x64 b) {
    return __builtin_tachy_valt_b_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F64x2 a, F64x2 b) {
    return __builtin_tachy_valt_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F64x4 a, F64x4 b) {
    return __builtin_tachy_valt_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F64x8 a, F64x8 b) {
    return __builtin_tachy_valt_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F32x4 a, F32x4 b) {
    return __builtin_tachy_valt_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F32x8 a, F32x8 b) {
    return __builtin_tachy_valt_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(F32x16 a, F32x16 b) {
    return __builtin_tachy_valt_f_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U16x8 a, U16x8 b) {
    return __builtin_tachy_valt_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U16x16 a, U16x16 b) {
    return __builtin_tachy_valt_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U16x32 a, U16x32 b) {
    return __builtin_tachy_valt_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U64x2 a, U64x2 b) {
    return __builtin_tachy_valt_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U64x4 a, U64x4 b) {
    return __builtin_tachy_valt_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U64x8 a, U64x8 b) {
    return __builtin_tachy_valt_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U32x4 a, U32x4 b) {
    return __builtin_tachy_valt_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U32x8 a, U32x8 b) {
    return __builtin_tachy_valt_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
valt(U32x16 a, U32x16 b) {
    return __builtin_tachy_valt_w_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F64x2 a, F64x2 b) {
    return __builtin_tachy_vamax_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F64x4 a, F64x4 b) {
    return __builtin_tachy_vamax_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F64x8 a, F64x8 b) {
    return __builtin_tachy_vamax_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F32x4 a, F32x4 b) {
    return __builtin_tachy_vamax_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F32x8 a, F32x8 b) {
    return __builtin_tachy_vamax_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamax(F32x16 a, F32x16 b) {
    return __builtin_tachy_vamax_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F64x2 a, F64x2 b) {
    return __builtin_tachy_vamin_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F64x4 a, F64x4 b) {
    return __builtin_tachy_vamin_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F64x8 a, F64x8 b) {
    return __builtin_tachy_vamin_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F32x4 a, F32x4 b) {
    return __builtin_tachy_vamin_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F32x8 a, F32x8 b) {
    return __builtin_tachy_vamin_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vamin(F32x16 a, F32x16 b) {
    return __builtin_tachy_vamin_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U8x16 a, U8x16 b) {
    return __builtin_tachy_vand1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U8x32 a, U8x32 b) {
    return __builtin_tachy_vand1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U8x64 a, U8x64 b) {
    return __builtin_tachy_vand1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U16x8 a, U16x8 b) {
    return __builtin_tachy_vand2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U16x16 a, U16x16 b) {
    return __builtin_tachy_vand2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U16x32 a, U16x32 b) {
    return __builtin_tachy_vand2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U32x4 a, U32x4 b) {
    return __builtin_tachy_vand4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U32x8 a, U32x8 b) {
    return __builtin_tachy_vand4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U32x16 a, U32x16 b) {
    return __builtin_tachy_vand4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U64x2 a, U64x2 b) {
    return __builtin_tachy_vand8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U64x4 a, U64x4 b) {
    return __builtin_tachy_vand8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vand(U64x8 a, U64x8 b) {
    return __builtin_tachy_vand8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U8x16 a, U8x16 b) {
    return __builtin_tachy_vann1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U8x32 a, U8x32 b) {
    return __builtin_tachy_vann1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U8x64 a, U8x64 b) {
    return __builtin_tachy_vann1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U16x8 a, U16x8 b) {
    return __builtin_tachy_vann2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U16x16 a, U16x16 b) {
    return __builtin_tachy_vann2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U16x32 a, U16x32 b) {
    return __builtin_tachy_vann2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U32x4 a, U32x4 b) {
    return __builtin_tachy_vann4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U32x8 a, U32x8 b) {
    return __builtin_tachy_vann4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U32x16 a, U32x16 b) {
    return __builtin_tachy_vann4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U64x2 a, U64x2 b) {
    return __builtin_tachy_vann8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U64x4 a, U64x4 b) {
    return __builtin_tachy_vann8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vann(U64x8 a, U64x8 b) {
    return __builtin_tachy_vann8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U8x16 a, U8x16 b) {
    return __builtin_tachy_vavg1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U8x32 a, U8x32 b) {
    return __builtin_tachy_vavg1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U8x64 a, U8x64 b) {
    return __builtin_tachy_vavg1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U16x8 a, U16x8 b) {
    return __builtin_tachy_vavg2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U16x16 a, U16x16 b) {
    return __builtin_tachy_vavg2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U16x32 a, U16x32 b) {
    return __builtin_tachy_vavg2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U32x4 a, U32x4 b) {
    return __builtin_tachy_vavg4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U32x8 a, U32x8 b) {
    return __builtin_tachy_vavg4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U32x16 a, U32x16 b) {
    return __builtin_tachy_vavg4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U64x2 a, U64x2 b) {
    return __builtin_tachy_vavg8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U64x4 a, U64x4 b) {
    return __builtin_tachy_vavg8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vavg(U64x8 a, U64x8 b) {
    return __builtin_tachy_vavg8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U8x16 a) {
    return __builtin_tachy_vclz1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U8x32 a) {
    return __builtin_tachy_vclz1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U8x64 a) {
    return __builtin_tachy_vclz1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U16x8 a) {
    return __builtin_tachy_vclz2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U16x16 a) {
    return __builtin_tachy_vclz2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U16x32 a) {
    return __builtin_tachy_vclz2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U32x4 a) {
    return __builtin_tachy_vclz4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U32x8 a) {
    return __builtin_tachy_vclz4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U32x16 a) {
    return __builtin_tachy_vclz4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U64x2 a) {
    return __builtin_tachy_vclz8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U64x4 a) {
    return __builtin_tachy_vclz8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vclz(U64x8 a) {
    return __builtin_tachy_vclz8_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U8x16 a) {
    return __builtin_tachy_vctz1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U8x32 a) {
    return __builtin_tachy_vctz1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U8x64 a) {
    return __builtin_tachy_vctz1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U16x8 a) {
    return __builtin_tachy_vctz2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U16x16 a) {
    return __builtin_tachy_vctz2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U16x32 a) {
    return __builtin_tachy_vctz2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U32x4 a) {
    return __builtin_tachy_vctz4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U32x8 a) {
    return __builtin_tachy_vctz4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U32x16 a) {
    return __builtin_tachy_vctz4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U64x2 a) {
    return __builtin_tachy_vctz8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U64x4 a) {
    return __builtin_tachy_vctz8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vctz(U64x8 a) {
    return __builtin_tachy_vctz8_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_d2l(F64x2 a) {
    return __builtin_tachy_vcvt_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_d2l(F64x4 a) {
    return __builtin_tachy_vcvt_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_d2l(F64x8 a) {
    return __builtin_tachy_vcvt_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_d2w(F64x4 a) {
    return __builtin_tachy_vcvt_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_d2w(F64x8 a) {
    return __builtin_tachy_vcvt_d2w_256(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2d(F32x4 a) {
    return __builtin_tachy_vcvt_f2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2d(F32x8 a) {
    return __builtin_tachy_vcvt_f2d_512(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2l(F32x4 a) {
    return __builtin_tachy_vcvt_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2l(F32x8 a) {
    return __builtin_tachy_vcvt_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2w(F32x4 a) {
    return __builtin_tachy_vcvt_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2w(F32x8 a) {
    return __builtin_tachy_vcvt_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_f2w(F32x16 a) {
    return __builtin_tachy_vcvt_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_l2d(S64x2 a) {
    return __builtin_tachy_vcvt_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_l2d(S64x4 a) {
    return __builtin_tachy_vcvt_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_l2d(S64x8 a) {
    return __builtin_tachy_vcvt_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_l2f(S64x4 a) {
    return __builtin_tachy_vcvt_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_l2f(S64x8 a) {
    return __builtin_tachy_vcvt_l2f_256(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_w2d(S32x4 a) {
    return __builtin_tachy_vcvt_w2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_w2d(S32x8 a) {
    return __builtin_tachy_vcvt_w2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_w2f(S32x4 a) {
    return __builtin_tachy_vcvt_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_w2f(S32x8 a) {
    return __builtin_tachy_vcvt_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvt_w2f(S32x16 a) {
    return __builtin_tachy_vcvt_w2f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_d2l(F64x2 a) {
    return __builtin_tachy_vcvta_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_d2l(F64x4 a) {
    return __builtin_tachy_vcvta_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_d2l(F64x8 a) {
    return __builtin_tachy_vcvta_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_d2w(F64x4 a) {
    return __builtin_tachy_vcvta_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_d2w(F64x8 a) {
    return __builtin_tachy_vcvta_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_f2l(F32x4 a) {
    return __builtin_tachy_vcvta_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_f2l(F32x8 a) {
    return __builtin_tachy_vcvta_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_f2w(F32x4 a) {
    return __builtin_tachy_vcvta_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_f2w(F32x8 a) {
    return __builtin_tachy_vcvta_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_f2w(F32x16 a) {
    return __builtin_tachy_vcvta_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_l2d(S64x2 a) {
    return __builtin_tachy_vcvta_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_l2d(S64x4 a) {
    return __builtin_tachy_vcvta_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_l2d(S64x8 a) {
    return __builtin_tachy_vcvta_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_l2f(S64x4 a) {
    return __builtin_tachy_vcvta_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_l2f(S64x8 a) {
    return __builtin_tachy_vcvta_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_w2f(S32x4 a) {
    return __builtin_tachy_vcvta_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_w2f(S32x8 a) {
    return __builtin_tachy_vcvta_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvta_w2f(S32x16 a) {
    return __builtin_tachy_vcvta_w2f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_d2l(F64x2 a) {
    return __builtin_tachy_vcvtn_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_d2l(F64x4 a) {
    return __builtin_tachy_vcvtn_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_d2l(F64x8 a) {
    return __builtin_tachy_vcvtn_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_d2w(F64x4 a) {
    return __builtin_tachy_vcvtn_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_d2w(F64x8 a) {
    return __builtin_tachy_vcvtn_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_f2l(F32x4 a) {
    return __builtin_tachy_vcvtn_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_f2l(F32x8 a) {
    return __builtin_tachy_vcvtn_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_f2w(F32x4 a) {
    return __builtin_tachy_vcvtn_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_f2w(F32x8 a) {
    return __builtin_tachy_vcvtn_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_f2w(F32x16 a) {
    return __builtin_tachy_vcvtn_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_l2d(S64x2 a) {
    return __builtin_tachy_vcvtn_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_l2d(S64x4 a) {
    return __builtin_tachy_vcvtn_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_l2d(S64x8 a) {
    return __builtin_tachy_vcvtn_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_l2f(S64x4 a) {
    return __builtin_tachy_vcvtn_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_l2f(S64x8 a) {
    return __builtin_tachy_vcvtn_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_w2f(S32x4 a) {
    return __builtin_tachy_vcvtn_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_w2f(S32x8 a) {
    return __builtin_tachy_vcvtn_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtn_w2f(S32x16 a) {
    return __builtin_tachy_vcvtn_w2f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_d2l(F64x2 a) {
    return __builtin_tachy_vcvtp_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_d2l(F64x4 a) {
    return __builtin_tachy_vcvtp_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_d2l(F64x8 a) {
    return __builtin_tachy_vcvtp_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_d2w(F64x4 a) {
    return __builtin_tachy_vcvtp_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_d2w(F64x8 a) {
    return __builtin_tachy_vcvtp_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_f2l(F32x4 a) {
    return __builtin_tachy_vcvtp_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_f2l(F32x8 a) {
    return __builtin_tachy_vcvtp_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_f2w(F32x4 a) {
    return __builtin_tachy_vcvtp_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_f2w(F32x8 a) {
    return __builtin_tachy_vcvtp_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_f2w(F32x16 a) {
    return __builtin_tachy_vcvtp_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_l2d(S64x2 a) {
    return __builtin_tachy_vcvtp_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_l2d(S64x4 a) {
    return __builtin_tachy_vcvtp_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_l2d(S64x8 a) {
    return __builtin_tachy_vcvtp_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_l2f(S64x4 a) {
    return __builtin_tachy_vcvtp_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_l2f(S64x8 a) {
    return __builtin_tachy_vcvtp_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_w2f(S32x4 a) {
    return __builtin_tachy_vcvtp_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_w2f(S32x8 a) {
    return __builtin_tachy_vcvtp_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtp_w2f(S32x16 a) {
    return __builtin_tachy_vcvtp_w2f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_d2l(F64x2 a) {
    return __builtin_tachy_vcvtr_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_d2l(F64x4 a) {
    return __builtin_tachy_vcvtr_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_d2l(F64x8 a) {
    return __builtin_tachy_vcvtr_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_d2w(F64x4 a) {
    return __builtin_tachy_vcvtr_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_d2w(F64x8 a) {
    return __builtin_tachy_vcvtr_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_f2l(F32x4 a) {
    return __builtin_tachy_vcvtr_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_f2l(F32x8 a) {
    return __builtin_tachy_vcvtr_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_f2w(F32x4 a) {
    return __builtin_tachy_vcvtr_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_f2w(F32x8 a) {
    return __builtin_tachy_vcvtr_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_f2w(F32x16 a) {
    return __builtin_tachy_vcvtr_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_l2d(S64x2 a) {
    return __builtin_tachy_vcvtr_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_l2d(S64x4 a) {
    return __builtin_tachy_vcvtr_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_l2d(S64x8 a) {
    return __builtin_tachy_vcvtr_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_l2f(S64x4 a) {
    return __builtin_tachy_vcvtr_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_l2f(S64x8 a) {
    return __builtin_tachy_vcvtr_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_w2f(S32x4 a) {
    return __builtin_tachy_vcvtr_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_w2f(S32x8 a) {
    return __builtin_tachy_vcvtr_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtr_w2f(S32x16 a) {
    return __builtin_tachy_vcvtr_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_d2l(F64x2 a) {
    return __builtin_tachy_vcvts_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_d2l(F64x4 a) {
    return __builtin_tachy_vcvts_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_d2l(F64x8 a) {
    return __builtin_tachy_vcvts_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_d2w(F64x4 a) {
    return __builtin_tachy_vcvts_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_d2w(F64x8 a) {
    return __builtin_tachy_vcvts_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_f2l(F32x4 a) {
    return __builtin_tachy_vcvts_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_f2l(F32x8 a) {
    return __builtin_tachy_vcvts_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_f2w(F32x4 a) {
    return __builtin_tachy_vcvts_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_f2w(F32x8 a) {
    return __builtin_tachy_vcvts_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_f2w(F32x16 a) {
    return __builtin_tachy_vcvts_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_l2d(S64x2 a) {
    return __builtin_tachy_vcvts_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_l2d(S64x4 a) {
    return __builtin_tachy_vcvts_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_l2d(S64x8 a) {
    return __builtin_tachy_vcvts_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_l2f(S64x4 a) {
    return __builtin_tachy_vcvts_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_l2f(S64x8 a) {
    return __builtin_tachy_vcvts_l2f_256(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_w2d(S32x4 a) {
    return __builtin_tachy_vcvts_w2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_w2d(S32x8 a) {
    return __builtin_tachy_vcvts_w2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_w2f(S32x4 a) {
    return __builtin_tachy_vcvts_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_w2f(S32x8 a) {
    return __builtin_tachy_vcvts_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvts_w2f(S32x16 a) {
    return __builtin_tachy_vcvts_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_d2l(F64x2 a) {
    return __builtin_tachy_vcvtsa_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_d2l(F64x4 a) {
    return __builtin_tachy_vcvtsa_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_d2l(F64x8 a) {
    return __builtin_tachy_vcvtsa_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_d2w(F64x4 a) {
    return __builtin_tachy_vcvtsa_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_d2w(F64x8 a) {
    return __builtin_tachy_vcvtsa_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_f2l(F32x4 a) {
    return __builtin_tachy_vcvtsa_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_f2l(F32x8 a) {
    return __builtin_tachy_vcvtsa_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_f2w(F32x4 a) {
    return __builtin_tachy_vcvtsa_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_f2w(F32x8 a) {
    return __builtin_tachy_vcvtsa_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_f2w(F32x16 a) {
    return __builtin_tachy_vcvtsa_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_l2d(S64x2 a) {
    return __builtin_tachy_vcvtsa_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_l2d(S64x4 a) {
    return __builtin_tachy_vcvtsa_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_l2d(S64x8 a) {
    return __builtin_tachy_vcvtsa_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_l2f(S64x4 a) {
    return __builtin_tachy_vcvtsa_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_l2f(S64x8 a) {
    return __builtin_tachy_vcvtsa_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_w2f(S32x4 a) {
    return __builtin_tachy_vcvtsa_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_w2f(S32x8 a) {
    return __builtin_tachy_vcvtsa_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsa_w2f(S32x16 a) {
    return __builtin_tachy_vcvtsa_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_d2l(F64x2 a) {
    return __builtin_tachy_vcvtsn_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_d2l(F64x4 a) {
    return __builtin_tachy_vcvtsn_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_d2l(F64x8 a) {
    return __builtin_tachy_vcvtsn_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_d2w(F64x4 a) {
    return __builtin_tachy_vcvtsn_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_d2w(F64x8 a) {
    return __builtin_tachy_vcvtsn_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_f2l(F32x4 a) {
    return __builtin_tachy_vcvtsn_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_f2l(F32x8 a) {
    return __builtin_tachy_vcvtsn_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_f2w(F32x4 a) {
    return __builtin_tachy_vcvtsn_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_f2w(F32x8 a) {
    return __builtin_tachy_vcvtsn_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_f2w(F32x16 a) {
    return __builtin_tachy_vcvtsn_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_l2d(S64x2 a) {
    return __builtin_tachy_vcvtsn_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_l2d(S64x4 a) {
    return __builtin_tachy_vcvtsn_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_l2d(S64x8 a) {
    return __builtin_tachy_vcvtsn_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_l2f(S64x4 a) {
    return __builtin_tachy_vcvtsn_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_l2f(S64x8 a) {
    return __builtin_tachy_vcvtsn_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_w2f(S32x4 a) {
    return __builtin_tachy_vcvtsn_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_w2f(S32x8 a) {
    return __builtin_tachy_vcvtsn_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsn_w2f(S32x16 a) {
    return __builtin_tachy_vcvtsn_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_d2l(F64x2 a) {
    return __builtin_tachy_vcvtsp_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_d2l(F64x4 a) {
    return __builtin_tachy_vcvtsp_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_d2l(F64x8 a) {
    return __builtin_tachy_vcvtsp_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_d2w(F64x4 a) {
    return __builtin_tachy_vcvtsp_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_d2w(F64x8 a) {
    return __builtin_tachy_vcvtsp_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_f2l(F32x4 a) {
    return __builtin_tachy_vcvtsp_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_f2l(F32x8 a) {
    return __builtin_tachy_vcvtsp_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_f2w(F32x4 a) {
    return __builtin_tachy_vcvtsp_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_f2w(F32x8 a) {
    return __builtin_tachy_vcvtsp_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_f2w(F32x16 a) {
    return __builtin_tachy_vcvtsp_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_l2d(S64x2 a) {
    return __builtin_tachy_vcvtsp_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_l2d(S64x4 a) {
    return __builtin_tachy_vcvtsp_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_l2d(S64x8 a) {
    return __builtin_tachy_vcvtsp_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_l2f(S64x4 a) {
    return __builtin_tachy_vcvtsp_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_l2f(S64x8 a) {
    return __builtin_tachy_vcvtsp_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_w2f(S32x4 a) {
    return __builtin_tachy_vcvtsp_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_w2f(S32x8 a) {
    return __builtin_tachy_vcvtsp_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsp_w2f(S32x16 a) {
    return __builtin_tachy_vcvtsp_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_d2l(F64x2 a) {
    return __builtin_tachy_vcvtsr_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_d2l(F64x4 a) {
    return __builtin_tachy_vcvtsr_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_d2l(F64x8 a) {
    return __builtin_tachy_vcvtsr_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_d2w(F64x4 a) {
    return __builtin_tachy_vcvtsr_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_d2w(F64x8 a) {
    return __builtin_tachy_vcvtsr_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_f2l(F32x4 a) {
    return __builtin_tachy_vcvtsr_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_f2l(F32x8 a) {
    return __builtin_tachy_vcvtsr_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_f2w(F32x4 a) {
    return __builtin_tachy_vcvtsr_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_f2w(F32x8 a) {
    return __builtin_tachy_vcvtsr_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_f2w(F32x16 a) {
    return __builtin_tachy_vcvtsr_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_l2d(S64x2 a) {
    return __builtin_tachy_vcvtsr_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_l2d(S64x4 a) {
    return __builtin_tachy_vcvtsr_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_l2d(S64x8 a) {
    return __builtin_tachy_vcvtsr_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_l2f(S64x4 a) {
    return __builtin_tachy_vcvtsr_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_l2f(S64x8 a) {
    return __builtin_tachy_vcvtsr_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_w2f(S32x4 a) {
    return __builtin_tachy_vcvtsr_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_w2f(S32x8 a) {
    return __builtin_tachy_vcvtsr_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsr_w2f(S32x16 a) {
    return __builtin_tachy_vcvtsr_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_d2l(F64x2 a) {
    return __builtin_tachy_vcvtst_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_d2l(F64x4 a) {
    return __builtin_tachy_vcvtst_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_d2l(F64x8 a) {
    return __builtin_tachy_vcvtst_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_d2w(F64x4 a) {
    return __builtin_tachy_vcvtst_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_d2w(F64x8 a) {
    return __builtin_tachy_vcvtst_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_f2l(F32x4 a) {
    return __builtin_tachy_vcvtst_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_f2l(F32x8 a) {
    return __builtin_tachy_vcvtst_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_f2w(F32x4 a) {
    return __builtin_tachy_vcvtst_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_f2w(F32x8 a) {
    return __builtin_tachy_vcvtst_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_f2w(F32x16 a) {
    return __builtin_tachy_vcvtst_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_l2d(S64x2 a) {
    return __builtin_tachy_vcvtst_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_l2d(S64x4 a) {
    return __builtin_tachy_vcvtst_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_l2d(S64x8 a) {
    return __builtin_tachy_vcvtst_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_l2f(S64x4 a) {
    return __builtin_tachy_vcvtst_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_l2f(S64x8 a) {
    return __builtin_tachy_vcvtst_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_w2f(S32x4 a) {
    return __builtin_tachy_vcvtst_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_w2f(S32x8 a) {
    return __builtin_tachy_vcvtst_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtst_w2f(S32x16 a) {
    return __builtin_tachy_vcvtst_w2f_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_d2l(F64x2 a) {
    return __builtin_tachy_vcvtsx_d2l_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_d2l(F64x4 a) {
    return __builtin_tachy_vcvtsx_d2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_d2l(F64x8 a) {
    return __builtin_tachy_vcvtsx_d2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_d2w(F64x4 a) {
    return __builtin_tachy_vcvtsx_d2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_d2w(F64x8 a) {
    return __builtin_tachy_vcvtsx_d2w_256(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_f2l(F32x4 a) {
    return __builtin_tachy_vcvtsx_f2l_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_f2l(F32x8 a) {
    return __builtin_tachy_vcvtsx_f2l_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_f2w(F32x4 a) {
    return __builtin_tachy_vcvtsx_f2w_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_f2w(F32x8 a) {
    return __builtin_tachy_vcvtsx_f2w_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_f2w(F32x16 a) {
    return __builtin_tachy_vcvtsx_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_l2d(S64x2 a) {
    return __builtin_tachy_vcvtsx_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_l2d(S64x4 a) {
    return __builtin_tachy_vcvtsx_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_l2d(S64x8 a) {
    return __builtin_tachy_vcvtsx_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_l2f(S64x4 a) {
    return __builtin_tachy_vcvtsx_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_l2f(S64x8 a) {
    return __builtin_tachy_vcvtsx_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_w2f(S32x4 a) {
    return __builtin_tachy_vcvtsx_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_w2f(S32x8 a) {
    return __builtin_tachy_vcvtsx_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtsx_w2f(S32x16 a) {
    return __builtin_tachy_vcvtsx_w2f_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2f(F64x4 a) {
    return __builtin_tachy_vcvtt_d2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2f(F64x8 a) {
    return __builtin_tachy_vcvtt_d2f_256(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2l(F64x2 a) {
    return __builtin_tachy_vcvtt_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2l(F64x4 a) {
    return __builtin_tachy_vcvtt_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2l(F64x8 a) {
    return __builtin_tachy_vcvtt_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2w(F64x4 a) {
    return __builtin_tachy_vcvtt_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_d2w(F64x8 a) {
    return __builtin_tachy_vcvtt_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_f2l(F32x4 a) {
    return __builtin_tachy_vcvtt_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_f2l(F32x8 a) {
    return __builtin_tachy_vcvtt_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_f2w(F32x4 a) {
    return __builtin_tachy_vcvtt_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_f2w(F32x8 a) {
    return __builtin_tachy_vcvtt_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_f2w(F32x16 a) {
    return __builtin_tachy_vcvtt_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_l2d(S64x2 a) {
    return __builtin_tachy_vcvtt_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_l2d(S64x4 a) {
    return __builtin_tachy_vcvtt_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_l2d(S64x8 a) {
    return __builtin_tachy_vcvtt_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_l2f(S64x4 a) {
    return __builtin_tachy_vcvtt_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_l2f(S64x8 a) {
    return __builtin_tachy_vcvtt_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_w2f(S32x4 a) {
    return __builtin_tachy_vcvtt_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_w2f(S32x8 a) {
    return __builtin_tachy_vcvtt_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtt_w2f(S32x16 a) {
    return __builtin_tachy_vcvtt_w2f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_d2l(F64x2 a) {
    return __builtin_tachy_vcvtx_d2l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_d2l(F64x4 a) {
    return __builtin_tachy_vcvtx_d2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_d2l(F64x8 a) {
    return __builtin_tachy_vcvtx_d2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_d2w(F64x4 a) {
    return __builtin_tachy_vcvtx_d2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_d2w(F64x8 a) {
    return __builtin_tachy_vcvtx_d2w_256(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_f2l(F32x4 a) {
    return __builtin_tachy_vcvtx_f2l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_f2l(F32x8 a) {
    return __builtin_tachy_vcvtx_f2l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_f2w(F32x4 a) {
    return __builtin_tachy_vcvtx_f2w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_f2w(F32x8 a) {
    return __builtin_tachy_vcvtx_f2w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_f2w(F32x16 a) {
    return __builtin_tachy_vcvtx_f2w_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_l2d(S64x2 a) {
    return __builtin_tachy_vcvtx_l2d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_l2d(S64x4 a) {
    return __builtin_tachy_vcvtx_l2d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_l2d(S64x8 a) {
    return __builtin_tachy_vcvtx_l2d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_l2f(S64x4 a) {
    return __builtin_tachy_vcvtx_l2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_l2f(S64x8 a) {
    return __builtin_tachy_vcvtx_l2f_256(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_w2f(S32x4 a) {
    return __builtin_tachy_vcvtx_w2f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_w2f(S32x8 a) {
    return __builtin_tachy_vcvtx_w2f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcvtx_w2f(S32x16 a) {
    return __builtin_tachy_vcvtx_w2f_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(U8x64 a, U8x64 b) {
    return __builtin_tachy_vdiv1_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(U16x32 a, U16x32 b) {
    return __builtin_tachy_vdiv2_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(U32x16 a, U32x16 b) {
    return __builtin_tachy_vdiv4_512(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(U64x8 a, U64x8 b) {
    return __builtin_tachy_vdiv8_512(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(F64x8 a, F64x8 b) {
    return __builtin_tachy_vdiv_d_512(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdiv(F32x16 a, F32x16 b) {
    return __builtin_tachy_vdiv_f_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivs(S8x64 a, S8x64 b) {
    return __builtin_tachy_vdivs1_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivs(S16x32 a, S16x32 b) {
    return __builtin_tachy_vdivs2_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivs(S32x16 a, S32x16 b) {
    return __builtin_tachy_vdivs4_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivs(S64x8 a, S64x8 b) {
    return __builtin_tachy_vdivs8_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S8x16 a, S8x16 b) {
    return __builtin_tachy_vdivsz1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S8x32 a, S8x32 b) {
    return __builtin_tachy_vdivsz1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S8x64 a, S8x64 b) {
    return __builtin_tachy_vdivsz1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S16x8 a, S16x8 b) {
    return __builtin_tachy_vdivsz2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S16x16 a, S16x16 b) {
    return __builtin_tachy_vdivsz2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S16x32 a, S16x32 b) {
    return __builtin_tachy_vdivsz2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S32x4 a, S32x4 b) {
    return __builtin_tachy_vdivsz4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S32x8 a, S32x8 b) {
    return __builtin_tachy_vdivsz4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S32x16 a, S32x16 b) {
    return __builtin_tachy_vdivsz4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S64x2 a, S64x2 b) {
    return __builtin_tachy_vdivsz8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S64x4 a, S64x4 b) {
    return __builtin_tachy_vdivsz8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivsz(S64x8 a, S64x8 b) {
    return __builtin_tachy_vdivsz8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U8x16 a, U8x16 b) {
    return __builtin_tachy_vdivz1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U8x32 a, U8x32 b) {
    return __builtin_tachy_vdivz1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U8x64 a, U8x64 b) {
    return __builtin_tachy_vdivz1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U16x8 a, U16x8 b) {
    return __builtin_tachy_vdivz2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U16x16 a, U16x16 b) {
    return __builtin_tachy_vdivz2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U16x32 a, U16x32 b) {
    return __builtin_tachy_vdivz2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U32x4 a, U32x4 b) {
    return __builtin_tachy_vdivz4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U32x8 a, U32x8 b) {
    return __builtin_tachy_vdivz4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U32x16 a, U32x16 b) {
    return __builtin_tachy_vdivz4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U64x2 a, U64x2 b) {
    return __builtin_tachy_vdivz8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U64x4 a, U64x4 b) {
    return __builtin_tachy_vdivz8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vdivz(U64x8 a, U64x8 b) {
    return __builtin_tachy_vdivz8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U8x16 a, U8x16 b) {
    return __builtin_tachy_veq1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U8x32 a, U8x32 b) {
    return __builtin_tachy_veq1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U8x64 a, U8x64 b) {
    return __builtin_tachy_veq1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U16x8 a, U16x8 b) {
    return __builtin_tachy_veq2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U16x16 a, U16x16 b) {
    return __builtin_tachy_veq2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U16x32 a, U16x32 b) {
    return __builtin_tachy_veq2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U32x4 a, U32x4 b) {
    return __builtin_tachy_veq4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U32x8 a, U32x8 b) {
    return __builtin_tachy_veq4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U32x16 a, U32x16 b) {
    return __builtin_tachy_veq4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U64x2 a, U64x2 b) {
    return __builtin_tachy_veq8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U64x4 a, U64x4 b) {
    return __builtin_tachy_veq8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(U64x8 a, U64x8 b) {
    return __builtin_tachy_veq8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F64x2 a, F64x2 b) {
    return __builtin_tachy_veq_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F64x4 a, F64x4 b) {
    return __builtin_tachy_veq_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F64x8 a, F64x8 b) {
    return __builtin_tachy_veq_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F32x4 a, F32x4 b) {
    return __builtin_tachy_veq_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F32x8 a, F32x8 b) {
    return __builtin_tachy_veq_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq(F32x16 a, F32x16 b) {
    return __builtin_tachy_veq_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_veq_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_veq_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_veq_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_veq_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_veq_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
veq_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_veq_u_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F64x2 a) {
    return __builtin_tachy_verf_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F64x4 a) {
    return __builtin_tachy_verf_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F64x8 a) {
    return __builtin_tachy_verf_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F32x4 a) {
    return __builtin_tachy_verf_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F32x8 a) {
    return __builtin_tachy_verf_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
verf(F32x16 a) {
    return __builtin_tachy_verf_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F64x2 a) {
    return __builtin_tachy_vexp2_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F64x4 a) {
    return __builtin_tachy_vexp2_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F64x8 a) {
    return __builtin_tachy_vexp2_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F32x4 a) {
    return __builtin_tachy_vexp2_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F32x8 a) {
    return __builtin_tachy_vexp2_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexp2(F32x16 a) {
    return __builtin_tachy_vexp2_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F64x2 a) {
    return __builtin_tachy_vexpe_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F64x4 a) {
    return __builtin_tachy_vexpe_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F64x8 a) {
    return __builtin_tachy_vexpe_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F32x4 a) {
    return __builtin_tachy_vexpe_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F32x8 a) {
    return __builtin_tachy_vexpe_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vexpe(F32x16 a) {
    return __builtin_tachy_vexpe_f_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U8x16 a) {
    return __builtin_tachy_vfill1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U8x32 a) {
    return __builtin_tachy_vfill1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U8x64 a) {
    return __builtin_tachy_vfill1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U16x8 a) {
    return __builtin_tachy_vfill2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U16x16 a) {
    return __builtin_tachy_vfill2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U16x32 a) {
    return __builtin_tachy_vfill2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U32x4 a) {
    return __builtin_tachy_vfill4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U32x8 a) {
    return __builtin_tachy_vfill4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U32x16 a) {
    return __builtin_tachy_vfill4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U64x2 a) {
    return __builtin_tachy_vfill8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U64x4 a) {
    return __builtin_tachy_vfill8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vfill(U64x8 a) {
    return __builtin_tachy_vfill8_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx128_fill_b(uint8_t a) {
    return __builtin_tachy_vfillr1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx256_fill_b(uint8_t a) {
    return __builtin_tachy_vfillr1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx512_fill_b(uint8_t a) {
    return __builtin_tachy_vfillr1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx128_fill_h(uint16_t a) {
    return __builtin_tachy_vfillr2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx256_fill_h(uint16_t a) {
    return __builtin_tachy_vfillr2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx512_fill_h(uint16_t a) {
    return __builtin_tachy_vfillr2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx128_fill_w(uint32_t a) {
    return __builtin_tachy_vfillr4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx256_fill_w(uint32_t a) {
    return __builtin_tachy_vfillr4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx512_fill_w(uint32_t a) {
    return __builtin_tachy_vfillr4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx128_fill_l(uint64_t a) {
    return __builtin_tachy_vfillr8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx256_fill_l(uint64_t a) {
    return __builtin_tachy_vfillr8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
__vx512_fill_l(uint64_t a) {
    return __builtin_tachy_vfillr8_512(a);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S8x16 a, S8x16 b) {
    return __builtin_tachy_vge1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S8x32 a, S8x32 b) {
    return __builtin_tachy_vge1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S8x64 a, S8x64 b) {
    return __builtin_tachy_vge1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S16x8 a, S16x8 b) {
    return __builtin_tachy_vge2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S16x16 a, S16x16 b) {
    return __builtin_tachy_vge2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S16x32 a, S16x32 b) {
    return __builtin_tachy_vge2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S32x4 a, S32x4 b) {
    return __builtin_tachy_vge4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S32x8 a, S32x8 b) {
    return __builtin_tachy_vge4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S32x16 a, S32x16 b) {
    return __builtin_tachy_vge4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S64x2 a, S64x2 b) {
    return __builtin_tachy_vge8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S64x4 a, S64x4 b) {
    return __builtin_tachy_vge8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(S64x8 a, S64x8 b) {
    return __builtin_tachy_vge8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F64x2 a, F64x2 b) {
    return __builtin_tachy_vge_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F64x4 a, F64x4 b) {
    return __builtin_tachy_vge_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F64x8 a, F64x8 b) {
    return __builtin_tachy_vge_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F32x4 a, F32x4 b) {
    return __builtin_tachy_vge_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F32x8 a, F32x8 b) {
    return __builtin_tachy_vge_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge(F32x16 a, F32x16 b) {
    return __builtin_tachy_vge_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_vge_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_vge_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_vge_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_vge_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_vge_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vge_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_vge_u_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F64x2 a, F64x2 b) {
    return __builtin_tachy_vgt_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F64x4 a, F64x4 b) {
    return __builtin_tachy_vgt_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F64x8 a, F64x8 b) {
    return __builtin_tachy_vgt_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F32x4 a, F32x4 b) {
    return __builtin_tachy_vgt_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F32x8 a, F32x8 b) {
    return __builtin_tachy_vgt_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vgt(F32x16 a, F32x16 b) {
    return __builtin_tachy_vgt_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U8x16 a, U8x16 b) {
    return __builtin_tachy_vhs1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U8x32 a, U8x32 b) {
    return __builtin_tachy_vhs1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U8x64 a, U8x64 b) {
    return __builtin_tachy_vhs1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U16x8 a, U16x8 b) {
    return __builtin_tachy_vhs2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U16x16 a, U16x16 b) {
    return __builtin_tachy_vhs2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U16x32 a, U16x32 b) {
    return __builtin_tachy_vhs2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U32x4 a, U32x4 b) {
    return __builtin_tachy_vhs4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U32x8 a, U32x8 b) {
    return __builtin_tachy_vhs4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U32x16 a, U32x16 b) {
    return __builtin_tachy_vhs4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U64x2 a, U64x2 b) {
    return __builtin_tachy_vhs8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U64x4 a, U64x4 b) {
    return __builtin_tachy_vhs8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vhs(U64x8 a, U64x8 b) {
    return __builtin_tachy_vhs8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x16 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vins_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x32 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vins_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x64 a, U8x64 b, unsigned int c) {
    return __builtin_tachy_vins_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x8 a, U16x8 b, unsigned int c) {
    return __builtin_tachy_vins_h_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x16 a, U16x16 b, unsigned int c) {
    return __builtin_tachy_vins_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x32 a, U16x32 b, unsigned int c) {
    return __builtin_tachy_vins_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x2 a, U64x2 b, unsigned int c) {
    return __builtin_tachy_vins_l_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x4 a, U64x4 b, unsigned int c) {
    return __builtin_tachy_vins_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x8 a, U64x8 b, unsigned int c) {
    return __builtin_tachy_vins_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x16 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x32 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U8x64 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x8 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_h_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x16 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U16x32 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x2 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_l_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x4 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U64x8 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x4 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_w_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x8 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x16 a, unsigned int b, unsigned int c) {
    return __builtin_tachy_vins_r_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x4 a, U32x4 b, unsigned int c) {
    return __builtin_tachy_vins_w_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x8 a, U32x8 b, unsigned int c) {
    return __builtin_tachy_vins_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vins(U32x16 a, U32x16 b, unsigned int c) {
    return __builtin_tachy_vins_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U8x32 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U8x64 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U16x16 a, U16x8 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_h_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U16x32 a, U16x8 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U64x4 a, U64x2 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_l_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U64x8 a, U64x2 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U32x8 a, U32x4 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_w_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsx(U32x16 a, U32x4 b, unsigned int c) {
    return __builtin_tachy_vinsx_x_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsxx(U8x64 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vinsxx_x_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsxx(U16x32 a, U16x16 b, unsigned int c) {
    return __builtin_tachy_vinsxx_x_h_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsxx(U64x8 a, U64x4 b, unsigned int c) {
    return __builtin_tachy_vinsxx_x_l_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vinsxx(U32x16 a, U32x8 b, unsigned int c) {
    return __builtin_tachy_vinsxx_x_w_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F64x2 a, F64x2 b) {
    return __builtin_tachy_vle_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F64x4 a, F64x4 b) {
    return __builtin_tachy_vle_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F64x8 a, F64x8 b) {
    return __builtin_tachy_vle_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F32x4 a, F32x4 b) {
    return __builtin_tachy_vle_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F32x8 a, F32x8 b) {
    return __builtin_tachy_vle_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vle(F32x16 a, F32x16 b) {
    return __builtin_tachy_vle_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U8x16 a, U8x16 b) {
    return __builtin_tachy_vlo1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U8x32 a, U8x32 b) {
    return __builtin_tachy_vlo1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U8x64 a, U8x64 b) {
    return __builtin_tachy_vlo1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U16x8 a, U16x8 b) {
    return __builtin_tachy_vlo2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U16x16 a, U16x16 b) {
    return __builtin_tachy_vlo2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U16x32 a, U16x32 b) {
    return __builtin_tachy_vlo2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U32x4 a, U32x4 b) {
    return __builtin_tachy_vlo4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U32x8 a, U32x8 b) {
    return __builtin_tachy_vlo4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U32x16 a, U32x16 b) {
    return __builtin_tachy_vlo4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U64x2 a, U64x2 b) {
    return __builtin_tachy_vlo8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U64x4 a, U64x4 b) {
    return __builtin_tachy_vlo8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlo(U64x8 a, U64x8 b) {
    return __builtin_tachy_vlo8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F64x2 a) {
    return __builtin_tachy_vlog2_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F64x4 a) {
    return __builtin_tachy_vlog2_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F64x8 a) {
    return __builtin_tachy_vlog2_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F32x4 a) {
    return __builtin_tachy_vlog2_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F32x8 a) {
    return __builtin_tachy_vlog2_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlog2(F32x16 a) {
    return __builtin_tachy_vlog2_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F64x2 a) {
    return __builtin_tachy_vloge_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F64x4 a) {
    return __builtin_tachy_vloge_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F64x8 a) {
    return __builtin_tachy_vloge_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F32x4 a) {
    return __builtin_tachy_vloge_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F32x8 a) {
    return __builtin_tachy_vloge_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vloge(F32x16 a) {
    return __builtin_tachy_vloge_f_512(a);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S8x16 a, S8x16 b) {
    return __builtin_tachy_vlt1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S8x32 a, S8x32 b) {
    return __builtin_tachy_vlt1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S8x64 a, S8x64 b) {
    return __builtin_tachy_vlt1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S16x8 a, S16x8 b) {
    return __builtin_tachy_vlt2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S16x16 a, S16x16 b) {
    return __builtin_tachy_vlt2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S16x32 a, S16x32 b) {
    return __builtin_tachy_vlt2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S32x4 a, S32x4 b) {
    return __builtin_tachy_vlt4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S32x8 a, S32x8 b) {
    return __builtin_tachy_vlt4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S32x16 a, S32x16 b) {
    return __builtin_tachy_vlt4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S64x2 a, S64x2 b) {
    return __builtin_tachy_vlt8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S64x4 a, S64x4 b) {
    return __builtin_tachy_vlt8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(S64x8 a, S64x8 b) {
    return __builtin_tachy_vlt8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F64x2 a, F64x2 b) {
    return __builtin_tachy_vlt_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F64x4 a, F64x4 b) {
    return __builtin_tachy_vlt_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F64x8 a, F64x8 b) {
    return __builtin_tachy_vlt_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F32x4 a, F32x4 b) {
    return __builtin_tachy_vlt_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F32x8 a, F32x8 b) {
    return __builtin_tachy_vlt_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt(F32x16 a, F32x16 b) {
    return __builtin_tachy_vlt_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_vlt_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_vlt_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_vlt_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_vlt_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_vlt_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vlt_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_vlt_u_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmadd1_128(a, b, c);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S8x16 a, S8x16 b, S8x16 c) {
    return (S8x16) __builtin_tachy_vmadd1_128((__u8x16) a, (__u8x16) b, (__u8x16) c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmadd1_256(a, b, c);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S8x32 a, S8x32 b, S8x32 c) {
    return (S8x32) __builtin_tachy_vmadd1_256((__u8x32) a, (__u8x32) b, (__u8x32) c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmadd1_512(a, b, c);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S8x64 a, S8x64 b, S8x64 c) {
    return (S8x64) __builtin_tachy_vmadd1_512((__u8x64) a, (__u8x64) b, (__u8x64) c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmadd2_128(a, b, c);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S16x8 a, S16x8 b, S16x8 c) {
    return (S16x8) __builtin_tachy_vmadd2_128((__u16x8) a, (__u16x8) b, (__u16x8) c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmadd2_256(a, b, c);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S16x16 a, S16x16 b, S16x16 c) {
    return (S16x16) __builtin_tachy_vmadd2_256((__u16x16) a, (__u16x16) b, (__u16x16) c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmadd2_512(a, b, c);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S16x32 a, S16x32 b, S16x32 c) {
    return (S16x32) __builtin_tachy_vmadd2_512((__u16x32) a, (__u16x32) b, (__u16x32) c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmadd4_128(a, b, c);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S32x4 a, S32x4 b, S32x4 c) {
    return (S32x4) __builtin_tachy_vmadd4_128((__u32x4) a, (__u32x4) b, (__u32x4) c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmadd4_256(a, b, c);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S32x8 a, S32x8 b, S32x8 c) {
    return (S32x8) __builtin_tachy_vmadd4_256((__u32x8) a, (__u32x8) b, (__u32x8) c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmadd4_512(a, b, c);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S32x16 a, S32x16 b, S32x16 c) {
    return (S32x16) __builtin_tachy_vmadd4_512((__u32x16) a, (__u32x16) b, (__u32x16) c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmadd8_128(a, b, c);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S64x2 a, S64x2 b, S64x2 c) {
    return (S64x2) __builtin_tachy_vmadd8_128((__u64x2) a, (__u64x2) b, (__u64x2) c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmadd8_256(a, b, c);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S64x4 a, S64x4 b, S64x4 c) {
    return (S64x4) __builtin_tachy_vmadd8_256((__u64x4) a, (__u64x4) b, (__u64x4) c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmadd8_512(a, b, c);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(S64x8 a, S64x8 b, S64x8 c) {
    return (S64x8) __builtin_tachy_vmadd8_512((__u64x8) a, (__u64x8) b, (__u64x8) c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmadd_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmadd_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmadd_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmadd_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmadd_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadd(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmadd_f_512(a, b, c);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S8x16 a, S8x16 b, S8x16 c) {
    return __builtin_tachy_vmadds1_128(a, b, c);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S8x32 a, S8x32 b, S8x32 c) {
    return __builtin_tachy_vmadds1_256(a, b, c);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S8x64 a, S8x64 b, S8x64 c) {
    return __builtin_tachy_vmadds1_512(a, b, c);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S16x8 a, S16x8 b, S16x8 c) {
    return __builtin_tachy_vmadds2_128(a, b, c);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S16x16 a, S16x16 b, S16x16 c) {
    return __builtin_tachy_vmadds2_256(a, b, c);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S16x32 a, S16x32 b, S16x32 c) {
    return __builtin_tachy_vmadds2_512(a, b, c);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S32x4 a, S32x4 b, S32x4 c) {
    return __builtin_tachy_vmadds4_128(a, b, c);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S32x8 a, S32x8 b, S32x8 c) {
    return __builtin_tachy_vmadds4_256(a, b, c);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S32x16 a, S32x16 b, S32x16 c) {
    return __builtin_tachy_vmadds4_512(a, b, c);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S64x2 a, S64x2 b, S64x2 c) {
    return __builtin_tachy_vmadds8_128(a, b, c);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S64x4 a, S64x4 b, S64x4 c) {
    return __builtin_tachy_vmadds8_256(a, b, c);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmadds(S64x8 a, S64x8 b, S64x8 c) {
    return __builtin_tachy_vmadds8_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmaddu1_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmaddu1_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmaddu1_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmaddu2_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmaddu2_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmaddu2_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmaddu4_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmaddu4_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmaddu4_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmaddu8_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmaddu8_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaddu(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmaddu8_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmas_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmas_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmas_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmas_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmas_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmas(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmas_f_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmax1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmax1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmax1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmax2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmax2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmax2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmax4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmax4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmax4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmax8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmax8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmax8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F64x2 a, F64x2 b) {
    return __builtin_tachy_vmax_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F64x4 a, F64x4 b) {
    return __builtin_tachy_vmax_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F64x8 a, F64x8 b) {
    return __builtin_tachy_vmax_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F32x4 a, F32x4 b) {
    return __builtin_tachy_vmax_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F32x8 a, F32x8 b) {
    return __builtin_tachy_vmax_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmax(F32x16 a, F32x16 b) {
    return __builtin_tachy_vmax_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F64x2 a, F64x2 b) {
    return __builtin_tachy_vmaxn_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F64x4 a, F64x4 b) {
    return __builtin_tachy_vmaxn_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F64x8 a, F64x8 b) {
    return __builtin_tachy_vmaxn_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F32x4 a, F32x4 b) {
    return __builtin_tachy_vmaxn_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F32x8 a, F32x8 b) {
    return __builtin_tachy_vmaxn_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxn(F32x16 a, F32x16 b) {
    return __builtin_tachy_vmaxn_f_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmaxp_b_512(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F64x8 a, F64x8 b) {
    return __builtin_tachy_vmaxp_d_512(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F32x16 a, F32x16 b) {
    return __builtin_tachy_vmaxp_f_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmaxp_h_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U8x16 a) {
    return __builtin_tachy_vmaxp_k_b_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U8x32 a) {
    return __builtin_tachy_vmaxp_k_b_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U8x64 a) {
    return __builtin_tachy_vmaxp_k_b_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F64x2 a) {
    return __builtin_tachy_vmaxp_k_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F64x4 a) {
    return __builtin_tachy_vmaxp_k_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F64x8 a) {
    return __builtin_tachy_vmaxp_k_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F32x4 a) {
    return __builtin_tachy_vmaxp_k_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F32x8 a) {
    return __builtin_tachy_vmaxp_k_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(F32x16 a) {
    return __builtin_tachy_vmaxp_k_f_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U16x8 a) {
    return __builtin_tachy_vmaxp_k_h_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U16x16 a) {
    return __builtin_tachy_vmaxp_k_h_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U16x32 a) {
    return __builtin_tachy_vmaxp_k_h_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U64x2 a) {
    return __builtin_tachy_vmaxp_k_l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U64x4 a) {
    return __builtin_tachy_vmaxp_k_l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U64x8 a) {
    return __builtin_tachy_vmaxp_k_l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U32x4 a) {
    return __builtin_tachy_vmaxp_k_w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U32x8 a) {
    return __builtin_tachy_vmaxp_k_w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U32x16 a) {
    return __builtin_tachy_vmaxp_k_w_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmaxp_l_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxp(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmaxp_w_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S8x16 a, S8x16 b) {
    return __builtin_tachy_vmaxs1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S8x32 a, S8x32 b) {
    return __builtin_tachy_vmaxs1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S8x64 a, S8x64 b) {
    return __builtin_tachy_vmaxs1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S16x8 a, S16x8 b) {
    return __builtin_tachy_vmaxs2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S16x16 a, S16x16 b) {
    return __builtin_tachy_vmaxs2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S16x32 a, S16x32 b) {
    return __builtin_tachy_vmaxs2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S32x4 a, S32x4 b) {
    return __builtin_tachy_vmaxs4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S32x8 a, S32x8 b) {
    return __builtin_tachy_vmaxs4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S32x16 a, S32x16 b) {
    return __builtin_tachy_vmaxs4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S64x2 a, S64x2 b) {
    return __builtin_tachy_vmaxs8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S64x4 a, S64x4 b) {
    return __builtin_tachy_vmaxs8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxs(S64x8 a, S64x8 b) {
    return __builtin_tachy_vmaxs8_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmaxsp_b_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmaxsp_h_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U8x16 a) {
    return __builtin_tachy_vmaxsp_k_b_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U8x32 a) {
    return __builtin_tachy_vmaxsp_k_b_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U8x64 a) {
    return __builtin_tachy_vmaxsp_k_b_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U16x8 a) {
    return __builtin_tachy_vmaxsp_k_h_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U16x16 a) {
    return __builtin_tachy_vmaxsp_k_h_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U16x32 a) {
    return __builtin_tachy_vmaxsp_k_h_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U64x2 a) {
    return __builtin_tachy_vmaxsp_k_l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U64x4 a) {
    return __builtin_tachy_vmaxsp_k_l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U64x8 a) {
    return __builtin_tachy_vmaxsp_k_l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U32x4 a) {
    return __builtin_tachy_vmaxsp_k_w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U32x8 a) {
    return __builtin_tachy_vmaxsp_k_w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U32x16 a) {
    return __builtin_tachy_vmaxsp_k_w_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmaxsp_l_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmaxsp(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmaxsp_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmeq_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmeq_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmeq_b_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmeq_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmeq_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmeq_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmeq_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmeq_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmeq_f_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmeq_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmeq_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmeq_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmeq_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmeq_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmeq_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmeq_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmeq_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmeq(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmeq_w_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmge_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmge_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmge_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmge_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmge_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmge(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmge_f_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmgt_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmgt_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmgt_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmgt_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmgt_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmgt(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmgt_f_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmin1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmin1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmin1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmin2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmin2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmin2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmin4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmin4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmin4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmin8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmin8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmin8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F64x2 a, F64x2 b) {
    return __builtin_tachy_vmin_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F64x4 a, F64x4 b) {
    return __builtin_tachy_vmin_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F64x8 a, F64x8 b) {
    return __builtin_tachy_vmin_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F32x4 a, F32x4 b) {
    return __builtin_tachy_vmin_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F32x8 a, F32x8 b) {
    return __builtin_tachy_vmin_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmin(F32x16 a, F32x16 b) {
    return __builtin_tachy_vmin_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F64x2 a, F64x2 b) {
    return __builtin_tachy_vminn_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F64x4 a, F64x4 b) {
    return __builtin_tachy_vminn_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F64x8 a, F64x8 b) {
    return __builtin_tachy_vminn_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F32x4 a, F32x4 b) {
    return __builtin_tachy_vminn_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F32x8 a, F32x8 b) {
    return __builtin_tachy_vminn_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminn(F32x16 a, F32x16 b) {
    return __builtin_tachy_vminn_f_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U8x64 a, U8x64 b) {
    return __builtin_tachy_vminp_b_512(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F64x8 a, F64x8 b) {
    return __builtin_tachy_vminp_d_512(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F32x16 a, F32x16 b) {
    return __builtin_tachy_vminp_f_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U16x32 a, U16x32 b) {
    return __builtin_tachy_vminp_h_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U8x16 a) {
    return __builtin_tachy_vminp_k_b_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U8x32 a) {
    return __builtin_tachy_vminp_k_b_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U8x64 a) {
    return __builtin_tachy_vminp_k_b_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F64x2 a) {
    return __builtin_tachy_vminp_k_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F64x4 a) {
    return __builtin_tachy_vminp_k_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F64x8 a) {
    return __builtin_tachy_vminp_k_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F32x4 a) {
    return __builtin_tachy_vminp_k_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F32x8 a) {
    return __builtin_tachy_vminp_k_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(F32x16 a) {
    return __builtin_tachy_vminp_k_f_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U16x8 a) {
    return __builtin_tachy_vminp_k_h_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U16x16 a) {
    return __builtin_tachy_vminp_k_h_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U16x32 a) {
    return __builtin_tachy_vminp_k_h_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U64x2 a) {
    return __builtin_tachy_vminp_k_l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U64x4 a) {
    return __builtin_tachy_vminp_k_l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U64x8 a) {
    return __builtin_tachy_vminp_k_l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U32x4 a) {
    return __builtin_tachy_vminp_k_w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U32x8 a) {
    return __builtin_tachy_vminp_k_w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U32x16 a) {
    return __builtin_tachy_vminp_k_w_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U64x8 a, U64x8 b) {
    return __builtin_tachy_vminp_l_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminp(U32x16 a, U32x16 b) {
    return __builtin_tachy_vminp_w_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S8x16 a, S8x16 b) {
    return __builtin_tachy_vmins1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S8x32 a, S8x32 b) {
    return __builtin_tachy_vmins1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S8x64 a, S8x64 b) {
    return __builtin_tachy_vmins1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S16x8 a, S16x8 b) {
    return __builtin_tachy_vmins2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S16x16 a, S16x16 b) {
    return __builtin_tachy_vmins2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S16x32 a, S16x32 b) {
    return __builtin_tachy_vmins2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S32x4 a, S32x4 b) {
    return __builtin_tachy_vmins4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S32x8 a, S32x8 b) {
    return __builtin_tachy_vmins4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S32x16 a, S32x16 b) {
    return __builtin_tachy_vmins4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S64x2 a, S64x2 b) {
    return __builtin_tachy_vmins8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S64x4 a, S64x4 b) {
    return __builtin_tachy_vmins8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmins(S64x8 a, S64x8 b) {
    return __builtin_tachy_vmins8_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U8x64 a, U8x64 b) {
    return __builtin_tachy_vminsp_b_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U16x32 a, U16x32 b) {
    return __builtin_tachy_vminsp_h_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U8x16 a) {
    return __builtin_tachy_vminsp_k_b_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U8x32 a) {
    return __builtin_tachy_vminsp_k_b_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U8x64 a) {
    return __builtin_tachy_vminsp_k_b_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U16x8 a) {
    return __builtin_tachy_vminsp_k_h_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U16x16 a) {
    return __builtin_tachy_vminsp_k_h_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U16x32 a) {
    return __builtin_tachy_vminsp_k_h_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U64x2 a) {
    return __builtin_tachy_vminsp_k_l_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U64x4 a) {
    return __builtin_tachy_vminsp_k_l_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U64x8 a) {
    return __builtin_tachy_vminsp_k_l_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U32x4 a) {
    return __builtin_tachy_vminsp_k_w_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U32x8 a) {
    return __builtin_tachy_vminsp_k_w_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U32x16 a) {
    return __builtin_tachy_vminsp_k_w_512(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U64x8 a, U64x8 b) {
    return __builtin_tachy_vminsp_l_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vminsp(U32x16 a, U32x16 b) {
    return __builtin_tachy_vminsp_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmle_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmle_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmle_b_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmle_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmle_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmle_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmle_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmle_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmle_f_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmle_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmle_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmle_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmle_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmle_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmle_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmle_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmle_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmle(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmle_w_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmlt_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmlt_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmlt_b_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmlt_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmlt_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmlt_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmlt_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmlt_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmlt_f_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmlt_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmlt_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmlt_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmlt_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmlt_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmlt_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmlt_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmlt_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmlt(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmlt_w_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmne_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmne_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmne_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmne_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmne_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmne(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmne_f_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U8x16 a) {
    return __builtin_tachy_vmov1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U8x32 a) {
    return __builtin_tachy_vmov1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U8x64 a) {
    return __builtin_tachy_vmov1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U16x8 a) {
    return __builtin_tachy_vmov2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U16x16 a) {
    return __builtin_tachy_vmov2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U16x32 a) {
    return __builtin_tachy_vmov2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U32x4 a) {
    return __builtin_tachy_vmov4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U32x8 a) {
    return __builtin_tachy_vmov4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U32x16 a) {
    return __builtin_tachy_vmov4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U64x2 a) {
    return __builtin_tachy_vmov8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U64x4 a) {
    return __builtin_tachy_vmov8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmov(U64x8 a) {
    return __builtin_tachy_vmov8_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmovh_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmovh_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmovh_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmovh_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmovh_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmovh_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmovh_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmovh_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmovh_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmovh_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmovh_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovh(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmovh_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmovhl_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmovhl_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmovhl_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmovhl_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmovhl_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmovhl_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmovhl_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmovhl_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmovhl_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmovhl_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmovhl_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovhl(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmovhl_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmovl_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmovl_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmovl_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmovl_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmovl_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmovl_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmovl_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmovl_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmovl_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmovl_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmovl_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovl(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmovl_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmovlh_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmovlh_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmovlh_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmovlh_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmovlh_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmovlh_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmovlh_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmovlh_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmovlh_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmovlh_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmovlh_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmovlh(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmovlh_w_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmsa_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmsa_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmsa_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmsa_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmsa_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsa(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmsa_f_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vmsub_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vmsub_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vmsub_b_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vmsub_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vmsub_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vmsub_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vmsub_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vmsub_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vmsub_f_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vmsub_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vmsub_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vmsub_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vmsub_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vmsub_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vmsub_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vmsub_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vmsub_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmsub(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vmsub_w_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmul1_128(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S8x16 a, S8x16 b) {
    return (S8x16) __builtin_tachy_vmul1_128((__u8x16) a, (__u8x16) b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmul1_256(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S8x32 a, S8x32 b) {
    return (S8x32) __builtin_tachy_vmul1_256((__u8x32) a, (__u8x32) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmul1_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vmul1_512((__u8x64) a, (__u8x64) b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmul2_128(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S16x8 a, S16x8 b) {
    return (S16x8) __builtin_tachy_vmul2_128((__u16x8) a, (__u16x8) b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmul2_256(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S16x16 a, S16x16 b) {
    return (S16x16) __builtin_tachy_vmul2_256((__u16x16) a, (__u16x16) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmul2_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vmul2_512((__u16x32) a, (__u16x32) b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmul4_128(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S32x4 a, S32x4 b) {
    return (S32x4) __builtin_tachy_vmul4_128((__u32x4) a, (__u32x4) b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmul4_256(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S32x8 a, S32x8 b) {
    return (S32x8) __builtin_tachy_vmul4_256((__u32x8) a, (__u32x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmul4_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vmul4_512((__u32x16) a, (__u32x16) b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmul8_128(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S64x2 a, S64x2 b) {
    return (S64x2) __builtin_tachy_vmul8_128((__u64x2) a, (__u64x2) b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmul8_256(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S64x4 a, S64x4 b) {
    return (S64x4) __builtin_tachy_vmul8_256((__u64x4) a, (__u64x4) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmul8_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vmul8_512((__u64x8) a, (__u64x8) b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F64x2 a, F64x2 b) {
    return __builtin_tachy_vmul_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F64x4 a, F64x4 b) {
    return __builtin_tachy_vmul_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F64x8 a, F64x8 b) {
    return __builtin_tachy_vmul_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F32x4 a, F32x4 b) {
    return __builtin_tachy_vmul_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F32x8 a, F32x8 b) {
    return __builtin_tachy_vmul_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmul(F32x16 a, F32x16 b) {
    return __builtin_tachy_vmul_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmulh1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmulh1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmulh1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmulh2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmulh2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmulh2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmulh4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmulh4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmulh4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmulh8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmulh8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulh(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmulh8_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S8x16 a, S8x16 b) {
    return __builtin_tachy_vmulhs1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S8x32 a, S8x32 b) {
    return __builtin_tachy_vmulhs1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S8x64 a, S8x64 b) {
    return __builtin_tachy_vmulhs1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S16x8 a, S16x8 b) {
    return __builtin_tachy_vmulhs2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S16x16 a, S16x16 b) {
    return __builtin_tachy_vmulhs2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S16x32 a, S16x32 b) {
    return __builtin_tachy_vmulhs2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S32x4 a, S32x4 b) {
    return __builtin_tachy_vmulhs4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S32x8 a, S32x8 b) {
    return __builtin_tachy_vmulhs4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S32x16 a, S32x16 b) {
    return __builtin_tachy_vmulhs4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S64x2 a, S64x2 b) {
    return __builtin_tachy_vmulhs8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S64x4 a, S64x4 b) {
    return __builtin_tachy_vmulhs8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulhs(S64x8 a, S64x8 b) {
    return __builtin_tachy_vmulhs8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmuls_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmuls_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmuls_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmuls_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmuls_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmuls_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmuls_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmuls_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmuls_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmuls_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmuls_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmuls(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmuls_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmulu_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmulu_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmulu_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmulu_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmulu_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmulu_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U64x2 a, U64x2 b) {
    return __builtin_tachy_vmulu_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U64x4 a, U64x4 b) {
    return __builtin_tachy_vmulu_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U64x8 a, U64x8 b) {
    return __builtin_tachy_vmulu_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmulu_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmulu_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulu(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmulu_w_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U8x16 a, U8x16 b) {
    return __builtin_tachy_vmulx2_1_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U8x32 a, U8x32 b) {
    return __builtin_tachy_vmulx2_1_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U8x64 a, U8x64 b) {
    return __builtin_tachy_vmulx2_1_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U16x8 a, U16x8 b) {
    return __builtin_tachy_vmulx4_2_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U16x16 a, U16x16 b) {
    return __builtin_tachy_vmulx4_2_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U16x32 a, U16x32 b) {
    return __builtin_tachy_vmulx4_2_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U32x4 a, U32x4 b) {
    return __builtin_tachy_vmulx8_4_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U32x8 a, U32x8 b) {
    return __builtin_tachy_vmulx8_4_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulx(U32x16 a, U32x16 b) {
    return __builtin_tachy_vmulx8_4_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S8x16 a, S8x16 b) {
    return __builtin_tachy_vmulxs2_1_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S8x32 a, S8x32 b) {
    return __builtin_tachy_vmulxs2_1_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S8x64 a, S8x64 b) {
    return __builtin_tachy_vmulxs2_1_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S16x8 a, S16x8 b) {
    return __builtin_tachy_vmulxs4_2_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S16x16 a, S16x16 b) {
    return __builtin_tachy_vmulxs4_2_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S16x32 a, S16x32 b) {
    return __builtin_tachy_vmulxs4_2_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S32x4 a, S32x4 b) {
    return __builtin_tachy_vmulxs8_4_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S32x8 a, S32x8 b) {
    return __builtin_tachy_vmulxs8_4_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vmulxs(S32x16 a, S32x16 b) {
    return __builtin_tachy_vmulxs8_4_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S8x16 a) {
    return __builtin_tachy_vnabs1_128(a);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S8x32 a) {
    return __builtin_tachy_vnabs1_256(a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S8x64 a) {
    return __builtin_tachy_vnabs1_512(a);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S16x8 a) {
    return __builtin_tachy_vnabs2_128(a);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S16x16 a) {
    return __builtin_tachy_vnabs2_256(a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S16x32 a) {
    return __builtin_tachy_vnabs2_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S32x4 a) {
    return __builtin_tachy_vnabs4_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S32x8 a) {
    return __builtin_tachy_vnabs4_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S32x16 a) {
    return __builtin_tachy_vnabs4_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S64x2 a) {
    return __builtin_tachy_vnabs8_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S64x4 a) {
    return __builtin_tachy_vnabs8_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(S64x8 a) {
    return __builtin_tachy_vnabs8_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F64x2 a) {
    return __builtin_tachy_vnabs_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F64x4 a) {
    return __builtin_tachy_vnabs_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F64x8 a) {
    return __builtin_tachy_vnabs_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F32x4 a) {
    return __builtin_tachy_vnabs_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F32x8 a) {
    return __builtin_tachy_vnabs_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnabs(F32x16 a) {
    return __builtin_tachy_vnabs_f_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U8x16 a, U8x16 b) {
    return __builtin_tachy_vne1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U8x32 a, U8x32 b) {
    return __builtin_tachy_vne1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U8x64 a, U8x64 b) {
    return __builtin_tachy_vne1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U16x8 a, U16x8 b) {
    return __builtin_tachy_vne2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U16x16 a, U16x16 b) {
    return __builtin_tachy_vne2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U16x32 a, U16x32 b) {
    return __builtin_tachy_vne2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U32x4 a, U32x4 b) {
    return __builtin_tachy_vne4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U32x8 a, U32x8 b) {
    return __builtin_tachy_vne4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U32x16 a, U32x16 b) {
    return __builtin_tachy_vne4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U64x2 a, U64x2 b) {
    return __builtin_tachy_vne8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U64x4 a, U64x4 b) {
    return __builtin_tachy_vne8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(U64x8 a, U64x8 b) {
    return __builtin_tachy_vne8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F64x2 a, F64x2 b) {
    return __builtin_tachy_vne_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F64x4 a, F64x4 b) {
    return __builtin_tachy_vne_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F64x8 a, F64x8 b) {
    return __builtin_tachy_vne_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F32x4 a, F32x4 b) {
    return __builtin_tachy_vne_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F32x8 a, F32x8 b) {
    return __builtin_tachy_vne_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne(F32x16 a, F32x16 b) {
    return __builtin_tachy_vne_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_vne_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_vne_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_vne_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_vne_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_vne_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vne_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_vne_u_f_512(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S8x16 a) {
    return __builtin_tachy_vneg1_128(a);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S8x32 a) {
    return __builtin_tachy_vneg1_256(a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S8x64 a) {
    return __builtin_tachy_vneg1_512(a);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S16x8 a) {
    return __builtin_tachy_vneg2_128(a);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S16x16 a) {
    return __builtin_tachy_vneg2_256(a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S16x32 a) {
    return __builtin_tachy_vneg2_512(a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S32x4 a) {
    return __builtin_tachy_vneg4_128(a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S32x8 a) {
    return __builtin_tachy_vneg4_256(a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S32x16 a) {
    return __builtin_tachy_vneg4_512(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S64x2 a) {
    return __builtin_tachy_vneg8_128(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S64x4 a) {
    return __builtin_tachy_vneg8_256(a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(S64x8 a) {
    return __builtin_tachy_vneg8_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F64x2 a) {
    return __builtin_tachy_vneg_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F64x4 a) {
    return __builtin_tachy_vneg_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F64x8 a) {
    return __builtin_tachy_vneg_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F32x4 a) {
    return __builtin_tachy_vneg_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F32x8 a) {
    return __builtin_tachy_vneg_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vneg(F32x16 a) {
    return __builtin_tachy_vneg_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vnmadd_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vnmadd_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vnmadd_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vnmadd_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vnmadd_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmadd(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vnmadd_f_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F64x2 a, F64x2 b, F64x2 c) {
    return __builtin_tachy_vnmsub_d_128(a, b, c);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F64x4 a, F64x4 b, F64x4 c) {
    return __builtin_tachy_vnmsub_d_256(a, b, c);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F64x8 a, F64x8 b, F64x8 c) {
    return __builtin_tachy_vnmsub_d_512(a, b, c);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F32x4 a, F32x4 b, F32x4 c) {
    return __builtin_tachy_vnmsub_f_128(a, b, c);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F32x8 a, F32x8 b, F32x8 c) {
    return __builtin_tachy_vnmsub_f_256(a, b, c);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmsub(F32x16 a, F32x16 b, F32x16 c) {
    return __builtin_tachy_vnmsub_f_512(a, b, c);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F64x2 a, F64x2 b) {
    return __builtin_tachy_vnmul_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F64x4 a, F64x4 b) {
    return __builtin_tachy_vnmul_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F64x8 a, F64x8 b) {
    return __builtin_tachy_vnmul_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F32x4 a, F32x4 b) {
    return __builtin_tachy_vnmul_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F32x8 a, F32x8 b) {
    return __builtin_tachy_vnmul_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnmul(F32x16 a, F32x16 b) {
    return __builtin_tachy_vnmul_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U8x16 a) {
    return __builtin_tachy_vnot1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U8x32 a) {
    return __builtin_tachy_vnot1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U8x64 a) {
    return __builtin_tachy_vnot1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U16x8 a) {
    return __builtin_tachy_vnot2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U16x16 a) {
    return __builtin_tachy_vnot2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U16x32 a) {
    return __builtin_tachy_vnot2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U32x4 a) {
    return __builtin_tachy_vnot4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U32x8 a) {
    return __builtin_tachy_vnot4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U32x16 a) {
    return __builtin_tachy_vnot4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U64x2 a) {
    return __builtin_tachy_vnot8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U64x4 a) {
    return __builtin_tachy_vnot8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vnot(U64x8 a) {
    return __builtin_tachy_vnot8_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U8x16 a, U8x16 b) {
    return __builtin_tachy_vor1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U8x32 a, U8x32 b) {
    return __builtin_tachy_vor1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U8x64 a, U8x64 b) {
    return __builtin_tachy_vor1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U16x8 a, U16x8 b) {
    return __builtin_tachy_vor2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U16x16 a, U16x16 b) {
    return __builtin_tachy_vor2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U16x32 a, U16x32 b) {
    return __builtin_tachy_vor2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U32x4 a, U32x4 b) {
    return __builtin_tachy_vor4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U32x8 a, U32x8 b) {
    return __builtin_tachy_vor4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U32x16 a, U32x16 b) {
    return __builtin_tachy_vor4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U64x2 a, U64x2 b) {
    return __builtin_tachy_vor8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U64x4 a, U64x4 b) {
    return __builtin_tachy_vor8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vor(U64x8 a, U64x8 b) {
    return __builtin_tachy_vor8_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F64x2 a, F64x2 b) {
    return __builtin_tachy_vord_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F64x4 a, F64x4 b) {
    return __builtin_tachy_vord_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F64x8 a, F64x8 b) {
    return __builtin_tachy_vord_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F32x4 a, F32x4 b) {
    return __builtin_tachy_vord_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F32x8 a, F32x8 b) {
    return __builtin_tachy_vord_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord(F32x16 a, F32x16 b) {
    return __builtin_tachy_vord_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_vord_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_vord_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_vord_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_vord_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_vord_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vord_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_vord_u_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U8x16 a) {
    return __builtin_tachy_vpar1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U8x32 a) {
    return __builtin_tachy_vpar1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U8x64 a) {
    return __builtin_tachy_vpar1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U16x8 a) {
    return __builtin_tachy_vpar2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U16x16 a) {
    return __builtin_tachy_vpar2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U16x32 a) {
    return __builtin_tachy_vpar2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U32x4 a) {
    return __builtin_tachy_vpar4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U32x8 a) {
    return __builtin_tachy_vpar4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U32x16 a) {
    return __builtin_tachy_vpar4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U64x2 a) {
    return __builtin_tachy_vpar8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U64x4 a) {
    return __builtin_tachy_vpar8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpar(U64x8 a) {
    return __builtin_tachy_vpar8_512(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(U16x32 a, U16x32 b) {
    return __builtin_tachy_vpck_b_h_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(S16x32 a, S16x32 b) {
    return (S8x64) __builtin_tachy_vpck_b_h_512((__u16x32) a, (__u16x32) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(U32x16 a, U32x16 b) {
    return __builtin_tachy_vpck_h_w_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(S32x16 a, S32x16 b) {
    return (S16x32) __builtin_tachy_vpck_h_w_512((__u32x16) a, (__u32x16) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(U64x8 a, U64x8 b) {
    return __builtin_tachy_vpck_w_l_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpck(S64x8 a, S64x8 b) {
    return (S32x16) __builtin_tachy_vpck_w_l_512((__u64x8) a, (__u64x8) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcks(U16x32 a, U16x32 b) {
    return __builtin_tachy_vpcks_b_h_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcks(U32x16 a, U32x16 b) {
    return __builtin_tachy_vpcks_h_w_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcks(U64x8 a, U64x8 b) {
    return __builtin_tachy_vpcks_w_l_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcku(U16x32 a, U16x32 b) {
    return __builtin_tachy_vpcku_b_h_512(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcku(U32x16 a, U32x16 b) {
    return __builtin_tachy_vpcku_h_w_512(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpcku(U64x8 a, U64x8 b) {
    return __builtin_tachy_vpcku_w_l_512(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U8x64 a, U8x64 b) {
    return __builtin_tachy_vperm_b_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vperm_b_512((__u8x64) a, (__u8x64) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U16x32 a, U16x32 b) {
    return __builtin_tachy_vperm_h_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vperm_h_512((__u16x32) a, (__u16x32) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U64x8 a, U64x8 b) {
    return __builtin_tachy_vperm_l_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vperm_l_512((__u64x8) a, (__u64x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U32x16 a, U32x16 b) {
    return __builtin_tachy_vperm_w_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vperm_w_512((__u32x16) a, (__u32x16) b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vperm_x_b_128(a, b, c);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S8x16 a, S8x16 b, S8x16 c) {
    return (S8x16) __builtin_tachy_vperm_x_b_128((__u8x16) a, (__u8x16) b, (__u8x16) c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vperm_x_b_256(a, b, c);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S8x32 a, S8x32 b, S8x32 c) {
    return (S8x32) __builtin_tachy_vperm_x_b_256((__u8x32) a, (__u8x32) b, (__u8x32) c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vperm_x_b_512(a, b, c);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S8x64 a, S8x64 b, S8x64 c) {
    return (S8x64) __builtin_tachy_vperm_x_b_512((__u8x64) a, (__u8x64) b, (__u8x64) c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vperm_x_h_128(a, b, c);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S16x8 a, S16x8 b, S16x8 c) {
    return (S16x8) __builtin_tachy_vperm_x_h_128((__u16x8) a, (__u16x8) b, (__u16x8) c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vperm_x_h_256(a, b, c);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S16x16 a, S16x16 b, S16x16 c) {
    return (S16x16) __builtin_tachy_vperm_x_h_256((__u16x16) a, (__u16x16) b, (__u16x16) c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vperm_x_h_512(a, b, c);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S16x32 a, S16x32 b, S16x32 c) {
    return (S16x32) __builtin_tachy_vperm_x_h_512((__u16x32) a, (__u16x32) b, (__u16x32) c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vperm_x_l_128(a, b, c);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S64x2 a, S64x2 b, S64x2 c) {
    return (S64x2) __builtin_tachy_vperm_x_l_128((__u64x2) a, (__u64x2) b, (__u64x2) c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vperm_x_l_256(a, b, c);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S64x4 a, S64x4 b, S64x4 c) {
    return (S64x4) __builtin_tachy_vperm_x_l_256((__u64x4) a, (__u64x4) b, (__u64x4) c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vperm_x_l_512(a, b, c);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S64x8 a, S64x8 b, S64x8 c) {
    return (S64x8) __builtin_tachy_vperm_x_l_512((__u64x8) a, (__u64x8) b, (__u64x8) c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vperm_x_w_128(a, b, c);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S32x4 a, S32x4 b, S32x4 c) {
    return (S32x4) __builtin_tachy_vperm_x_w_128((__u32x4) a, (__u32x4) b, (__u32x4) c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vperm_x_w_256(a, b, c);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S32x8 a, S32x8 b, S32x8 c) {
    return (S32x8) __builtin_tachy_vperm_x_w_256((__u32x8) a, (__u32x8) b, (__u32x8) c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vperm_x_w_512(a, b, c);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperm(S32x16 a, S32x16 b, S32x16 c) {
    return (S32x16) __builtin_tachy_vperm_x_w_512((__u32x16) a, (__u32x16) b, (__u32x16) c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U8x16 a, U8x16 b) {
    return __builtin_tachy_vperml_b_128(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S8x16 a, S8x16 b) {
    return (S8x16) __builtin_tachy_vperml_b_128((__u8x16) a, (__u8x16) b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U8x32 a, U8x32 b) {
    return __builtin_tachy_vperml_b_256(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S8x32 a, S8x32 b) {
    return (S8x32) __builtin_tachy_vperml_b_256((__u8x32) a, (__u8x32) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U8x64 a, U8x64 b) {
    return __builtin_tachy_vperml_b_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vperml_b_512((__u8x64) a, (__u8x64) b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U16x8 a, U16x8 b) {
    return __builtin_tachy_vperml_h_128(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S16x8 a, S16x8 b) {
    return (S16x8) __builtin_tachy_vperml_h_128((__u16x8) a, (__u16x8) b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U16x16 a, U16x16 b) {
    return __builtin_tachy_vperml_h_256(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S16x16 a, S16x16 b) {
    return (S16x16) __builtin_tachy_vperml_h_256((__u16x16) a, (__u16x16) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U16x32 a, U16x32 b) {
    return __builtin_tachy_vperml_h_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vperml_h_512((__u16x32) a, (__u16x32) b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U64x2 a, U64x2 b) {
    return __builtin_tachy_vperml_l_128(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S64x2 a, S64x2 b) {
    return (S64x2) __builtin_tachy_vperml_l_128((__u64x2) a, (__u64x2) b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U64x4 a, U64x4 b) {
    return __builtin_tachy_vperml_l_256(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S64x4 a, S64x4 b) {
    return (S64x4) __builtin_tachy_vperml_l_256((__u64x4) a, (__u64x4) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U64x8 a, U64x8 b) {
    return __builtin_tachy_vperml_l_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vperml_l_512((__u64x8) a, (__u64x8) b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U32x4 a, U32x4 b) {
    return __builtin_tachy_vperml_w_128(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S32x4 a, S32x4 b) {
    return (S32x4) __builtin_tachy_vperml_w_128((__u32x4) a, (__u32x4) b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U32x8 a, U32x8 b) {
    return __builtin_tachy_vperml_w_256(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S32x8 a, S32x8 b) {
    return (S32x8) __builtin_tachy_vperml_w_256((__u32x8) a, (__u32x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(U32x16 a, U32x16 b) {
    return __builtin_tachy_vperml_w_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vperml(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vperml_w_512((__u32x16) a, (__u32x16) b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U8x16 a, U8x16 b) {
    return __builtin_tachy_vpermxx_b_128(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S8x16 a, S8x16 b) {
    return (S8x16) __builtin_tachy_vpermxx_b_128((__u8x16) a, (__u8x16) b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U8x32 a, U8x32 b) {
    return __builtin_tachy_vpermxx_b_256(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S8x32 a, S8x32 b) {
    return (S8x32) __builtin_tachy_vpermxx_b_256((__u8x32) a, (__u8x32) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U8x64 a, U8x64 b) {
    return __builtin_tachy_vpermxx_b_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vpermxx_b_512((__u8x64) a, (__u8x64) b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U16x8 a, U16x8 b) {
    return __builtin_tachy_vpermxx_h_128(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S16x8 a, S16x8 b) {
    return (S16x8) __builtin_tachy_vpermxx_h_128((__u16x8) a, (__u16x8) b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U16x16 a, U16x16 b) {
    return __builtin_tachy_vpermxx_h_256(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S16x16 a, S16x16 b) {
    return (S16x16) __builtin_tachy_vpermxx_h_256((__u16x16) a, (__u16x16) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U16x32 a, U16x32 b) {
    return __builtin_tachy_vpermxx_h_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vpermxx_h_512((__u16x32) a, (__u16x32) b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U64x2 a, U64x2 b) {
    return __builtin_tachy_vpermxx_l_128(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S64x2 a, S64x2 b) {
    return (S64x2) __builtin_tachy_vpermxx_l_128((__u64x2) a, (__u64x2) b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U64x4 a, U64x4 b) {
    return __builtin_tachy_vpermxx_l_256(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S64x4 a, S64x4 b) {
    return (S64x4) __builtin_tachy_vpermxx_l_256((__u64x4) a, (__u64x4) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U64x8 a, U64x8 b) {
    return __builtin_tachy_vpermxx_l_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vpermxx_l_512((__u64x8) a, (__u64x8) b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U32x4 a, U32x4 b) {
    return __builtin_tachy_vpermxx_w_128(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S32x4 a, S32x4 b) {
    return (S32x4) __builtin_tachy_vpermxx_w_128((__u32x4) a, (__u32x4) b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U32x8 a, U32x8 b) {
    return __builtin_tachy_vpermxx_w_256(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S32x8 a, S32x8 b) {
    return (S32x8) __builtin_tachy_vpermxx_w_256((__u32x8) a, (__u32x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(U32x16 a, U32x16 b) {
    return __builtin_tachy_vpermxx_w_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpermxx(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vpermxx_w_512((__u32x16) a, (__u32x16) b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U8x16 a) {
    return __builtin_tachy_vpopc1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U8x32 a) {
    return __builtin_tachy_vpopc1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U8x64 a) {
    return __builtin_tachy_vpopc1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U16x8 a) {
    return __builtin_tachy_vpopc2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U16x16 a) {
    return __builtin_tachy_vpopc2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U16x32 a) {
    return __builtin_tachy_vpopc2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U32x4 a) {
    return __builtin_tachy_vpopc4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U32x8 a) {
    return __builtin_tachy_vpopc4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U32x16 a) {
    return __builtin_tachy_vpopc4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U64x2 a) {
    return __builtin_tachy_vpopc8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U64x4 a) {
    return __builtin_tachy_vpopc8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vpopc(U64x8 a) {
    return __builtin_tachy_vpopc8_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U8x16 a, U8x16 b) {
    return __builtin_tachy_vprod2_1_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U8x32 a, U8x32 b) {
    return __builtin_tachy_vprod2_1_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U8x64 a, U8x64 b) {
    return __builtin_tachy_vprod2_1_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U16x8 a, U16x8 b) {
    return __builtin_tachy_vprod4_2_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U16x16 a, U16x16 b) {
    return __builtin_tachy_vprod4_2_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U16x32 a, U16x32 b) {
    return __builtin_tachy_vprod4_2_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U32x4 a, U32x4 b) {
    return __builtin_tachy_vprod8_4_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U32x8 a, U32x8 b) {
    return __builtin_tachy_vprod8_4_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprod(U32x16 a, U32x16 b) {
    return __builtin_tachy_vprod8_4_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S8x16 a, S8x16 b) {
    return __builtin_tachy_vprods2_1_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S8x32 a, S8x32 b) {
    return __builtin_tachy_vprods2_1_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S8x64 a, S8x64 b) {
    return __builtin_tachy_vprods2_1_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S16x8 a, S16x8 b) {
    return __builtin_tachy_vprods4_2_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S16x16 a, S16x16 b) {
    return __builtin_tachy_vprods4_2_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S16x32 a, S16x32 b) {
    return __builtin_tachy_vprods4_2_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S32x4 a, S32x4 b) {
    return __builtin_tachy_vprods8_4_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S32x8 a, S32x8 b) {
    return __builtin_tachy_vprods8_4_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprods(S32x16 a, S32x16 b) {
    return __builtin_tachy_vprods8_4_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U8x16 a, S8x16 b) {
    return __builtin_tachy_vprodus2_1_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U8x32 a, S8x32 b) {
    return __builtin_tachy_vprodus2_1_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U8x64 a, S8x64 b) {
    return __builtin_tachy_vprodus2_1_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U16x8 a, S16x8 b) {
    return __builtin_tachy_vprodus4_2_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U16x16 a, S16x16 b) {
    return __builtin_tachy_vprodus4_2_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U16x32 a, S16x32 b) {
    return __builtin_tachy_vprodus4_2_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U32x4 a, S32x4 b) {
    return __builtin_tachy_vprodus8_4_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U32x8 a, S32x8 b) {
    return __builtin_tachy_vprodus8_4_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vprodus(U32x16 a, S32x16 b) {
    return __builtin_tachy_vprodus8_4_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U8x16 a) {
    return __builtin_tachy_vrbit1_128(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U8x32 a) {
    return __builtin_tachy_vrbit1_256(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U8x64 a) {
    return __builtin_tachy_vrbit1_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U16x8 a) {
    return __builtin_tachy_vrbit2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U16x16 a) {
    return __builtin_tachy_vrbit2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U16x32 a) {
    return __builtin_tachy_vrbit2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U32x4 a) {
    return __builtin_tachy_vrbit4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U32x8 a) {
    return __builtin_tachy_vrbit4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U32x16 a) {
    return __builtin_tachy_vrbit4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U64x2 a) {
    return __builtin_tachy_vrbit8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U64x4 a) {
    return __builtin_tachy_vrbit8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrbit(U64x8 a) {
    return __builtin_tachy_vrbit8_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U16x8 a) {
    return __builtin_tachy_vrev2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U16x16 a) {
    return __builtin_tachy_vrev2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U16x32 a) {
    return __builtin_tachy_vrev2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U32x4 a) {
    return __builtin_tachy_vrev4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U32x8 a) {
    return __builtin_tachy_vrev4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U32x16 a) {
    return __builtin_tachy_vrev4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U64x2 a) {
    return __builtin_tachy_vrev8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U64x4 a) {
    return __builtin_tachy_vrev8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrev(U64x8 a) {
    return __builtin_tachy_vrev8_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x16 a, U8x16 b) {
    return __builtin_tachy_vrol1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x32 a, U8x32 b) {
    return __builtin_tachy_vrol1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x64 a, U8x64 b) {
    return __builtin_tachy_vrol1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x8 a, U16x8 b) {
    return __builtin_tachy_vrol2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x16 a, U16x16 b) {
    return __builtin_tachy_vrol2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x32 a, U16x32 b) {
    return __builtin_tachy_vrol2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x4 a, U32x4 b) {
    return __builtin_tachy_vrol4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x8 a, U32x8 b) {
    return __builtin_tachy_vrol4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x16 a, U32x16 b) {
    return __builtin_tachy_vrol4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x2 a, U64x2 b) {
    return __builtin_tachy_vrol8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x4 a, U64x4 b) {
    return __builtin_tachy_vrol8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x8 a, U64x8 b) {
    return __builtin_tachy_vrol8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x16 a, unsigned int b) {
    return __builtin_tachy_vrol_r_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x32 a, unsigned int b) {
    return __builtin_tachy_vrol_r_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U8x64 a, unsigned int b) {
    return __builtin_tachy_vrol_r_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x8 a, unsigned int b) {
    return __builtin_tachy_vrol_r_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x16 a, unsigned int b) {
    return __builtin_tachy_vrol_r_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U16x32 a, unsigned int b) {
    return __builtin_tachy_vrol_r_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x2 a, unsigned int b) {
    return __builtin_tachy_vrol_r_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x4 a, unsigned int b) {
    return __builtin_tachy_vrol_r_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U64x8 a, unsigned int b) {
    return __builtin_tachy_vrol_r_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x4 a, unsigned int b) {
    return __builtin_tachy_vrol_r_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x8 a, unsigned int b) {
    return __builtin_tachy_vrol_r_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrol(U32x16 a, unsigned int b) {
    return __builtin_tachy_vrol_r_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U8x16 a, unsigned int b) {
    return __builtin_tachy_vrole_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U8x32 a, unsigned int b) {
    return __builtin_tachy_vrole_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U8x64 a, unsigned int b) {
    return __builtin_tachy_vrole_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U16x8 a, unsigned int b) {
    return __builtin_tachy_vrole_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U16x16 a, unsigned int b) {
    return __builtin_tachy_vrole_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U16x32 a, unsigned int b) {
    return __builtin_tachy_vrole_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U64x2 a, unsigned int b) {
    return __builtin_tachy_vrole_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U64x4 a, unsigned int b) {
    return __builtin_tachy_vrole_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U64x8 a, unsigned int b) {
    return __builtin_tachy_vrole_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U32x4 a, unsigned int b) {
    return __builtin_tachy_vrole_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U32x8 a, unsigned int b) {
    return __builtin_tachy_vrole_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrole(U32x16 a, unsigned int b) {
    return __builtin_tachy_vrole_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x16 a, U8x16 b) {
    return __builtin_tachy_vror1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x32 a, U8x32 b) {
    return __builtin_tachy_vror1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x64 a, U8x64 b) {
    return __builtin_tachy_vror1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x8 a, U16x8 b) {
    return __builtin_tachy_vror2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x16 a, U16x16 b) {
    return __builtin_tachy_vror2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x32 a, U16x32 b) {
    return __builtin_tachy_vror2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x4 a, U32x4 b) {
    return __builtin_tachy_vror4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x8 a, U32x8 b) {
    return __builtin_tachy_vror4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x16 a, U32x16 b) {
    return __builtin_tachy_vror4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x2 a, U64x2 b) {
    return __builtin_tachy_vror8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x4 a, U64x4 b) {
    return __builtin_tachy_vror8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x8 a, U64x8 b) {
    return __builtin_tachy_vror8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x16 a, unsigned int b) {
    return __builtin_tachy_vror_r_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x32 a, unsigned int b) {
    return __builtin_tachy_vror_r_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U8x64 a, unsigned int b) {
    return __builtin_tachy_vror_r_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x8 a, unsigned int b) {
    return __builtin_tachy_vror_r_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x16 a, unsigned int b) {
    return __builtin_tachy_vror_r_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U16x32 a, unsigned int b) {
    return __builtin_tachy_vror_r_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x2 a, unsigned int b) {
    return __builtin_tachy_vror_r_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x4 a, unsigned int b) {
    return __builtin_tachy_vror_r_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U64x8 a, unsigned int b) {
    return __builtin_tachy_vror_r_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x4 a, unsigned int b) {
    return __builtin_tachy_vror_r_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x8 a, unsigned int b) {
    return __builtin_tachy_vror_r_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vror(U32x16 a, unsigned int b) {
    return __builtin_tachy_vror_r_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U8x16 a, unsigned int b) {
    return __builtin_tachy_vrore_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U8x32 a, unsigned int b) {
    return __builtin_tachy_vrore_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U8x64 a, unsigned int b) {
    return __builtin_tachy_vrore_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U16x8 a, unsigned int b) {
    return __builtin_tachy_vrore_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U16x16 a, unsigned int b) {
    return __builtin_tachy_vrore_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U16x32 a, unsigned int b) {
    return __builtin_tachy_vrore_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U64x2 a, unsigned int b) {
    return __builtin_tachy_vrore_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U64x4 a, unsigned int b) {
    return __builtin_tachy_vrore_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U64x8 a, unsigned int b) {
    return __builtin_tachy_vrore_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U32x4 a, unsigned int b) {
    return __builtin_tachy_vrore_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U32x8 a, unsigned int b) {
    return __builtin_tachy_vrore_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrore(U32x16 a, unsigned int b) {
    return __builtin_tachy_vrore_w_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F64x2 a) {
    return __builtin_tachy_vround_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F64x4 a) {
    return __builtin_tachy_vround_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F64x8 a) {
    return __builtin_tachy_vround_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F32x4 a) {
    return __builtin_tachy_vround_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F32x8 a) {
    return __builtin_tachy_vround_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vround(F32x16 a) {
    return __builtin_tachy_vround_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F64x2 a) {
    return __builtin_tachy_vrounda_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F64x4 a) {
    return __builtin_tachy_vrounda_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F64x8 a) {
    return __builtin_tachy_vrounda_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F32x4 a) {
    return __builtin_tachy_vrounda_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F32x8 a) {
    return __builtin_tachy_vrounda_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrounda(F32x16 a) {
    return __builtin_tachy_vrounda_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F64x2 a) {
    return __builtin_tachy_vroundi_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F64x4 a) {
    return __builtin_tachy_vroundi_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F64x8 a) {
    return __builtin_tachy_vroundi_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F32x4 a) {
    return __builtin_tachy_vroundi_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F32x8 a) {
    return __builtin_tachy_vroundi_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundi(F32x16 a) {
    return __builtin_tachy_vroundi_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F64x2 a) {
    return __builtin_tachy_vroundia_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F64x4 a) {
    return __builtin_tachy_vroundia_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F64x8 a) {
    return __builtin_tachy_vroundia_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F32x4 a) {
    return __builtin_tachy_vroundia_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F32x8 a) {
    return __builtin_tachy_vroundia_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundia(F32x16 a) {
    return __builtin_tachy_vroundia_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F64x2 a) {
    return __builtin_tachy_vroundin_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F64x4 a) {
    return __builtin_tachy_vroundin_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F64x8 a) {
    return __builtin_tachy_vroundin_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F32x4 a) {
    return __builtin_tachy_vroundin_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F32x8 a) {
    return __builtin_tachy_vroundin_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundin(F32x16 a) {
    return __builtin_tachy_vroundin_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F64x2 a) {
    return __builtin_tachy_vroundip_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F64x4 a) {
    return __builtin_tachy_vroundip_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F64x8 a) {
    return __builtin_tachy_vroundip_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F32x4 a) {
    return __builtin_tachy_vroundip_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F32x8 a) {
    return __builtin_tachy_vroundip_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundip(F32x16 a) {
    return __builtin_tachy_vroundip_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F64x2 a) {
    return __builtin_tachy_vroundir_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F64x4 a) {
    return __builtin_tachy_vroundir_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F64x8 a) {
    return __builtin_tachy_vroundir_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F32x4 a) {
    return __builtin_tachy_vroundir_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F32x8 a) {
    return __builtin_tachy_vroundir_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundir(F32x16 a) {
    return __builtin_tachy_vroundir_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F64x2 a) {
    return __builtin_tachy_vroundit_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F64x4 a) {
    return __builtin_tachy_vroundit_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F64x8 a) {
    return __builtin_tachy_vroundit_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F32x4 a) {
    return __builtin_tachy_vroundit_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F32x8 a) {
    return __builtin_tachy_vroundit_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundit(F32x16 a) {
    return __builtin_tachy_vroundit_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F64x2 a) {
    return __builtin_tachy_vroundix_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F64x4 a) {
    return __builtin_tachy_vroundix_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F64x8 a) {
    return __builtin_tachy_vroundix_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F32x4 a) {
    return __builtin_tachy_vroundix_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F32x8 a) {
    return __builtin_tachy_vroundix_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundix(F32x16 a) {
    return __builtin_tachy_vroundix_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F64x2 a) {
    return __builtin_tachy_vroundn_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F64x4 a) {
    return __builtin_tachy_vroundn_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F64x8 a) {
    return __builtin_tachy_vroundn_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F32x4 a) {
    return __builtin_tachy_vroundn_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F32x8 a) {
    return __builtin_tachy_vroundn_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundn(F32x16 a) {
    return __builtin_tachy_vroundn_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F64x2 a) {
    return __builtin_tachy_vroundp_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F64x4 a) {
    return __builtin_tachy_vroundp_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F64x8 a) {
    return __builtin_tachy_vroundp_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F32x4 a) {
    return __builtin_tachy_vroundp_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F32x8 a) {
    return __builtin_tachy_vroundp_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundp(F32x16 a) {
    return __builtin_tachy_vroundp_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F64x2 a) {
    return __builtin_tachy_vroundr_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F64x4 a) {
    return __builtin_tachy_vroundr_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F64x8 a) {
    return __builtin_tachy_vroundr_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F32x4 a) {
    return __builtin_tachy_vroundr_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F32x8 a) {
    return __builtin_tachy_vroundr_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundr(F32x16 a) {
    return __builtin_tachy_vroundr_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F64x2 a) {
    return __builtin_tachy_vroundt_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F64x4 a) {
    return __builtin_tachy_vroundt_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F64x8 a) {
    return __builtin_tachy_vroundt_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F32x4 a) {
    return __builtin_tachy_vroundt_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F32x8 a) {
    return __builtin_tachy_vroundt_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundt(F32x16 a) {
    return __builtin_tachy_vroundt_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F64x2 a) {
    return __builtin_tachy_vroundx_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F64x4 a) {
    return __builtin_tachy_vroundx_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F64x8 a) {
    return __builtin_tachy_vroundx_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F32x4 a) {
    return __builtin_tachy_vroundx_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F32x8 a) {
    return __builtin_tachy_vroundx_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vroundx(F32x16 a) {
    return __builtin_tachy_vroundx_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F64x2 a) {
    return __builtin_tachy_vrsqrt_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F64x4 a) {
    return __builtin_tachy_vrsqrt_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F64x8 a) {
    return __builtin_tachy_vrsqrt_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F32x4 a) {
    return __builtin_tachy_vrsqrt_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F32x8 a) {
    return __builtin_tachy_vrsqrt_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrt(F32x16 a) {
    return __builtin_tachy_vrsqrt_f_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F64x2 a) {
    return __builtin_tachy_vrsqrtx_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F64x4 a) {
    return __builtin_tachy_vrsqrtx_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F64x8 a) {
    return __builtin_tachy_vrsqrtx_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F32x4 a) {
    return __builtin_tachy_vrsqrtx_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F32x8 a) {
    return __builtin_tachy_vrsqrtx_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vrsqrtx(F32x16 a) {
    return __builtin_tachy_vrsqrtx_f_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsad(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsad_l_b_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsad(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsad_l_b_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsad(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsad_l_b_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsada(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsada_l_b_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsada(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsada_l_b_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsada(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsada_l_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadm(U8x16 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vsadm_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadm(U8x32 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vsadm_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadm(U8x64 a, U8x64 b, unsigned int c) {
    return __builtin_tachy_vsadm_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadma(U8x16 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vsadma_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadma(U8x32 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vsadma_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadma(U8x64 a, U8x64 b, unsigned int c) {
    return __builtin_tachy_vsadma_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadt(U8x16 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vsadt_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadt(U8x32 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vsadt_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadt(U8x64 a, U8x64 b, unsigned int c) {
    return __builtin_tachy_vsadt_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadta(U8x16 a, U8x16 b, unsigned int c) {
    return __builtin_tachy_vsadta_h_b_128(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadta(U8x32 a, U8x32 b, unsigned int c) {
    return __builtin_tachy_vsadta_h_b_256(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsadta(U8x64 a, U8x64 b, unsigned int c) {
    return __builtin_tachy_vsadta_h_b_512(a, b, __builtin_constant_p (c) ? (c) : -1);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S8x16 a, S8x16 b) {
    return __builtin_tachy_vsar1_128(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S8x32 a, S8x32 b) {
    return __builtin_tachy_vsar1_256(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S8x64 a, S8x64 b) {
    return __builtin_tachy_vsar1_512(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S16x8 a, S16x8 b) {
    return __builtin_tachy_vsar2_128(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S16x16 a, S16x16 b) {
    return __builtin_tachy_vsar2_256(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S16x32 a, S16x32 b) {
    return __builtin_tachy_vsar2_512(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S32x4 a, S32x4 b) {
    return __builtin_tachy_vsar4_128(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S32x8 a, S32x8 b) {
    return __builtin_tachy_vsar4_256(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S32x16 a, S32x16 b) {
    return __builtin_tachy_vsar4_512(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S64x2 a, S64x2 b) {
    return __builtin_tachy_vsar8_128(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S64x4 a, S64x4 b) {
    return __builtin_tachy_vsar8_256(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(S64x8 a, S64x8 b) {
    return __builtin_tachy_vsar8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U8x16 a, unsigned int b) {
    return __builtin_tachy_vsar_r_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U8x32 a, unsigned int b) {
    return __builtin_tachy_vsar_r_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U8x64 a, unsigned int b) {
    return __builtin_tachy_vsar_r_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U16x8 a, unsigned int b) {
    return __builtin_tachy_vsar_r_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U16x16 a, unsigned int b) {
    return __builtin_tachy_vsar_r_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U16x32 a, unsigned int b) {
    return __builtin_tachy_vsar_r_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U64x2 a, unsigned int b) {
    return __builtin_tachy_vsar_r_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U64x4 a, unsigned int b) {
    return __builtin_tachy_vsar_r_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U64x8 a, unsigned int b) {
    return __builtin_tachy_vsar_r_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U32x4 a, unsigned int b) {
    return __builtin_tachy_vsar_r_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U32x8 a, unsigned int b) {
    return __builtin_tachy_vsar_r_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsar(U32x16 a, unsigned int b) {
    return __builtin_tachy_vsar_r_w_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshf_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshf_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshf_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U32x4 a, unsigned int b) {
    return __builtin_tachy_vshf_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U32x8 a, unsigned int b) {
    return __builtin_tachy_vshf_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshf(U32x16 a, unsigned int b) {
    return __builtin_tachy_vshf_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfh(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshfh_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfh(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshfh_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfh(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshfh_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfl(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshfl_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfl(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshfl_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshfl(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshfl_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x16 a, U8x16 b) {
    return __builtin_tachy_vshl1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x32 a, U8x32 b) {
    return __builtin_tachy_vshl1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x64 a, U8x64 b) {
    return __builtin_tachy_vshl1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x8 a, U16x8 b) {
    return __builtin_tachy_vshl2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x16 a, U16x16 b) {
    return __builtin_tachy_vshl2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x32 a, U16x32 b) {
    return __builtin_tachy_vshl2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x4 a, U32x4 b) {
    return __builtin_tachy_vshl4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x8 a, U32x8 b) {
    return __builtin_tachy_vshl4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x16 a, U32x16 b) {
    return __builtin_tachy_vshl4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x2 a, U64x2 b) {
    return __builtin_tachy_vshl8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x4 a, U64x4 b) {
    return __builtin_tachy_vshl8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x8 a, U64x8 b) {
    return __builtin_tachy_vshl8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x16 a, unsigned int b) {
    return __builtin_tachy_vshl_r_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x32 a, unsigned int b) {
    return __builtin_tachy_vshl_r_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U8x64 a, unsigned int b) {
    return __builtin_tachy_vshl_r_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshl_r_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshl_r_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshl_r_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x2 a, unsigned int b) {
    return __builtin_tachy_vshl_r_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x4 a, unsigned int b) {
    return __builtin_tachy_vshl_r_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U64x8 a, unsigned int b) {
    return __builtin_tachy_vshl_r_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x4 a, unsigned int b) {
    return __builtin_tachy_vshl_r_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x8 a, unsigned int b) {
    return __builtin_tachy_vshl_r_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshl(U32x16 a, unsigned int b) {
    return __builtin_tachy_vshl_r_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U8x16 a, unsigned int b) {
    return __builtin_tachy_vshle_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U8x32 a, unsigned int b) {
    return __builtin_tachy_vshle_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U8x64 a, unsigned int b) {
    return __builtin_tachy_vshle_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshle_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshle_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshle_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U64x2 a, unsigned int b) {
    return __builtin_tachy_vshle_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U64x4 a, unsigned int b) {
    return __builtin_tachy_vshle_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U64x8 a, unsigned int b) {
    return __builtin_tachy_vshle_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U32x4 a, unsigned int b) {
    return __builtin_tachy_vshle_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U32x8 a, unsigned int b) {
    return __builtin_tachy_vshle_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshle(U32x16 a, unsigned int b) {
    return __builtin_tachy_vshle_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x16 a, U8x16 b) {
    return __builtin_tachy_vshr1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x32 a, U8x32 b) {
    return __builtin_tachy_vshr1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x64 a, U8x64 b) {
    return __builtin_tachy_vshr1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x8 a, U16x8 b) {
    return __builtin_tachy_vshr2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x16 a, U16x16 b) {
    return __builtin_tachy_vshr2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x32 a, U16x32 b) {
    return __builtin_tachy_vshr2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x4 a, U32x4 b) {
    return __builtin_tachy_vshr4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x8 a, U32x8 b) {
    return __builtin_tachy_vshr4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x16 a, U32x16 b) {
    return __builtin_tachy_vshr4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x2 a, U64x2 b) {
    return __builtin_tachy_vshr8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x4 a, U64x4 b) {
    return __builtin_tachy_vshr8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x8 a, U64x8 b) {
    return __builtin_tachy_vshr8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x16 a, unsigned int b) {
    return __builtin_tachy_vshr_r_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x32 a, unsigned int b) {
    return __builtin_tachy_vshr_r_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U8x64 a, unsigned int b) {
    return __builtin_tachy_vshr_r_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshr_r_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshr_r_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshr_r_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x2 a, unsigned int b) {
    return __builtin_tachy_vshr_r_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x4 a, unsigned int b) {
    return __builtin_tachy_vshr_r_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U64x8 a, unsigned int b) {
    return __builtin_tachy_vshr_r_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x4 a, unsigned int b) {
    return __builtin_tachy_vshr_r_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x8 a, unsigned int b) {
    return __builtin_tachy_vshr_r_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshr(U32x16 a, unsigned int b) {
    return __builtin_tachy_vshr_r_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U8x16 a, unsigned int b) {
    return __builtin_tachy_vshre_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U8x32 a, unsigned int b) {
    return __builtin_tachy_vshre_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U8x64 a, unsigned int b) {
    return __builtin_tachy_vshre_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U16x8 a, unsigned int b) {
    return __builtin_tachy_vshre_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U16x16 a, unsigned int b) {
    return __builtin_tachy_vshre_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U16x32 a, unsigned int b) {
    return __builtin_tachy_vshre_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U64x2 a, unsigned int b) {
    return __builtin_tachy_vshre_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U64x4 a, unsigned int b) {
    return __builtin_tachy_vshre_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U64x8 a, unsigned int b) {
    return __builtin_tachy_vshre_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U32x4 a, unsigned int b) {
    return __builtin_tachy_vshre_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U32x8 a, unsigned int b) {
    return __builtin_tachy_vshre_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vshre(U32x16 a, unsigned int b) {
    return __builtin_tachy_vshre_w_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsprod2_1_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsprod2_1_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsprod2_1_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsprod4_2_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsprod4_2_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsprod4_2_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsprod8_4_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsprod8_4_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprod(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsprod8_4_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsprods2_1_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsprods2_1_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsprods2_1_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsprods4_2_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsprods4_2_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsprods4_2_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsprods8_4_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsprods8_4_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprods(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsprods8_4_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsprodus2_1_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsprodus2_1_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsprodus2_1_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsprodus4_2_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsprodus4_2_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsprodus4_2_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsprodus8_4_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsprodus8_4_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsprodus(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsprodus8_4_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F64x2 a) {
    return __builtin_tachy_vsqrt_d_128(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F64x4 a) {
    return __builtin_tachy_vsqrt_d_256(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F64x8 a) {
    return __builtin_tachy_vsqrt_d_512(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F32x4 a) {
    return __builtin_tachy_vsqrt_f_128(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F32x8 a) {
    return __builtin_tachy_vsqrt_f_256(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsqrt(F32x16 a) {
    return __builtin_tachy_vsqrt_f_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsub1_128(a, b);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S8x16 a, S8x16 b) {
    return (S8x16) __builtin_tachy_vsub1_128((__u8x16) a, (__u8x16) b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsub1_256(a, b);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S8x32 a, S8x32 b) {
    return (S8x32) __builtin_tachy_vsub1_256((__u8x32) a, (__u8x32) b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsub1_512(a, b);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S8x64 a, S8x64 b) {
    return (S8x64) __builtin_tachy_vsub1_512((__u8x64) a, (__u8x64) b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsub2_128(a, b);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S16x8 a, S16x8 b) {
    return (S16x8) __builtin_tachy_vsub2_128((__u16x8) a, (__u16x8) b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsub2_256(a, b);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S16x16 a, S16x16 b) {
    return (S16x16) __builtin_tachy_vsub2_256((__u16x16) a, (__u16x16) b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsub2_512(a, b);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S16x32 a, S16x32 b) {
    return (S16x32) __builtin_tachy_vsub2_512((__u16x32) a, (__u16x32) b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsub4_128(a, b);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S32x4 a, S32x4 b) {
    return (S32x4) __builtin_tachy_vsub4_128((__u32x4) a, (__u32x4) b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsub4_256(a, b);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S32x8 a, S32x8 b) {
    return (S32x8) __builtin_tachy_vsub4_256((__u32x8) a, (__u32x8) b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsub4_512(a, b);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S32x16 a, S32x16 b) {
    return (S32x16) __builtin_tachy_vsub4_512((__u32x16) a, (__u32x16) b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U64x2 a, U64x2 b) {
    return __builtin_tachy_vsub8_128(a, b);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S64x2 a, S64x2 b) {
    return (S64x2) __builtin_tachy_vsub8_128((__u64x2) a, (__u64x2) b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U64x4 a, U64x4 b) {
    return __builtin_tachy_vsub8_256(a, b);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S64x4 a, S64x4 b) {
    return (S64x4) __builtin_tachy_vsub8_256((__u64x4) a, (__u64x4) b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(U64x8 a, U64x8 b) {
    return __builtin_tachy_vsub8_512(a, b);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(S64x8 a, S64x8 b) {
    return (S64x8) __builtin_tachy_vsub8_512((__u64x8) a, (__u64x8) b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F64x2 a, F64x2 b) {
    return __builtin_tachy_vsub_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F64x4 a, F64x4 b) {
    return __builtin_tachy_vsub_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F64x8 a, F64x8 b) {
    return __builtin_tachy_vsub_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F32x4 a, F32x4 b) {
    return __builtin_tachy_vsub_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F32x8 a, F32x8 b) {
    return __builtin_tachy_vsub_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsub(F32x16 a, F32x16 b) {
    return __builtin_tachy_vsub_f_512(a, b);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F64x2 a, F64x2 b) {
    return __builtin_tachy_vsuba_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F64x4 a, F64x4 b) {
    return __builtin_tachy_vsuba_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F64x8 a, F64x8 b) {
    return __builtin_tachy_vsuba_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F32x4 a, F32x4 b) {
    return __builtin_tachy_vsuba_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F32x8 a, F32x8 b) {
    return __builtin_tachy_vsuba_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsuba(F32x16 a, F32x16 b) {
    return __builtin_tachy_vsuba_f_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vsubc1_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vsubc1_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vsubc1_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vsubc2_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vsubc2_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vsubc2_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vsubc4_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vsubc4_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vsubc4_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vsubc8_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vsubc8_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubc(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vsubc8_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsubco_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsubco_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsubco_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsubco_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsubco_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsubco_h_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x2 a, U64x2 b) {
    return __builtin_tachy_vsubco_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x4 a, U64x4 b) {
    return __builtin_tachy_vsubco_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x8 a, U64x8 b) {
    return __builtin_tachy_vsubco_l_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsubco_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsubco_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsubco_w_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x16 a, U8x16 b, U8x16 c) {
    return __builtin_tachy_vsubco_x_b_128(a, b, c);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x32 a, U8x32 b, U8x32 c) {
    return __builtin_tachy_vsubco_x_b_256(a, b, c);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U8x64 a, U8x64 b, U8x64 c) {
    return __builtin_tachy_vsubco_x_b_512(a, b, c);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x8 a, U16x8 b, U16x8 c) {
    return __builtin_tachy_vsubco_x_h_128(a, b, c);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x16 a, U16x16 b, U16x16 c) {
    return __builtin_tachy_vsubco_x_h_256(a, b, c);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U16x32 a, U16x32 b, U16x32 c) {
    return __builtin_tachy_vsubco_x_h_512(a, b, c);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x2 a, U64x2 b, U64x2 c) {
    return __builtin_tachy_vsubco_x_l_128(a, b, c);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x4 a, U64x4 b, U64x4 c) {
    return __builtin_tachy_vsubco_x_l_256(a, b, c);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U64x8 a, U64x8 b, U64x8 c) {
    return __builtin_tachy_vsubco_x_l_512(a, b, c);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x4 a, U32x4 b, U32x4 c) {
    return __builtin_tachy_vsubco_x_w_128(a, b, c);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x8 a, U32x8 b, U32x8 c) {
    return __builtin_tachy_vsubco_x_w_256(a, b, c);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubco(U32x16 a, U32x16 b, U32x16 c) {
    return __builtin_tachy_vsubco_x_w_512(a, b, c);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsubs1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsubs1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsubs1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsubs2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsubs2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsubs2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsubs4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsubs4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsubs4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U64x2 a, U64x2 b) {
    return __builtin_tachy_vsubs8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U64x4 a, U64x4 b) {
    return __builtin_tachy_vsubs8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubs(U64x8 a, U64x8 b) {
    return __builtin_tachy_vsubs8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U8x16 a, U8x16 b) {
    return __builtin_tachy_vsubu1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U8x32 a, U8x32 b) {
    return __builtin_tachy_vsubu1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U8x64 a, U8x64 b) {
    return __builtin_tachy_vsubu1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U16x8 a, U16x8 b) {
    return __builtin_tachy_vsubu2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U16x16 a, U16x16 b) {
    return __builtin_tachy_vsubu2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U16x32 a, U16x32 b) {
    return __builtin_tachy_vsubu2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U32x4 a, U32x4 b) {
    return __builtin_tachy_vsubu4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U32x8 a, U32x8 b) {
    return __builtin_tachy_vsubu4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U32x16 a, U32x16 b) {
    return __builtin_tachy_vsubu4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U64x2 a, U64x2 b) {
    return __builtin_tachy_vsubu8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U64x4 a, U64x4 b) {
    return __builtin_tachy_vsubu8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsubu(U64x8 a, U64x8 b) {
    return __builtin_tachy_vsubu8_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U16x8 a) {
    return __builtin_tachy_vsxb2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U16x16 a) {
    return __builtin_tachy_vsxb2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U16x32 a) {
    return __builtin_tachy_vsxb2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U32x4 a) {
    return __builtin_tachy_vsxb4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U32x8 a) {
    return __builtin_tachy_vsxb4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U32x16 a) {
    return __builtin_tachy_vsxb4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U64x2 a) {
    return __builtin_tachy_vsxb8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U64x4 a) {
    return __builtin_tachy_vsxb8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxb(U64x8 a) {
    return __builtin_tachy_vsxb8_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U32x4 a) {
    return __builtin_tachy_vsxh4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U32x8 a) {
    return __builtin_tachy_vsxh4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U32x16 a) {
    return __builtin_tachy_vsxh4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U64x2 a) {
    return __builtin_tachy_vsxh8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U64x4 a) {
    return __builtin_tachy_vsxh8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxh(U64x8 a) {
    return __builtin_tachy_vsxh8_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxw(U64x2 a) {
    return __builtin_tachy_vsxw8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxw(U64x4 a) {
    return __builtin_tachy_vsxw8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vsxw(U64x8 a) {
    return __builtin_tachy_vsxw8_512(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F64x2 a, F64x2 b) {
    return __builtin_tachy_vuno_d_128(a, b);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F64x4 a, F64x4 b) {
    return __builtin_tachy_vuno_d_256(a, b);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F64x8 a, F64x8 b) {
    return __builtin_tachy_vuno_d_512(a, b);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F32x4 a, F32x4 b) {
    return __builtin_tachy_vuno_f_128(a, b);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F32x8 a, F32x8 b) {
    return __builtin_tachy_vuno_f_256(a, b);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno(F32x16 a, F32x16 b) {
    return __builtin_tachy_vuno_f_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F64x2 a, F64x2 b) {
    return __builtin_tachy_vuno_u_d_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F64x4 a, F64x4 b) {
    return __builtin_tachy_vuno_u_d_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F64x8 a, F64x8 b) {
    return __builtin_tachy_vuno_u_d_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F32x4 a, F32x4 b) {
    return __builtin_tachy_vuno_u_f_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F32x8 a, F32x8 b) {
    return __builtin_tachy_vuno_u_f_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vuno_u(F32x16 a, F32x16 b) {
    return __builtin_tachy_vuno_u_f_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U8x16 a) {
    return __builtin_tachy_vunpk_h_b_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U8x32 a) {
    return __builtin_tachy_vunpk_h_b_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U8x64 a) {
    return __builtin_tachy_vunpk_h_b_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U32x4 a) {
    return __builtin_tachy_vunpk_l_w_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U32x8 a) {
    return __builtin_tachy_vunpk_l_w_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U32x16 a) {
    return __builtin_tachy_vunpk_l_w_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U16x8 a) {
    return __builtin_tachy_vunpk_w_h_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U16x16 a) {
    return __builtin_tachy_vunpk_w_h_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpk(U16x32 a) {
    return __builtin_tachy_vunpk_w_h_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U8x16 a) {
    return __builtin_tachy_vunpkh_h_b_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U8x32 a) {
    return __builtin_tachy_vunpkh_h_b_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U8x64 a) {
    return __builtin_tachy_vunpkh_h_b_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U32x4 a) {
    return __builtin_tachy_vunpkh_l_w_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U32x8 a) {
    return __builtin_tachy_vunpkh_l_w_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U32x16 a) {
    return __builtin_tachy_vunpkh_l_w_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U16x8 a) {
    return __builtin_tachy_vunpkh_w_h_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U16x16 a) {
    return __builtin_tachy_vunpkh_w_h_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkh(U16x32 a) {
    return __builtin_tachy_vunpkh_w_h_512(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U8x16 a) {
    return __builtin_tachy_vunpkhs_h_b_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U8x32 a) {
    return __builtin_tachy_vunpkhs_h_b_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U8x64 a) {
    return __builtin_tachy_vunpkhs_h_b_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U32x4 a) {
    return __builtin_tachy_vunpkhs_l_w_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U32x8 a) {
    return __builtin_tachy_vunpkhs_l_w_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U32x16 a) {
    return __builtin_tachy_vunpkhs_l_w_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U16x8 a) {
    return __builtin_tachy_vunpkhs_w_h_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U16x16 a) {
    return __builtin_tachy_vunpkhs_w_h_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkhs(U16x32 a) {
    return __builtin_tachy_vunpkhs_w_h_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U8x16 a, U8x16 b) {
    return __builtin_tachy_vunpki_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U8x32 a, U8x32 b) {
    return __builtin_tachy_vunpki_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U8x64 a, U8x64 b) {
    return __builtin_tachy_vunpki_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U16x8 a, U16x8 b) {
    return __builtin_tachy_vunpki_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U16x16 a, U16x16 b) {
    return __builtin_tachy_vunpki_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U16x32 a, U16x32 b) {
    return __builtin_tachy_vunpki_h_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U32x4 a, U32x4 b) {
    return __builtin_tachy_vunpki_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U32x8 a, U32x8 b) {
    return __builtin_tachy_vunpki_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U32x16 a, U32x16 b) {
    return __builtin_tachy_vunpki_w_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U64x2 a, U64x2 b) {
    return __builtin_tachy_vunpki_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U64x4 a, U64x4 b) {
    return __builtin_tachy_vunpki_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpki(U64x8 a, U64x8 b) {
    return __builtin_tachy_vunpki_l_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U8x16 a, U8x16 b) {
    return __builtin_tachy_vunpkih_b_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U8x32 a, U8x32 b) {
    return __builtin_tachy_vunpkih_b_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U8x64 a, U8x64 b) {
    return __builtin_tachy_vunpkih_b_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U16x8 a, U16x8 b) {
    return __builtin_tachy_vunpkih_h_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U16x16 a, U16x16 b) {
    return __builtin_tachy_vunpkih_h_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U16x32 a, U16x32 b) {
    return __builtin_tachy_vunpkih_h_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U32x4 a, U32x4 b) {
    return __builtin_tachy_vunpkih_w_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U32x8 a, U32x8 b) {
    return __builtin_tachy_vunpkih_w_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U32x16 a, U32x16 b) {
    return __builtin_tachy_vunpkih_w_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U64x2 a, U64x2 b) {
    return __builtin_tachy_vunpkih_l_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U64x4 a, U64x4 b) {
    return __builtin_tachy_vunpkih_l_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpkih(U64x8 a, U64x8 b) {
    return __builtin_tachy_vunpkih_l_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U8x16 a) {
    return __builtin_tachy_vunpks_h_b_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U8x32 a) {
    return __builtin_tachy_vunpks_h_b_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U8x64 a) {
    return __builtin_tachy_vunpks_h_b_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U32x4 a) {
    return __builtin_tachy_vunpks_l_w_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U32x8 a) {
    return __builtin_tachy_vunpks_l_w_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U32x16 a) {
    return __builtin_tachy_vunpks_l_w_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U16x8 a) {
    return __builtin_tachy_vunpks_w_h_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U16x16 a) {
    return __builtin_tachy_vunpks_w_h_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vunpks(U16x32 a) {
    return __builtin_tachy_vunpks_w_h_512(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U8x16 a, U8x16 b) {
    return __builtin_tachy_vxor1_128(a, b);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U8x32 a, U8x32 b) {
    return __builtin_tachy_vxor1_256(a, b);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U8x64 a, U8x64 b) {
    return __builtin_tachy_vxor1_512(a, b);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U16x8 a, U16x8 b) {
    return __builtin_tachy_vxor2_128(a, b);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U16x16 a, U16x16 b) {
    return __builtin_tachy_vxor2_256(a, b);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U16x32 a, U16x32 b) {
    return __builtin_tachy_vxor2_512(a, b);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U32x4 a, U32x4 b) {
    return __builtin_tachy_vxor4_128(a, b);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U32x8 a, U32x8 b) {
    return __builtin_tachy_vxor4_256(a, b);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U32x16 a, U32x16 b) {
    return __builtin_tachy_vxor4_512(a, b);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U64x2 a, U64x2 b) {
    return __builtin_tachy_vxor8_128(a, b);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U64x4 a, U64x4 b) {
    return __builtin_tachy_vxor8_256(a, b);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxor(U64x8 a, U64x8 b) {
    return __builtin_tachy_vxor8_512(a, b);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U8x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U8x32 a, unsigned int b) {
    return __builtin_tachy_vxtr_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U8x64 a, unsigned int b) {
    return __builtin_tachy_vxtr_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U16x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U16x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U16x32 a, unsigned int b) {
    return __builtin_tachy_vxtr_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U64x2 a, unsigned int b) {
    return __builtin_tachy_vxtr_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U64x4 a, unsigned int b) {
    return __builtin_tachy_vxtr_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U64x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U8x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U8x32 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint8_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U8x64 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U16x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U16x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint16_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U16x32 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U64x2 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U64x4 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint64_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U64x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U32x4 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U32x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline uint32_t
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr_el(U32x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_r_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U32x4 a, unsigned int b) {
    return __builtin_tachy_vxtr_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U32x8 a, unsigned int b) {
    return __builtin_tachy_vxtr_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtr(U32x16 a, unsigned int b) {
    return __builtin_tachy_vxtr_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U8x16 a, unsigned int b) {
    return __builtin_tachy_vxtrx_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U8x32 a, unsigned int b) {
    return __builtin_tachy_vxtrx_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U8x64 a, unsigned int b) {
    return __builtin_tachy_vxtrx_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U16x8 a, unsigned int b) {
    return __builtin_tachy_vxtrx_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U16x16 a, unsigned int b) {
    return __builtin_tachy_vxtrx_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U16x32 a, unsigned int b) {
    return __builtin_tachy_vxtrx_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U64x2 a, unsigned int b) {
    return __builtin_tachy_vxtrx_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U64x4 a, unsigned int b) {
    return __builtin_tachy_vxtrx_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U64x8 a, unsigned int b) {
    return __builtin_tachy_vxtrx_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U32x4 a, unsigned int b) {
    return __builtin_tachy_vxtrx_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U32x8 a, unsigned int b) {
    return __builtin_tachy_vxtrx_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrx(U32x16 a, unsigned int b) {
    return __builtin_tachy_vxtrx_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U8x16 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_b_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U8x32 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_b_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U8x64 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_b_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U16x8 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_h_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U16x16 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_h_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U16x32 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_h_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U64x2 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_l_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U64x4 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_l_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U64x8 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_l_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U32x4 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_w_128(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U32x8 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_w_256(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vxtrxx(U32x16 a, unsigned int b) {
    return __builtin_tachy_vxtrxx_w_512(a, __builtin_constant_p (b) ? (b) : -1);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U16x8 a) {
    return __builtin_tachy_vzxb2_128(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U16x16 a) {
    return __builtin_tachy_vzxb2_256(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U16x32 a) {
    return __builtin_tachy_vzxb2_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U32x4 a) {
    return __builtin_tachy_vzxb4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U32x8 a) {
    return __builtin_tachy_vzxb4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U32x16 a) {
    return __builtin_tachy_vzxb4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U64x2 a) {
    return __builtin_tachy_vzxb8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U64x4 a) {
    return __builtin_tachy_vzxb8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxb(U64x8 a) {
    return __builtin_tachy_vzxb8_512(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U32x4 a) {
    return __builtin_tachy_vzxh4_128(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U32x8 a) {
    return __builtin_tachy_vzxh4_256(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U32x16 a) {
    return __builtin_tachy_vzxh4_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U64x2 a) {
    return __builtin_tachy_vzxh8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U64x4 a) {
    return __builtin_tachy_vzxh8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxh(U64x8 a) {
    return __builtin_tachy_vzxh8_512(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxw(U64x2 a) {
    return __builtin_tachy_vzxw8_128(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxw(U64x4 a) {
    return __builtin_tachy_vzxw8_256(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vzxw(U64x8 a) {
    return __builtin_tachy_vzxw8_512(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(F64x8 a)
{
    return __builtin_tachy_cast_v8df_to_v4df(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(F32x16 a)
{
    return __builtin_tachy_cast_v16sf_to_v8sf(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U64x8 a)
{
    return __builtin_tachy_cast_v8di_to_v4di(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U32x16 a)
{
    return __builtin_tachy_cast_v16si_to_v8si(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U16x32 a)
{
    return __builtin_tachy_cast_v32hi_to_v16hi(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U8x64 a)
{
    return __builtin_tachy_cast_v64qi_to_v32qi(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(F64x4 a)
{
    return __builtin_tachy_cast_v4df_to_v8df(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(F32x8 a)
{
    return __builtin_tachy_cast_v8sf_to_v16sf(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U64x4 a)
{
    return __builtin_tachy_cast_v4di_to_v8di(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U32x8 a)
{
    return __builtin_tachy_cast_v8si_to_v16si(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U16x16 a)
{
    return __builtin_tachy_cast_v16hi_to_v32hi(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U8x32 a)
{
    return __builtin_tachy_cast_v32qi_to_v64qi(a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(F64x8 a)
{
    return __builtin_tachy_cast_v8df_to_v2df(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(F32x16 a)
{
    return __builtin_tachy_cast_v16sf_to_v4sf(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U64x8 a)
{
    return __builtin_tachy_cast_v8di_to_v2di(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U32x16 a)
{
    return __builtin_tachy_cast_v16si_to_v4si(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U16x32 a)
{
    return __builtin_tachy_cast_v32hi_to_v8hi(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U8x64 a)
{
    return __builtin_tachy_cast_v64qi_to_v16qi(a);
}

static inline F64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(F64x2 a)
{
    return __builtin_tachy_cast_v2df_to_v8df(a);
}

static inline F32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(F32x4 a)
{
    return __builtin_tachy_cast_v4sf_to_v16sf(a);
}

static inline U64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U64x2 a)
{
    return __builtin_tachy_cast_v2di_to_v8di(a);
}

static inline U32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U32x4 a)
{
    return __builtin_tachy_cast_v4si_to_v16si(a);
}

static inline U16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U16x8 a)
{
    return __builtin_tachy_cast_v8hi_to_v32hi(a);
}

static inline U8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(U8x16 a)
{
    return __builtin_tachy_cast_v16qi_to_v64qi(a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S64x8 a)
{
    return (S64x4) __builtin_tachy_cast_v8di_to_v4di((U64x8) a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S32x16 a)
{
    return (S32x8) __builtin_tachy_cast_v16si_to_v8si((U32x16) a);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S16x32 a)
{
    return (S16x16) __builtin_tachy_cast_v32hi_to_v16hi((U16x32) a);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S8x64 a)
{
    return (S8x32) __builtin_tachy_cast_v64qi_to_v32qi((U8x64) a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S64x4 a)
{
    return (S64x8) __builtin_tachy_cast_v4di_to_v8di((U64x4) a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S32x8 a)
{
    return (S32x16) __builtin_tachy_cast_v8si_to_v16si((U32x8) a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S16x16 a)
{
    return (S16x32) __builtin_tachy_cast_v16hi_to_v32hi((U16x16) a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S8x32 a)
{
    return (S8x64) __builtin_tachy_cast_v32qi_to_v64qi((U8x32) a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S64x8 a)
{
    return (S64x2) __builtin_tachy_cast_v8di_to_v2di((U64x8) a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S32x16 a)
{
    return (S32x4) __builtin_tachy_cast_v16si_to_v4si((U32x16) a);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S16x32 a)
{
    return (S16x8) __builtin_tachy_cast_v32hi_to_v8hi((U16x32) a);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S8x64 a)
{
    return (S8x16) __builtin_tachy_cast_v64qi_to_v16qi((U8x64) a);
}

static inline S64x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S64x2 a)
{
    return (S64x8) __builtin_tachy_cast_v2di_to_v8di((U64x2) a);
}

static inline S32x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S32x4 a)
{
    return (S32x16) __builtin_tachy_cast_v4si_to_v16si((U32x4) a);
}

static inline S16x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S16x8 a)
{
    return (S16x32) __builtin_tachy_cast_v8hi_to_v32hi((U16x8) a);
}

static inline S8x64
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_512(S8x16 a)
{
    return (S8x64) __builtin_tachy_cast_v16qi_to_v64qi((U8x16) a);
}

static inline F64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(F64x4 a)
{
    return __builtin_tachy_cast_v4df_to_v2df(a);
}

static inline F32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(F32x8 a)
{
    return __builtin_tachy_cast_v8sf_to_v4sf(a);
}

static inline U64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U64x4 a)
{
    return __builtin_tachy_cast_v4di_to_v2di(a);
}

static inline U32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U32x8 a)
{
    return __builtin_tachy_cast_v8si_to_v4si(a);
}

static inline U16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U16x16 a)
{
    return __builtin_tachy_cast_v16hi_to_v8hi(a);
}

static inline U8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(U8x32 a)
{
    return __builtin_tachy_cast_v32qi_to_v16qi(a);
}

static inline F64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(F64x2 a)
{
    return __builtin_tachy_cast_v2df_to_v4df(a);
}

static inline F32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(F32x4 a)
{
    return __builtin_tachy_cast_v4sf_to_v8sf(a);
}

static inline U64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U64x2 a)
{
    return __builtin_tachy_cast_v2di_to_v4di(a);
}

static inline U32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U32x4 a)
{
    return __builtin_tachy_cast_v4si_to_v8si(a);
}

static inline U16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U16x8 a)
{
    return __builtin_tachy_cast_v8hi_to_v16hi(a);
}

static inline U8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(U8x16 a)
{
    return __builtin_tachy_cast_v16qi_to_v32qi(a);
}

static inline S64x2
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S64x4 a)
{
    return (S64x2) __builtin_tachy_cast_v4di_to_v2di((U64x4) a);
}

static inline S32x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S32x8 a)
{
    return (S32x4) __builtin_tachy_cast_v8si_to_v4si((U32x8) a);
}

static inline S16x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S16x16 a)
{
    return (S16x8) __builtin_tachy_cast_v16hi_to_v8hi((U16x16) a);
}

static inline S8x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_128(S8x32 a)
{
    return (S8x16) __builtin_tachy_cast_v32qi_to_v16qi((U8x32) a);
}

static inline S64x4
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S64x2 a)
{
    return (S64x4) __builtin_tachy_cast_v2di_to_v4di((U64x2) a);
}

static inline S32x8
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S32x4 a)
{
    return (S32x8) __builtin_tachy_cast_v4si_to_v8si((U32x4) a);
}

static inline S16x16
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S16x8 a)
{
    return (S16x16) __builtin_tachy_cast_v8hi_to_v16hi((U16x8) a);
}

static inline S8x32
__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
vcast_to_256(S8x16 a)
{
    return (S8x32) __builtin_tachy_cast_v16qi_to_v32qi((U8x16) a);
}
} // end of tvx namespace
